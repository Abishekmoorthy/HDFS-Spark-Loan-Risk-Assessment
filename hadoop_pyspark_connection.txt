shek@DESKTOP-I4C3SAV:~$ sudo apt-get install default-jdk
[sudo] password for shek:
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages were automatically installed and are no longer required:
  hwloc-nox libmpich12 libslurm40t64
Use 'sudo apt autoremove' to remove them.
The following additional packages will be installed:
  default-jdk-headless default-jre default-jre-headless openjdk-21-jdk openjdk-21-jdk-headless openjdk-21-jre
  openjdk-21-jre-headless
Suggested packages:
  openjdk-21-demo openjdk-21-source visualvm libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei
  | fonts-wqy-zenhei fonts-indic
The following NEW packages will be installed:
  default-jdk default-jdk-headless default-jre default-jre-headless openjdk-21-jdk openjdk-21-jdk-headless
  openjdk-21-jre openjdk-21-jre-headless
0 upgraded, 8 newly installed, 0 to remove and 125 not upgraded.
Need to get 131 MB of archives.
After this operation, 301 MB of additional disk space will be used.
Do you want to continue? [Y/n] Y
Get:1 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 openjdk-21-jre-headless amd64 21.0.8+9~us1-0ubuntu1~24.04.1 [46.4 MB]
Get:2 http://archive.ubuntu.com/ubuntu noble/main amd64 default-jre-headless amd64 2:1.21-75+exp1 [3094 B]
Get:3 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 openjdk-21-jre amd64 21.0.8+9~us1-0ubuntu1~24.04.1 [228 kB]
Get:4 http://archive.ubuntu.com/ubuntu noble/main amd64 default-jre amd64 2:1.21-75+exp1 [922 B]
Get:5 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 openjdk-21-jdk-headless amd64 21.0.8+9~us1-0ubuntu1~24.04.1 [82.7 MB]
Get:6 http://archive.ubuntu.com/ubuntu noble/main amd64 default-jdk-headless amd64 2:1.21-75+exp1 [960 B]
Get:7 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 openjdk-21-jdk amd64 21.0.8+9~us1-0ubuntu1~24.04.1 [1645 kB]
Get:8 http://archive.ubuntu.com/ubuntu noble/main amd64 default-jdk amd64 2:1.21-75+exp1 [926 B]
Fetched 131 MB in 5min 32s (394 kB/s)
Selecting previously unselected package openjdk-21-jre-headless:amd64.
(Reading database ... 67339 files and directories currently installed.)
Preparing to unpack .../0-openjdk-21-jre-headless_21.0.8+9~us1-0ubuntu1~24.04.1_amd64.deb ...
Unpacking openjdk-21-jre-headless:amd64 (21.0.8+9~us1-0ubuntu1~24.04.1) ...
Selecting previously unselected package default-jre-headless.
Preparing to unpack .../1-default-jre-headless_2%3a1.21-75+exp1_amd64.deb ...
Unpacking default-jre-headless (2:1.21-75+exp1) ...
Selecting previously unselected package openjdk-21-jre:amd64.
Preparing to unpack .../2-openjdk-21-jre_21.0.8+9~us1-0ubuntu1~24.04.1_amd64.deb ...
Unpacking openjdk-21-jre:amd64 (21.0.8+9~us1-0ubuntu1~24.04.1) ...
Selecting previously unselected package default-jre.
Preparing to unpack .../3-default-jre_2%3a1.21-75+exp1_amd64.deb ...
Unpacking default-jre (2:1.21-75+exp1) ...
Selecting previously unselected package openjdk-21-jdk-headless:amd64.
Preparing to unpack .../4-openjdk-21-jdk-headless_21.0.8+9~us1-0ubuntu1~24.04.1_amd64.deb ...
Unpacking openjdk-21-jdk-headless:amd64 (21.0.8+9~us1-0ubuntu1~24.04.1) ...
Selecting previously unselected package default-jdk-headless.
Preparing to unpack .../5-default-jdk-headless_2%3a1.21-75+exp1_amd64.deb ...
Unpacking default-jdk-headless (2:1.21-75+exp1) ...
Selecting previously unselected package openjdk-21-jdk:amd64.
Preparing to unpack .../6-openjdk-21-jdk_21.0.8+9~us1-0ubuntu1~24.04.1_amd64.deb ...
Unpacking openjdk-21-jdk:amd64 (21.0.8+9~us1-0ubuntu1~24.04.1) ...
Selecting previously unselected package default-jdk.
Preparing to unpack .../7-default-jdk_2%3a1.21-75+exp1_amd64.deb ...
Unpacking default-jdk (2:1.21-75+exp1) ...
Setting up openjdk-21-jre-headless:amd64 (21.0.8+9~us1-0ubuntu1~24.04.1) ...
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jpackage to provide /usr/bin/jpackage (jpackage) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode
Processing triggers for hicolor-icon-theme (0.17-2) ...
Processing triggers for ca-certificates-java (20240118) ...
done.
Setting up openjdk-21-jre:amd64 (21.0.8+9~us1-0ubuntu1~24.04.1) ...
Setting up openjdk-21-jdk-headless:amd64 (21.0.8+9~us1-0ubuntu1~24.04.1) ...
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jwebserver to provide /usr/bin/jwebserver (jwebserver) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode
Setting up default-jre-headless (2:1.21-75+exp1) ...
Setting up default-jre (2:1.21-75+exp1) ...
Setting up openjdk-21-jdk:amd64 (21.0.8+9~us1-0ubuntu1~24.04.1) ...
update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode
Setting up default-jdk-headless (2:1.21-75+exp1) ...
Setting up default-jdk (2:1.21-75+exp1) ...
shek@DESKTOP-I4C3SAV:~$ java -version
openjdk version "21.0.8" 2025-07-15
OpenJDK Runtime Environment (build 21.0.8+9-Ubuntu-0ubuntu124.04.1)
OpenJDK 64-Bit Server VM (build 21.0.8+9-Ubuntu-0ubuntu124.04.1, mixed mode, sharing)
shek@DESKTOP-I4C3SAV:~$ sudo addgroup hadoop
fatal: The group `hadoop' already exists.
shek@DESKTOP-I4C3SAV:~$ sudo adduser --ingroup hadoop hduser
Adding user
fatal: The user `hduser' already exists.
Adding: command not found
shek@DESKTOP-I4C3SAV:~$ sudo apt-get install ssh
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
ssh is already the newest version (1:9.6p1-3ubuntu13.14).
The following packages were automatically installed and are no longer required:
  hwloc-nox libmpich12 libslurm40t64
Use 'sudo apt autoremove' to remove them.
0 upgraded, 0 newly installed, 0 to remove and 125 not upgraded.
shek@DESKTOP-I4C3SAV:~$ su hduser
Password:
hduser@DESKTOP-I4C3SAV:/home/shek$ ssh-keygen -t rsa -P ""
Generating public/private rsa key pair.
Enter file in which to save the key (/home/hduser/.ssh/id_rsa):
/home/hduser/.ssh/id_rsa already exists.
Overwrite (y/n)? y
Your identification has been saved in /home/hduser/.ssh/id_rsa
Your public key has been saved in /home/hduser/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:o0wzHFssLn7Pzr14eWtP3K8cRsIQOA44nwS18kVEOyk hduser@DESKTOP-I4C3SAV
The key's randomart image is:
+---[RSA 3072]----+
|    .+.o+..      |
|    o o+oo .     |
|    .+E+B..      |
|     =oB.. o     |
|    . O S   o .  |
|   . + + .   + . |
|    . +    .  = .|
|     . + oo o+ ..|
|       .*.o+.o+..|
+----[SHA256]-----+
hduser@DESKTOP-I4C3SAV:/home/shek$ /home/k$ cat $HOME/.ssh/id_rsa.pub >>
bash: syntax error near unexpected token `newline'
hduser@DESKTOP-I4C3SAV:/home/shek$ /home/k$ cat $HOME/.ssh/id_rsa.pub >>
$HOME/.ssh/authorized_keys
bash: syntax error near unexpected token `newline'
bash: /home/hduser/.ssh/authorized_keys: No such file or directory
hduser@DESKTOP-I4C3SAV:/home/shek$ home/k$ cat $HOME/.ssh/id_rsa.pub >>$HOME/.ssh/authorized_keys
bash: home/k$: Permission denied
hduser@DESKTOP-I4C3SAV:/home/shek$ /home/k$ cat $HOME/.ssh/id_rsa.pub >>$HOME/.ssh/authorized_keys
bash: /home/k$: No such file or directory
hduser@DESKTOP-I4C3SAV:/home/shek$ /home/k$ ssh localhost
bash: /home/k$: No such file or directory
hduser@DESKTOP-I4C3SAV:/home/shek$ cat $HOME/.ssh/id_rsa.pub >>
bash: syntax error near unexpected token `newline'
hduser@DESKTOP-I4C3SAV:/home/shek$ cat $HOME/.ssh/id_rsa.pub
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDrvZIkHHaotf0q1O5CBFsLBvP0tzFWEBZnVf1cOV2UArYx21pvIsVvgSoENWJA1h+UIkAsqHSEoOknqy4/DaLTrbDBg5+QGi5V/zoWRLJVh7IsyDVb1reLXmfgtH1Wa/3eylxOp6Z6vPYDkaHYbhufLziwmom8gucFMv9g2XbxGJw5S+1Y72LkmoegXP/cS8OJ60qT6VcSfMxNyv/78Oxhfu41z/65nPZg9qyUxd5bfq/bhnybrLoVWI6iGdGYWZVCPDRotWwPkYneLAkfQPfXWVUhStRwVCnQy6aOOYtIgfR2RT3ZCnnCUnMn9YUDuLild3nILa6cJqQ9eecaX61wOWdRQvs/PPfRMF0sIqvzjAMcY73ATN3srmN+qtxsWm30MUdylsck4ENRf/WDyvPOaFJDgO57qQXaGpBEJtixss8Yhr31Racf9MsocYnJCxEkZiRIyRis3a38bUVryW2evhjwON/rysu81scppmGcsfpsp2k+O4vHQnYOVhKE2L8= hduser@DESKTOP-I4C3SAV
hduser@DESKTOP-I4C3SAV:/home/shek$ cat $HOME/.ssh/id_rsa.pub >>$HOME/.ssh/authorized_keys
hduser@DESKTOP-I4C3SAV:/home/shek$ /home/k$ ssh localhost
bash: /home/k$: No such file or directory
hduser@DESKTOP-I4C3SAV:/home/shek$ ssh localhost
The authenticity of host 'localhost (127.0.0.1)' can't be established.
ED25519 key fingerprint is SHA256:+Gn1B5yOJQc9JOM0TeZCJ7CkPz4AnQHuM1EQ45uNqNQ.
This key is not known by any other names.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.
Welcome to Ubuntu 24.04.1 LTS (GNU/Linux 6.6.87.2-microsoft-standard-WSL2 x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

 System information as of Fri Oct 10 09:39:14 UTC 2025

  System load:  0.07                Processes:             49
  Usage of /:   1.7% of 1006.85GB   Users logged in:       1
  Memory usage: 8%                  IPv4 address for eth0: 172.24.241.133
  Swap usage:   0%

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

Welcome to Ubuntu 24.04.1 LTS (GNU/Linux 6.6.87.2-microsoft-standard-WSL2 x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

 System information as of Fri Oct 10 09:39:14 UTC 2025

  System load:  0.07                Processes:             49
  Usage of /:   1.7% of 1006.85GB   Users logged in:       1
  Memory usage: 8%                  IPv4 address for eth0: 172.24.241.133
  Swap usage:   0%

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

This message is shown once a day. To disable it please create the
/home/hduser/.hushlogin file.
hduser@DESKTOP-I4C3SAV:~$ wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-
2.6.0/hadoop-2.6.0.tar.gz
--2025-10-10 09:39:27--  http://mirrors.sonic.net/apache/hadoop/common/hadoop-
Resolving mirrors.sonic.net (mirrors.sonic.net)... 157.131.224.201, 2001:5a8:601:4003:157:131:224:201
Connecting to mirrors.sonic.net (mirrors.sonic.net)|157.131.224.201|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2025-10-10 09:39:30 ERROR 404: Not Found.

-bash: 2.6.0/hadoop-2.6.0.tar.gz: No such file or directory
hduser@DESKTOP-I4C3SAV:~$ wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
--2025-10-10 09:39:44--  http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
Resolving mirrors.sonic.net (mirrors.sonic.net)... 157.131.224.201, 2001:5a8:601:4003:157:131:224:201
Connecting to mirrors.sonic.net (mirrors.sonic.net)|157.131.224.201|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2025-10-10 09:39:45 ERROR 404: Not Found.

hduser@DESKTOP-I4C3SAV:~$ wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
--2025-10-10 09:40:47--  http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
Resolving mirrors.sonic.net (mirrors.sonic.net)... 157.131.224.201, 2001:5a8:601:4003:157:131:224:201
Connecting to mirrors.sonic.net (mirrors.sonic.net)|157.131.224.201|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2025-10-10 09:40:48 ERROR 404: Not Found.

hduser@DESKTOP-I4C3SAV:~$ wget https://archive.apache.org/dist/hadoop/core/hadoop-2.6.0/hadoop-2.6.0.tar.gz
--2025-10-10 09:41:12--  https://archive.apache.org/dist/hadoop/core/hadoop-2.6.0/hadoop-2.6.0.tar.gz
Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2
Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 195257604 (186M) [application/x-gzip]
Saving to: ‘hadoop-2.6.0.tar.gz’

hadoop-2.6.0.tar.gz             1%[                                                  ]   2.14M  18.1KB/s    eta 3h 54m ^C
hduser@DESKTOP-I4C3SAV:~$ wget https://archive.apache.org/dist/hadoop/core/hadoop-2.6.0/hadoop-2.6.0.tar.gz
--2025-10-10 09:45:10--  https://archive.apache.org/dist/hadoop/core/hadoop-2.6.0/hadoop-2.6.0.tar.gz
Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2
Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 195257604 (186M) [application/x-gzip]
Saving to: ‘hadoop-2.6.0.tar.gz.1’

hadoop-2.6.0.tar.gz.1           90%[==============================================>      ] 168.62M  99.7KB/s    eta 1m 4hadoop-2.6.0.tar.gz.1          90%[============================================>     ] 168.64M  91.0KB/s    eta 1mhadoop-2.6.0.tar.gz.1          90%[=========================================>     ] 168.67M  83.8KB/s    eta 1m 42hadoop-2.6.0.tar.gz.1         90%[=========================================>     ] 168.68M  85.0KB/s    eta 1m 4hadoop-2.6.0.tar.gz.1         90%[========================================>     ] 168.70M  89.6KB/s    eta 1m hadoop-2.6.0.tar.gz.1         90%[=======================================>     ] 168.72M  90.8KB/s    eta 1mhadoop-2.6.0.tar.gz.1         90%[======================================>     ] 168.77M  92.2KB/s    eta 1hadoop-2.6.0.tar.gz.1        90%[======================================>     ] 168.81M  96.7KB/s    eta hadoop-2.6.0.tar.gz.1        90%[=====================================>     ] 168.82M  99.5KB/s    etahadoop-2.6.0.tar.gz.1        9hadoop-2.6.0.tar.gz.1    hadoop-2.6.0.tar.gz.1    90%[===========================>   ] 168.88M  97.2KB/s    hadoop-2.6.0.tar.gz.1    90%[==========================>   ] 168.92Mhadoop-2.6.0.tar.gz.1   90%[===========hadoop-2.6.0.hadoop-2.6.0.tar.gz.1    90%[==========hadoop-2.6.0.tar.gz.1     90%[===========================hadoop-2.6.0.tar.gz.1   hadoop-2.6.0.tar.gz.1         90%[======================================hadoop-2.6.0.tar.gz.1          90%[============================================>    hadoop-2.6.0.tar.gz.1            90%hadoop-2.6.0.tar.gz.1   hadoop-2.6.0hadoop-2.6.0.tar.gz.1   hadoop-2.6.0hadoop-2.6.0.tar.gz.1              90%[=======================================================>      ] 169.11M   113KBhadoop-2.6.0.tar.gz.1            90%[===============================================>     ] 169.13M   111KB/s hadoop-2.6.0.tahadoop-2.6.0.tar.gz.1            91%[===============================================>     ] 169.46M  hadoop-2.6.0.tar.gz.1      91%[=================================hadoop-2.6.0.tar.gz.1      91%[===============================>    ] 169.51M   11hadoop-2.6.0.tar.gz.1      91%[==========hadoop-2.6.0.tar.gz.1    91%[=================hadoop-2.6.0.tar.gz.1   91%[=========================>   ] 169.54M hadoop-2.6.0.tar.gz.1   91%[========================>   ] 169.55Mhadoop-2.6.0.tar.gz.1   91%[=======================>   ] 169hadoop-2.6.0.tar.gz.1          91%[===================================hadoop-2.6.0.tar.gz.1             91%[==================================================>      ] 169.61M   116KB/shadoop-2.6.0.tar.gz.1              91%[==================hadoop-2.6.0.tar.gzhadoop-2.6.0.tar.gz.1               91%[==========================================================>      ] 169.63M   118hadoop-2.6.0.tar.gz.1       hadoop-2.6.0.tar.gz.1                  100%[============================================================================>] 186.21M  42.5KB/s    in 19m 38s

2025-10-10 10:05:07 (162 KB/s) - ‘hadoop-2.6.0.tar.gz.1’ saved [195257604/195257604]

hduser@DESKTOP-I4C3SAV:~$ tar xvzf hadoop-2.6.0.tar.gz
hadoop-2.6.0/
hadoop-2.6.0/etc/
hadoop-2.6.0/etc/hadoop/
hadoop-2.6.0/etc/hadoop/hdfs-site.xml
hadoop-2.6.0/etc/hadoop/hadoop-metrics2.properties
hadoop-2.6.0/etc/hadoop/container-executor.cfg
hadoop-2.6.0/etc/hadoop/mapred-site.xml.template
hadoop-2.6.0/etc/hadoop/yarn-env.cmd
hadoop-2.6.0/etc/hadoop/hadoop-policy.xml
hadoop-2.6.0/etc/hadoop/log4j.properties
hadoop-2.6.0/etc/hadoop/httpfs-env.sh
hadoop-2.6.0/etc/hadoop/core-site.xml
hadoop-2.6.0/etc/hadoop/hadoop-env.cmd
hadoop-2.6.0/etc/hadoop/yarn-env.sh
hadoop-2.6.0/etc/hadoop/capacity-scheduler.xml
hadoop-2.6.0/etc/hadoop/kms-site.xml
hadoop-2.6.0/etc/hadoop/slaves
hadoop-2.6.0/etc/hadoop/ssl-server.xml.example
hadoop-2.6.0/etc/hadoop/mapred-env.sh
hadoop-2.6.0/etc/hadoop/mapred-queues.xml.template
hadoop-2.6.0/etc/hadoop/httpfs-site.xml
hadoop-2.6.0/etc/hadoop/configuration.xsl
hadoop-2.6.0/etc/hadoop/hadoop-env.sh
hadoop-2.6.0/etc/hadoop/hadoop-metrics.properties
hadoop-2.6.0/etc/hadoop/httpfs-signature.secret
hadoop-2.6.0/etc/hadoop/ssl-client.xml.example
hadoop-2.6.0/etc/hadoop/httpfs-log4j.properties
hadoop-2.6.0/etc/hadoop/kms-acls.xml
hadoop-2.6.0/etc/hadoop/kms-env.sh
hadoop-2.6.0/etc/hadoop/kms-log4j.properties
hadoop-2.6.0/etc/hadoop/yarn-site.xml
hadoop-2.6.0/etc/hadoop/mapred-env.cmd
hadoop-2.6.0/sbin/
hadoop-2.6.0/sbin/start-dfs.cmd
hadoop-2.6.0/sbin/stop-balancer.sh
hadoop-2.6.0/sbin/start-balancer.sh
hadoop-2.6.0/sbin/stop-all.sh
hadoop-2.6.0/sbin/yarn-daemon.sh
hadoop-2.6.0/sbin/yarn-daemons.sh
hadoop-2.6.0/sbin/stop-yarn.cmd
hadoop-2.6.0/sbin/stop-dfs.sh
hadoop-2.6.0/sbin/stop-dfs.cmd
hadoop-2.6.0/sbin/start-dfs.sh
hadoop-2.6.0/sbin/mr-jobhistory-daemon.sh
hadoop-2.6.0/sbin/hdfs-config.cmd
hadoop-2.6.0/sbin/hadoop-daemons.sh
hadoop-2.6.0/sbin/httpfs.sh
hadoop-2.6.0/sbin/start-yarn.cmd
hadoop-2.6.0/sbin/start-all.cmd
hadoop-2.6.0/sbin/start-secure-dns.sh
hadoop-2.6.0/sbin/stop-all.cmd
hadoop-2.6.0/sbin/start-all.sh
hadoop-2.6.0/sbin/kms.sh
hadoop-2.6.0/sbin/slaves.sh
hadoop-2.6.0/sbin/distribute-exclude.sh
hadoop-2.6.0/sbin/stop-secure-dns.sh
hadoop-2.6.0/sbin/hdfs-config.sh
hadoop-2.6.0/sbin/start-yarn.sh
hadoop-2.6.0/sbin/hadoop-daemon.sh
hadoop-2.6.0/sbin/stop-yarn.sh
hadoop-2.6.0/sbin/refresh-namenodes.sh
hadoop-2.6.0/libexec/
hadoop-2.6.0/libexec/hadoop-config.sh
hadoop-2.6.0/libexec/httpfs-config.sh
hadoop-2.6.0/libexec/yarn-config.sh
hadoop-2.6.0/libexec/hdfs-config.cmd
hadoop-2.6.0/libexec/yarn-config.cmd
hadoop-2.6.0/libexec/mapred-config.cmd
hadoop-2.6.0/libexec/hdfs-config.sh
hadoop-2.6.0/libexec/hadoop-config.cmd
hadoop-2.6.0/libexec/kms-config.sh
hadoop-2.6.0/libexec/mapred-config.sh
hadoop-2.6.0/NOTICE.txt
hadoop-2.6.0/lib/
hadoop-2.6.0/lib/native/
hadoop-2.6.0/lib/native/libhdfs.so
hadoop-2.6.0/lib/native/libhadooppipes.a
hadoop-2.6.0/lib/native/libhadooputils.a
hadoop-2.6.0/lib/native/libhdfs.a
hadoop-2.6.0/lib/native/libhdfs.so.0.0.0
hadoop-2.6.0/lib/native/libhadoop.so.1.0.0
hadoop-2.6.0/lib/native/libhadoop.a
hadoop-2.6.0/lib/native/libhadoop.so
hadoop-2.6.0/share/
hadoop-2.6.0/share/doc/
hadoop-2.6.0/share/doc/hadoop/
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/css/
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/css/maven-base.css
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/css/print.css
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/css/maven-theme.css
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/css/site.css
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/collapsed.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/h3.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/newwindow.png
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/logo_maven.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/maven-logo-2.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/banner.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/apache-maven-project-2.png
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/icon_info_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/expanded.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/external.png
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/logo_apache.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/icon_success_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/breadcrumbs.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/logos/
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/logos/build-by-maven-white.png
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/logos/build-by-maven-black.png
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/logos/maven-feather.png
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/h5.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/icon_warning_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/icon_error_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/images/bg.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/index.html
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/dependency-analysis.html
hadoop-2.6.0/share/doc/hadoop/hadoop-openstack/project-reports.html
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/css/
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/css/maven-base.css
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/css/print.css
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/css/maven-theme.css
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/css/site.css
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/collapsed.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/h3.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/newwindow.png
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/logo_maven.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/maven-logo-2.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/banner.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/apache-maven-project-2.png
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/icon_info_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/expanded.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/external.png
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/logo_apache.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/icon_success_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/breadcrumbs.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/logos/
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/logos/build-by-maven-white.png
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/logos/build-by-maven-black.png
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/logos/maven-feather.png
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/h5.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/icon_warning_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/icon_error_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/images/bg.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/dependency-analysis.html
hadoop-2.6.0/share/doc/hadoop/hadoop-maven-plugins/project-reports.html
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/css/
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/css/maven-base.css
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/css/print.css
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/css/maven-theme.css
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/css/site.css
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/collapsed.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/h3.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/newwindow.png
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/logo_maven.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/maven-logo-2.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/banner.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/apache-maven-project-2.png
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/icon_info_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/expanded.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/external.png
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/logo_apache.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/icon_success_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/breadcrumbs.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/logos/
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/logos/build-by-maven-white.png
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/logos/build-by-maven-black.png
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/logos/maven-feather.png
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/h5.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/icon_warning_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/icon_error_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/images/bg.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/dependency-analysis.html
hadoop-2.6.0/share/doc/hadoop/hadoop-extras/project-reports.html
hadoop-2.6.0/share/doc/hadoop/css/
hadoop-2.6.0/share/doc/hadoop/css/maven-base.css
hadoop-2.6.0/share/doc/hadoop/css/print.css
hadoop-2.6.0/share/doc/hadoop/css/maven-theme.css
hadoop-2.6.0/share/doc/hadoop/css/site.css
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/css/
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/css/maven-base.css
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/css/print.css
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/css/maven-theme.css
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/css/site.css
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/collapsed.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/h3.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/newwindow.png
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/logo_maven.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/maven-logo-2.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/banner.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/apache-maven-project-2.png
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/icon_info_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/expanded.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/external.png
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/logo_apache.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/icon_success_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/breadcrumbs.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/logos/
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/logos/build-by-maven-white.png
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/logos/build-by-maven-black.png
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/logos/maven-feather.png
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/h5.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/icon_warning_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/icon_error_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/images/bg.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/dependency-analysis.html
hadoop-2.6.0/share/doc/hadoop/hadoop-assemblies/project-reports.html
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/css/
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/css/maven-base.css
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/css/print.css
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/css/maven-theme.css
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/css/site.css
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/collapsed.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/h3.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/newwindow.png
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/logo_maven.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/maven-logo-2.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/banner.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/apache-maven-project-2.png
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/icon_info_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/expanded.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/external.png
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/logo_apache.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/icon_success_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/breadcrumbs.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/logos/
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/logos/build-by-maven-white.png
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/logos/build-by-maven-black.png
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/logos/maven-feather.png
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/h5.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/icon_warning_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/icon_error_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/images/bg.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/dependency-analysis.html
hadoop-2.6.0/share/doc/hadoop/hadoop-pipes/project-reports.html
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/css/
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/css/maven-base.css
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/css/print.css
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/css/maven-theme.css
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/css/site.css
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/collapsed.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/h3.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/newwindow.png
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/logo_maven.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/maven-logo-2.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/banner.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/apache-maven-project-2.png
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/icon_info_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/expanded.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/external.png
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/logo_apache.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/icon_success_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/breadcrumbs.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/logos/
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/logos/build-by-maven-white.png
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/logos/build-by-maven-black.png
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/logos/maven-feather.png
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/h5.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/icon_warning_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/icon_error_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/images/bg.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/dependency-analysis.html
hadoop-2.6.0/share/doc/hadoop/hadoop-auth-examples/project-reports.html
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/css/
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/css/maven-base.css
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/css/print.css
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/css/maven-theme.css
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/css/site.css
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/collapsed.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/h3.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/newwindow.png
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/logo_maven.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/maven-logo-2.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/banner.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/apache-maven-project-2.png
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/icon_info_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/expanded.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/external.png
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/logo_apache.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/icon_success_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/breadcrumbs.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/logos/
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/logos/build-by-maven-white.png
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/logos/build-by-maven-black.png
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/logos/maven-feather.png
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/h5.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/icon_warning_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/icon_error_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/images/bg.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/dependency-analysis.html
hadoop-2.6.0/share/doc/hadoop/hadoop-nfs/project-reports.html
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/css/
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/css/maven-base.css
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/css/print.css
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/css/maven-theme.css
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/css/site.css
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/collapsed.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/h3.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/newwindow.png
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/logo_maven.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/maven-logo-2.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/banner.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/apache-maven-project-2.png
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/icon_info_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/expanded.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/external.png
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/logo_apache.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/icon_success_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/breadcrumbs.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/logos/
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/logos/build-by-maven-white.png
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/logos/build-by-maven-black.png
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/logos/maven-feather.png
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/h5.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/icon_warning_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/icon_error_sml.gif
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/images/bg.jpg
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/dependency-analysis.html
hadoop-2.6.0/share/doc/hadoop/hadoop-ant/project-reports.html
hadoop-2.6.0/share/doc/hadoop/hdfs/
hadoop-2.6.0/share/doc/hadoop/hdfs/CHANGES.txt

gzip: stdin: unexpected end of file
tar: Unexpected EOF in archive
tar: Unexpected EOF in archive
tar: Error is not recoverable: exiting now
hduser@DESKTOP-I4C3SAV:~$ cd hadoop-2.6.0
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ sudo mv * /usr/local/hadoop
[sudo] password for hduser:
hduser is not in the sudoers file.
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ su k
su: user k does not exist or the user entry does not contain all the required fields
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ su
Password:
su: Authentication failure
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ su shek
Password:
bash: /home/shek/.bashrc: line 120: syntax error near unexpected token `('
bash: /home/shek/.bashrc: line 120: `export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Program Files (x86)/VMware/VMware Player/bin/:/mnt/c/Users/Welcome/AppData/Local/Programs/Python/Python312:/mnt/c/Users/Welcome/AppData/Local/Programs/Python/Python312/Scripts:/mnt/c/Program Files/OpenSSH/OpenSSH-Win64:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files/nodejs:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/PuTTY/:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/nodejs/:/mnt/d/MPI/Bin:/mnt/c/Program Files/mosquitto/:/mnt/c/Users/Welcome/AppData/Local/Google/Cloud SDK/google-cloud-sdk/bin:/mnt/c/sqlite:/Docker/host/bin:/mnt/c/ProgramData/chocolatey/lib/scala/tools/scala/3.6.4:/mnt/c/Users/Welcome/Downloads/go1.24.1.windows-amd64.msi:/mnt/c/Users/Welcome/Downloads/langchain-graphdb-qa-chain-demo-main/langchain-graphdb-qa-chain-demo-main/Scripts:/mnt/c/Program Files/Java/jdk-24/bin:/mnt/d/R-4.5.1/bin/:/mnt/c/Java/jdk-1.8/bin:/mnt/e/hadoop/bin:/mnt/e/hadoop/sbin:/mnt/d/Python310/Scripts/:/mnt/d/Python310/:/mnt/c/Users/Welcome/AppData/Local/Programs/Python/Launcher/:/mnt/c/Users/Welcome/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Welcome/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/ghcup/bin:/mnt/c/Users/Welcome/AppData/Roaming/cabal/bin:/mnt/c/tools/msys64:/mnt/c/tools/ghc-9.8.2/bin:/mnt/c/Users/Welcome/AppData/Local/Microsoft/WindowsApps:/mnt/d/flutter/bin:/mnt/c/Users/Welcome/AppData/Roaming/npm:/mnt/c/Users/Welcome/Downloads/get-pip.py:/mnt/c/msys64/mingw64/bin:/snap/bin:/usr/local/go/bin:/bin'
shek@DESKTOP-I4C3SAV:/home/hduser/hadoop-2.6.0$ sudo adduser hduser sudo
[sudo] password for shek:
info: Adding user `hduser' to group `sudo' ...
shek@DESKTOP-I4C3SAV:/home/hduser/hadoop-2.6.0$ cd ..
bash: cd: ..: Permission denied
shek@DESKTOP-I4C3SAV:/home/hduser/hadoop-2.6.0$ cd.
Command 'cd.' not found, did you mean:
  command 'cdo' from deb cdo (2.3.0-1)
  command 'cdb' from deb tinycdb (0.81-1)
  command 'cd5' from deb cd5 (0.1-4)
  command 'cdp' from deb irpas (0.10-9)
  command 'cdw' from deb cdw (0.8.1-3)
  command 'cde' from deb cde (0.1+git9-g551e54d-1.2)
  command 'cdi' from deb cdo (2.3.0-1)
Try: sudo apt install <deb name>
shek@DESKTOP-I4C3SAV:/home/hduser/hadoop-2.6.0$ cd ..
bash: cd: ..: Permission denied
shek@DESKTOP-I4C3SAV:/home/hduser/hadoop-2.6.0$ su cd..
su: user cd.. does not exist or the user entry does not contain all the required fields
shek@DESKTOP-I4C3SAV:/home/hduser/hadoop-2.6.0$ su cd ..
su: user cd does not exist or the user entry does not contain all the required fields
shek@DESKTOP-I4C3SAV:/home/hduser/hadoop-2.6.0$ cd /home/hduser
bash: cd: /home/hduser: Permission denied
shek@DESKTOP-I4C3SAV:/home/hduser/hadoop-2.6.0$ sudo -i
root@DESKTOP-I4C3SAV:~# cd /home
root@DESKTOP-I4C3SAV:/home# su - hduser
To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.

hduser@DESKTOP-I4C3SAV:~$ su shek
Password:
bash: /home/shek/.bashrc: line 120: syntax error near unexpected token `('
bash: /home/shek/.bashrc: line 120: `export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Program Files (x86)/VMware/VMware Player/bin/:/mnt/c/Users/Welcome/AppData/Local/Programs/Python/Python312:/mnt/c/Users/Welcome/AppData/Local/Programs/Python/Python312/Scripts:/mnt/c/Program Files/OpenSSH/OpenSSH-Win64:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files/nodejs:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/PuTTY/:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/nodejs/:/mnt/d/MPI/Bin:/mnt/c/Program Files/mosquitto/:/mnt/c/Users/Welcome/AppData/Local/Google/Cloud SDK/google-cloud-sdk/bin:/mnt/c/sqlite:/Docker/host/bin:/mnt/c/ProgramData/chocolatey/lib/scala/tools/scala/3.6.4:/mnt/c/Users/Welcome/Downloads/go1.24.1.windows-amd64.msi:/mnt/c/Users/Welcome/Downloads/langchain-graphdb-qa-chain-demo-main/langchain-graphdb-qa-chain-demo-main/Scripts:/mnt/c/Program Files/Java/jdk-24/bin:/mnt/d/R-4.5.1/bin/:/mnt/c/Java/jdk-1.8/bin:/mnt/e/hadoop/bin:/mnt/e/hadoop/sbin:/mnt/d/Python310/Scripts/:/mnt/d/Python310/:/mnt/c/Users/Welcome/AppData/Local/Programs/Python/Launcher/:/mnt/c/Users/Welcome/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Welcome/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/ghcup/bin:/mnt/c/Users/Welcome/AppData/Roaming/cabal/bin:/mnt/c/tools/msys64:/mnt/c/tools/ghc-9.8.2/bin:/mnt/c/Users/Welcome/AppData/Local/Microsoft/WindowsApps:/mnt/d/flutter/bin:/mnt/c/Users/Welcome/AppData/Roaming/npm:/mnt/c/Users/Welcome/Downloads/get-pip.py:/mnt/c/msys64/mingw64/bin:/snap/bin:/usr/local/go/bin:/bin'
shek@DESKTOP-I4C3SAV:/home/hduser$ sudo adduser hduser sudo
[sudo] password for shek:
info: The user `hduser' is already a member of `sudo'.
shek@DESKTOP-I4C3SAV:/home/hduser$ sudo su hduser
To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.

hduser@DESKTOP-I4C3SAV:~$ su - hduser
Password:
To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.

hduser@DESKTOP-I4C3SAV:~$ cd hadoop-2.6.0
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ sudo mv * /usr/local/hadoop
[sudo] password for hduser:
mv: cannot overwrite '/usr/local/hadoop/etc': Directory not empty
mv: cannot overwrite '/usr/local/hadoop/lib': Directory not empty
mv: cannot overwrite '/usr/local/hadoop/libexec': Directory not empty
mv: cannot overwrite '/usr/local/hadoop/sbin': Directory not empty
mv: cannot overwrite '/usr/local/hadoop/share': Directory not empty
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ sudo mv * /usr/local/hadoop
[sudo] password for hduser:
mv: cannot overwrite '/usr/local/hadoop/etc': Directory not empty
mv: cannot overwrite '/usr/local/hadoop/lib': Directory not empty
mv: cannot overwrite '/usr/local/hadoop/libexec': Directory not empty
mv: cannot overwrite '/usr/local/hadoop/sbin': Directory not empty
mv: cannot overwrite '/usr/local/hadoop/share': Directory not empty
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ sudo chown -R hduser:hadoop /usr/local/hadoop
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ vi ~/.bashrc
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ update-alternatives --config java
There are 2 choices for the alternative java (providing /usr/bin/java).

  Selection    Path                                         Priority   Status
------------------------------------------------------------
* 0            /usr/lib/jvm/java-21-openjdk-amd64/bin/java   2111      auto mode
  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java   1111      manual mode
  2            /usr/lib/jvm/java-21-openjdk-amd64/bin/java   2111      manual mode

Press <enter> to keep the current choice[*], or type selection number:
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ update-alternatives --config java
There are 2 choices for the alternative java (providing /usr/bin/java).

  Selection    Path                                         Priority   Status
------------------------------------------------------------
* 0            /usr/lib/jvm/java-21-openjdk-amd64/bin/java   2111      auto mode
  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java   1111      manual mode
  2            /usr/lib/jvm/java-21-openjdk-amd64/bin/java   2111      manual mode

Press <enter> to keep the current choice[*], or type selection number: 0
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ update-alternatives --config java
There are 2 choices for the alternative java (providing /usr/bin/java).

  Selection    Path                                         Priority   Status
------------------------------------------------------------
* 0            /usr/lib/jvm/java-21-openjdk-amd64/bin/java   2111      auto mode
  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java   1111      manual mode
  2            /usr/lib/jvm/java-21-openjdk-amd64/bin/java   2111      manual mode

Press <enter> to keep the current choice[*], or type selection number:
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ vi ~/.bashrc
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ vi ~/.bashrc
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ vi ~/.bashrc
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ vi ~/.bashrc
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ source ~/.bashrc
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ javac -version
javac 21.0.8
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ readlink -f /usr/bin/javac
/usr/lib/jvm/java-21-openjdk-amd64/bin/javac
hduser@DESKTOP-I4C3SAV:~/hadoop-2.6.0$ cd ..
hduser@DESKTOP-I4C3SAV:~$ vi /usr/local/hadoop/etc/hadoop/hadoop-env.sh
hduser@DESKTOP-I4C3SAV:~$ sudo mkdir -p /app/hadoop/tmp
hduser@DESKTOP-I4C3SAV:~$ sudo chown hduser:hadoop /app/hadoop/tmp
hduser@DESKTOP-I4C3SAV:~$ vi /usr/local/hadoop/etc/hadoop/core-site.xml
hduser@DESKTOP-I4C3SAV:~$ cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template/usr/local/hadoop/etc/hadoop/mapred-site.xml
cp: missing destination file operand after '/usr/local/hadoop/etc/hadoop/mapred-site.xml.template/usr/local/hadoop/etc/hadoop/mapred-site.xml'
Try 'cp --help' for more information.
hduser@DESKTOP-I4C3SAV:~$ cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template
/usr/local/hadoop/etc/hadoop/mapred-site.xml
cp: missing destination file operand after '/usr/local/hadoop/etc/hadoop/mapred-site.xml.template'
Try 'cp --help' for more information.
-bash: /usr/local/hadoop/etc/hadoop/mapred-site.xml: Permission denied
hduser@DESKTOP-I4C3SAV:~$ cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template
/usr/local/hadoop/etc/hadoop/mapred-site.xml
cp: missing destination file operand after '/usr/local/hadoop/etc/hadoop/mapred-site.xml.template'
Try 'cp --help' for more information.
-bash: /usr/local/hadoop/etc/hadoop/mapred-site.xml: Permission denied
hduser@DESKTOP-I4C3SAV:~$ sudo cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml
cp: cannot stat '/usr/local/hadoop/etc/hadoop/mapred-site.xml.template': No such file or directory
hduser@DESKTOP-I4C3SAV:~$ ls -l /usr/local/hadoop/etc/hadoop/
total 180
-rw-r--r-- 1 hduser hadoop  9213 Jun 18  2023 capacity-scheduler.xml
-rw-r--r-- 1 hduser hadoop  1335 Jun 18  2023 configuration.xsl
-rw-r--r-- 1 hduser hadoop  2567 Jun 18  2023 container-executor.cfg
-rw-r--r-- 1 hduser hadoop  1340 Oct 10 12:34 core-site.xml
-rw-r--r-- 1 hduser hadoop  3999 Jun 18  2023 hadoop-env.cmd
-rw-r--r-- 1 hduser hadoop 16687 Oct 10 12:33 hadoop-env.sh
-rw-r--r-- 1 hduser hadoop  3321 Jun 18  2023 hadoop-metrics2.properties
-rw-r--r-- 1 hduser hadoop 11765 Jun 18  2023 hadoop-policy.xml
-rw-r--r-- 1 hduser hadoop  3414 Jun 18  2023 hadoop-user-functions.sh.example
-rw-r--r-- 1 hduser hadoop   683 Jun 18  2023 hdfs-rbf-site.xml
-rw-r--r-- 1 hduser hadoop   775 Jun 18  2023 hdfs-site.xml
-rw-r--r-- 1 hduser hadoop  1484 Jun 18  2023 httpfs-env.sh
-rw-r--r-- 1 hduser hadoop  1657 Jun 18  2023 httpfs-log4j.properties
-rw-r--r-- 1 hduser hadoop   620 Jun 18  2023 httpfs-site.xml
-rw-r--r-- 1 hduser hadoop  3518 Jun 18  2023 kms-acls.xml
-rw-r--r-- 1 hduser hadoop  1351 Jun 18  2023 kms-env.sh
-rw-r--r-- 1 hduser hadoop  1860 Jun 18  2023 kms-log4j.properties
-rw-r--r-- 1 hduser hadoop   682 Jun 18  2023 kms-site.xml
-rw-r--r-- 1 hduser hadoop 13700 Jun 18  2023 log4j.properties
-rw-r--r-- 1 hduser hadoop   951 Jun 18  2023 mapred-env.cmd
-rw-r--r-- 1 hduser hadoop  1764 Jun 18  2023 mapred-env.sh
-rw-r--r-- 1 hduser hadoop  4113 Jun 18  2023 mapred-queues.xml.template
-rw-r--r-- 1 hduser hadoop   758 Jun 18  2023 mapred-site.xml
drwxr-xr-x 2 hduser hadoop  4096 Jun 18  2023 shellprofile.d
-rw-r--r-- 1 hduser hadoop  2316 Jun 18  2023 ssl-client.xml.example
-rw-r--r-- 1 hduser hadoop  2697 Jun 18  2023 ssl-server.xml.example
-rw-r--r-- 1 hduser hadoop  2681 Jun 18  2023 user_ec_policies.xml.template
-rw-r--r-- 1 hduser hadoop    10 Jun 18  2023 workers
-rw-r--r-- 1 hduser hadoop  2250 Jun 18  2023 yarn-env.cmd
-rw-r--r-- 1 hduser hadoop  6329 Jun 18  2023 yarn-env.sh
-rw-r--r-- 1 hduser hadoop   690 Jun 18  2023 yarn-site.xml
-rw-r--r-- 1 hduser hadoop  2591 Jun 18  2023 yarnservice-log4j.properties
hduser@DESKTOP-I4C3SAV:~$ nano /usr/local/hadoop/etc/hadoop/mapred-site.xml
hduser@DESKTOP-I4C3SAV:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
hduser@DESKTOP-I4C3SAV:~$ sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode
hduser@DESKTOP-I4C3SAV:~$ sudo chown -R hduser:hadoop /usr/local/hadoop_store
hduser@DESKTOP-I4C3SAV:~$ vi /usr/local/hadoop/etc/hadoop/hdfs-site.xml
hduser@DESKTOP-I4C3SAV:~$ hadoop namenode -format
ERROR: JAVA_HOME /usr/lib/jvm/java-7-openjdk-amd64 does not exist.
hduser@DESKTOP-I4C3SAV:~$ vi /usr/local/hadoop/etc/hadoop/hdfs-site.xml
hduser@DESKTOP-I4C3SAV:~$ nano /usr/local/hadoop/etc/hadoop/mapred-site.xml
hduser@DESKTOP-I4C3SAV:~$ vi /usr/local/hadoop/etc/hadoop/core-site.xml
hduser@DESKTOP-I4C3SAV:~$ hadoop namenode -format
ERROR: JAVA_HOME /usr/lib/jvm/java-7-openjdk-amd64 does not exist.
hduser@DESKTOP-I4C3SAV:~$ java -version
openjdk version "21.0.8" 2025-07-15
OpenJDK Runtime Environment (build 21.0.8+9-Ubuntu-0ubuntu124.04.1)
OpenJDK 64-Bit Server VM (build 21.0.8+9-Ubuntu-0ubuntu124.04.1, mixed mode, sharing)
hduser@DESKTOP-I4C3SAV:~$ nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh
hduser@DESKTOP-I4C3SAV:~$ source /usr/local/hadoop/etc/hadoop/hadoop-env.sh
hduser@DESKTOP-I4C3SAV:~$ hadoop namenode -format
WARNING: Use of this script to execute namenode is deprecated.
WARNING: Attempting to execute replacement "hdfs namenode" instead.

WARNING: /usr/local/hadoop/logs does not exist. Creating.
2025-10-10 12:42:26,696 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = DESKTOP-I4C3SAV/127.0.1.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.3.6
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z
STARTUP_MSG:   java = 21.0.8
************************************************************/
2025-10-10 12:42:26,708 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2025-10-10 12:42:26,787 ERROR conf.Configuration: error parsing conf hdfs-site.xml
com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
        at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
        at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:775)
        at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:759)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1846)
2025-10-10 12:42:26,789 ERROR namenode.NameNode: Failed to start namenode.
java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
        at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
        at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:775)
        at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:759)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1846)
Caused by: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        ... 12 more
2025-10-10 12:42:26,793 INFO util.ExitUtil: Exiting with status 1: java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
2025-10-10 12:42:26,817 ERROR conf.Configuration: error parsing conf hdfs-site.xml
com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
        at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
Exception in thread "Thread-1" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
        at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
Caused by: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        ... 10 more
hduser@DESKTOP-I4C3SAV:~$ hadoop namenode -format
WARNING: Use of this script to execute namenode is deprecated.
WARNING: Attempting to execute replacement "hdfs namenode" instead.

2025-10-10 12:42:54,860 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = DESKTOP-I4C3SAV/127.0.1.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.3.6
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z
STARTUP_MSG:   java = 21.0.8
************************************************************/
2025-10-10 12:42:54,867 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2025-10-10 12:42:54,943 ERROR conf.Configuration: error parsing conf hdfs-site.xml
com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
        at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
        at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:775)
        at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:759)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1846)
2025-10-10 12:42:54,945 ERROR namenode.NameNode: Failed to start namenode.
java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
        at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
        at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:775)
        at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:759)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1846)
Caused by: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        ... 12 more
2025-10-10 12:42:54,947 INFO util.ExitUtil: Exiting with status 1: java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
2025-10-10 12:42:54,973 ERROR conf.Configuration: error parsing conf hdfs-site.xml
com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
        at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
Exception in thread "Thread-1" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
        at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
Caused by: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        ... 10 more
hduser@DESKTOP-I4C3SAV:~$ su shek
Password:
bash: /home/shek/.bashrc: line 120: syntax error near unexpected token `('
bash: /home/shek/.bashrc: line 120: `export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Program Files (x86)/VMware/VMware Player/bin/:/mnt/c/Users/Welcome/AppData/Local/Programs/Python/Python312:/mnt/c/Users/Welcome/AppData/Local/Programs/Python/Python312/Scripts:/mnt/c/Program Files/OpenSSH/OpenSSH-Win64:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files/nodejs:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/PuTTY/:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/nodejs/:/mnt/d/MPI/Bin:/mnt/c/Program Files/mosquitto/:/mnt/c/Users/Welcome/AppData/Local/Google/Cloud SDK/google-cloud-sdk/bin:/mnt/c/sqlite:/Docker/host/bin:/mnt/c/ProgramData/chocolatey/lib/scala/tools/scala/3.6.4:/mnt/c/Users/Welcome/Downloads/go1.24.1.windows-amd64.msi:/mnt/c/Users/Welcome/Downloads/langchain-graphdb-qa-chain-demo-main/langchain-graphdb-qa-chain-demo-main/Scripts:/mnt/c/Program Files/Java/jdk-24/bin:/mnt/d/R-4.5.1/bin/:/mnt/c/Java/jdk-1.8/bin:/mnt/e/hadoop/bin:/mnt/e/hadoop/sbin:/mnt/d/Python310/Scripts/:/mnt/d/Python310/:/mnt/c/Users/Welcome/AppData/Local/Programs/Python/Launcher/:/mnt/c/Users/Welcome/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Welcome/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/ghcup/bin:/mnt/c/Users/Welcome/AppData/Roaming/cabal/bin:/mnt/c/tools/msys64:/mnt/c/tools/ghc-9.8.2/bin:/mnt/c/Users/Welcome/AppData/Local/Microsoft/WindowsApps:/mnt/d/flutter/bin:/mnt/c/Users/Welcome/AppData/Roaming/npm:/mnt/c/Users/Welcome/Downloads/get-pip.py:/mnt/c/msys64/mingw64/bin:/snap/bin:/usr/local/go/bin:/bin'
shek@DESKTOP-I4C3SAV:/home/hduser$ cd ..
shek@DESKTOP-I4C3SAV:/home$ cd ..
shek@DESKTOP-I4C3SAV:/$ cd ~
shek@DESKTOP-I4C3SAV:~$ cd /usr/local/hadoop/sbin
shek@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ ls
FederationStateStore   httpfs.sh                start-all.cmd      start-dfs.sh         stop-all.cmd      stop-dfs.sh         workers.sh
distribute-exclude.sh  kms.sh                   start-all.sh       start-secure-dns.sh  stop-all.sh       stop-secure-dns.sh  yarn-daemon.sh
hadoop-daemon.sh       mr-jobhistory-daemon.sh  start-balancer.sh  start-yarn.cmd       stop-balancer.sh  stop-yarn.cmd       yarn-daemons.sh
hadoop-daemons.sh      refresh-namenodes.sh     start-dfs.cmd      start-yarn.sh        stop-dfs.cmd      stop-yarn.sh
shek@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo su hduser
[sudo] password for shek:
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-all.sh
WARNING: Attempting to start all Apache Hadoop daemons as hduser in 10 seconds.
WARNING: This is not a recommended production deployment configuration.
WARNING: Use CTRL-C to abort.
^C
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ cd ..
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop$ cd..
cd..: command not found
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop$ cd ..
hduser@DESKTOP-I4C3SAV:/usr/local$ cd ..
hduser@DESKTOP-I4C3SAV:/usr$ cd ..
hduser@DESKTOP-I4C3SAV:/$ cd ~
hduser@DESKTOP-I4C3SAV:~$ start-all.sh
WARNING: Attempting to start all Apache Hadoop daemons as hduser in 10 seconds.
WARNING: This is not a recommended production deployment configuration.
WARNING: Use CTRL-C to abort.
Starting namenodes on [DESKTOP-I4C3SAV]
DESKTOP-I4C3SAV: Warning: Permanently added 'desktop-i4c3sav' (ED25519) to the list of known hosts.
DESKTOP-I4C3SAV: ERROR: Cannot set priority of namenode process 2576
Starting datanodes
localhost: ERROR: Cannot set priority of datanode process 2683
2025-10-10 12:45:06,399 ERROR conf.Configuration: error parsing conf hdfs-site.xml
com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1413)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1385)
        at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1727)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:348)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:581)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:182)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:164)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
        at org.apache.hadoop.hdfs.tools.GetConf.main(GetConf.java:361)
Exception in thread "main" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1413)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1385)
        at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1727)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:348)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:581)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:182)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:164)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
        at org.apache.hadoop.hdfs.tools.GetConf.main(GetConf.java:361)
Caused by: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        ... 13 more
Starting resourcemanagers on []
Starting nodemanagers
hduser@DESKTOP-I4C3SAV:~$ cd /usr/local/hadoop/sbin
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
3288 Jps
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ cd ..
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop$ cd ..
hduser@DESKTOP-I4C3SAV:/usr/local$ cd ..
hduser@DESKTOP-I4C3SAV:/usr$ cd ..
hduser@DESKTOP-I4C3SAV:/$ cd ~
hduser@DESKTOP-I4C3SAV:~$ jps
3307 Jps
hduser@DESKTOP-I4C3SAV:~$ cd /usr/local/hadoop/sbin
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
3329 Jps
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ echo $HADOOP_HOME

hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ echo $JAVA_HOME
/usr/lib/jvm/java-21-openjdk-amd64
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ echo $PATH
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ export HADOOP_HOME=/usr/local/hadoop
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ echo $HADOOP_HOME
/usr/local/hadoop
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano ~/.bashrc
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ source ~/.bashrc
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-dfs.sh
Starting namenodes on [DESKTOP-I4C3SAV]
DESKTOP-I4C3SAV: ERROR: Cannot set priority of namenode process 3507
Starting datanodes
localhost: ERROR: Cannot set priority of datanode process 3618
2025-10-10 12:49:01,337 ERROR conf.Configuration: error parsing conf hdfs-site.xml
com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1413)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1385)
        at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1727)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:348)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:581)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:182)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:164)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
        at org.apache.hadoop.hdfs.tools.GetConf.main(GetConf.java:361)
Exception in thread "main" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1413)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1385)
        at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1727)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:348)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:581)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:182)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:164)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
        at org.apache.hadoop.hdfs.tools.GetConf.main(GetConf.java:361)
Caused by: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        ... 13 more
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-yarn.sh
Starting resourcemanagers on []
Starting nodemanagers
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
4253 Jps
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo apt install openjdk-8-jdk
[sudo] password for hduser:
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages were automatically installed and are no longer required:
  hwloc-nox libmpich12 libslurm40t64
Use 'sudo apt autoremove' to remove them.
The following additional packages will be installed:
  libasyncns0 libflac12t64 libgail-common libgail18t64 libgtk2.0-0t64 libgtk2.0-bin libgtk2.0-common libmp3lame0 libmpg123-0t64 libogg0 libopus0 libpulse0
  libsndfile1 libvorbis0a libvorbisenc2 openjdk-8-jdk-headless openjdk-8-jre openjdk-8-jre-headless
Suggested packages:
  gvfs opus-tools pulseaudio openjdk-8-demo openjdk-8-source visualvm libnss-mdns fonts-nanum fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei
  fonts-wqy-zenhei fonts-indic
The following NEW packages will be installed:
  libasyncns0 libflac12t64 libgail-common libgail18t64 libgtk2.0-0t64 libgtk2.0-bin libgtk2.0-common libmp3lame0 libmpg123-0t64 libogg0 libopus0 libpulse0
  libsndfile1 libvorbis0a libvorbisenc2 openjdk-8-jdk openjdk-8-jdk-headless openjdk-8-jre openjdk-8-jre-headless
0 upgraded, 19 newly installed, 0 to remove and 125 not upgraded.
Need to get 47.4 MB of archives.
After this operation, 159 MB of additional disk space will be used.
Do you want to continue? [Y/n] Y
Get:1 http://archive.ubuntu.com/ubuntu noble/main amd64 libasyncns0 amd64 0.8-6build4 [11.3 kB]
Get:2 http://archive.ubuntu.com/ubuntu noble/main amd64 libogg0 amd64 1.3.5-3build1 [22.7 kB]
Get:3 http://archive.ubuntu.com/ubuntu noble/main amd64 libflac12t64 amd64 1.4.3+ds-2.1ubuntu2 [197 kB]
Get:4 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libgtk2.0-common all 2.24.33-4ubuntu1.1 [127 kB]
Get:5 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libgtk2.0-0t64 amd64 2.24.33-4ubuntu1.1 [2006 kB]
Get:6 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libgail18t64 amd64 2.24.33-4ubuntu1.1 [15.9 kB]
Get:7 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libgail-common amd64 2.24.33-4ubuntu1.1 [126 kB]
Get:8 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libgtk2.0-bin amd64 2.24.33-4ubuntu1.1 [7954 B]
Get:9 http://archive.ubuntu.com/ubuntu noble/main amd64 libmp3lame0 amd64 3.100-6build1 [142 kB]
Get:10 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libmpg123-0t64 amd64 1.32.5-1ubuntu1.1 [169 kB]
Get:11 http://archive.ubuntu.com/ubuntu noble/main amd64 libopus0 amd64 1.4-1build1 [208 kB]
Get:12 http://archive.ubuntu.com/ubuntu noble/main amd64 libvorbis0a amd64 1.3.7-1build3 [97.6 kB]
Get:13 http://archive.ubuntu.com/ubuntu noble/main amd64 libvorbisenc2 amd64 1.3.7-1build3 [80.8 kB]
Get:14 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libsndfile1 amd64 1.2.2-1ubuntu5.24.04.1 [209 kB]
Get:15 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libpulse0 amd64 1:16.1+dfsg1-2ubuntu10.1 [292 kB]
Get:16 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 openjdk-8-jre-headless amd64 8u462-ga~us1-0ubuntu2~24.04.2 [30.7 MB]
Get:17 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 openjdk-8-jre amd64 8u462-ga~us1-0ubuntu2~24.04.2 [74.0 kB]
Get:18 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 openjdk-8-jdk-headless amd64 8u462-ga~us1-0ubuntu2~24.04.2 [8825 kB]
Get:19 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 openjdk-8-jdk amd64 8u462-ga~us1-0ubuntu2~24.04.2 [4052 kB]
Fetched 47.4 MB in 1min 13s (645 kB/s)
Selecting previously unselected package libasyncns0:amd64.
(Reading database ... 67819 files and directories currently installed.)
Preparing to unpack .../00-libasyncns0_0.8-6build4_amd64.deb ...
Unpacking libasyncns0:amd64 (0.8-6build4) ...
Selecting previously unselected package libogg0:amd64.
Preparing to unpack .../01-libogg0_1.3.5-3build1_amd64.deb ...
Unpacking libogg0:amd64 (1.3.5-3build1) ...
Selecting previously unselected package libflac12t64:amd64.
Preparing to unpack .../02-libflac12t64_1.4.3+ds-2.1ubuntu2_amd64.deb ...
Unpacking libflac12t64:amd64 (1.4.3+ds-2.1ubuntu2) ...
Selecting previously unselected package libgtk2.0-common.
Preparing to unpack .../03-libgtk2.0-common_2.24.33-4ubuntu1.1_all.deb ...
Unpacking libgtk2.0-common (2.24.33-4ubuntu1.1) ...
Selecting previously unselected package libgtk2.0-0t64:amd64.
Preparing to unpack .../04-libgtk2.0-0t64_2.24.33-4ubuntu1.1_amd64.deb ...
Unpacking libgtk2.0-0t64:amd64 (2.24.33-4ubuntu1.1) ...
Selecting previously unselected package libgail18t64:amd64.
Preparing to unpack .../05-libgail18t64_2.24.33-4ubuntu1.1_amd64.deb ...
Unpacking libgail18t64:amd64 (2.24.33-4ubuntu1.1) ...
Selecting previously unselected package libgail-common:amd64.
Preparing to unpack .../06-libgail-common_2.24.33-4ubuntu1.1_amd64.deb ...
Unpacking libgail-common:amd64 (2.24.33-4ubuntu1.1) ...
Selecting previously unselected package libgtk2.0-bin.
Preparing to unpack .../07-libgtk2.0-bin_2.24.33-4ubuntu1.1_amd64.deb ...
Unpacking libgtk2.0-bin (2.24.33-4ubuntu1.1) ...
Selecting previously unselected package libmp3lame0:amd64.
Preparing to unpack .../08-libmp3lame0_3.100-6build1_amd64.deb ...
Unpacking libmp3lame0:amd64 (3.100-6build1) ...
Selecting previously unselected package libmpg123-0t64:amd64.
Preparing to unpack .../09-libmpg123-0t64_1.32.5-1ubuntu1.1_amd64.deb ...
Unpacking libmpg123-0t64:amd64 (1.32.5-1ubuntu1.1) ...
Selecting previously unselected package libopus0:amd64.
Preparing to unpack .../10-libopus0_1.4-1build1_amd64.deb ...
Unpacking libopus0:amd64 (1.4-1build1) ...
Selecting previously unselected package libvorbis0a:amd64.
Preparing to unpack .../11-libvorbis0a_1.3.7-1build3_amd64.deb ...
Unpacking libvorbis0a:amd64 (1.3.7-1build3) ...
Selecting previously unselected package libvorbisenc2:amd64.
Preparing to unpack .../12-libvorbisenc2_1.3.7-1build3_amd64.deb ...
Unpacking libvorbisenc2:amd64 (1.3.7-1build3) ...
Selecting previously unselected package libsndfile1:amd64.
Preparing to unpack .../13-libsndfile1_1.2.2-1ubuntu5.24.04.1_amd64.deb ...
Unpacking libsndfile1:amd64 (1.2.2-1ubuntu5.24.04.1) ...
Selecting previously unselected package libpulse0:amd64.
Preparing to unpack .../14-libpulse0_1%3a16.1+dfsg1-2ubuntu10.1_amd64.deb ...
Unpacking libpulse0:amd64 (1:16.1+dfsg1-2ubuntu10.1) ...
Selecting previously unselected package openjdk-8-jre-headless:amd64.
Preparing to unpack .../15-openjdk-8-jre-headless_8u462-ga~us1-0ubuntu2~24.04.2_amd64.deb ...
Unpacking openjdk-8-jre-headless:amd64 (8u462-ga~us1-0ubuntu2~24.04.2) ...
Selecting previously unselected package openjdk-8-jre:amd64.
Preparing to unpack .../16-openjdk-8-jre_8u462-ga~us1-0ubuntu2~24.04.2_amd64.deb ...
Unpacking openjdk-8-jre:amd64 (8u462-ga~us1-0ubuntu2~24.04.2) ...
Selecting previously unselected package openjdk-8-jdk-headless:amd64.
Preparing to unpack .../17-openjdk-8-jdk-headless_8u462-ga~us1-0ubuntu2~24.04.2_amd64.deb ...
Unpacking openjdk-8-jdk-headless:amd64 (8u462-ga~us1-0ubuntu2~24.04.2) ...
Selecting previously unselected package openjdk-8-jdk:amd64.
Preparing to unpack .../18-openjdk-8-jdk_8u462-ga~us1-0ubuntu2~24.04.2_amd64.deb ...
Unpacking openjdk-8-jdk:amd64 (8u462-ga~us1-0ubuntu2~24.04.2) ...
Setting up libogg0:amd64 (1.3.5-3build1) ...
Setting up libmpg123-0t64:amd64 (1.32.5-1ubuntu1.1) ...
Setting up libopus0:amd64 (1.4-1build1) ...
Setting up openjdk-8-jre-headless:amd64 (8u462-ga~us1-0ubuntu2~24.04.2) ...
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode
Setting up libvorbis0a:amd64 (1.3.7-1build3) ...
Setting up libasyncns0:amd64 (0.8-6build4) ...
Setting up libgtk2.0-common (2.24.33-4ubuntu1.1) ...
Setting up libflac12t64:amd64 (1.4.3+ds-2.1ubuntu2) ...
Setting up libmp3lame0:amd64 (3.100-6build1) ...
Setting up libvorbisenc2:amd64 (1.3.7-1build3) ...
Setting up libgtk2.0-0t64:amd64 (2.24.33-4ubuntu1.1) ...
Setting up libgail18t64:amd64 (2.24.33-4ubuntu1.1) ...
Setting up libgtk2.0-bin (2.24.33-4ubuntu1.1) ...
Setting up libsndfile1:amd64 (1.2.2-1ubuntu5.24.04.1) ...
Setting up libpulse0:amd64 (1:16.1+dfsg1-2ubuntu10.1) ...
Setting up libgail-common:amd64 (2.24.33-4ubuntu1.1) ...
Processing triggers for hicolor-icon-theme (0.17-2) ...
Processing triggers for libc-bin (2.39-0ubuntu8.6) ...
Processing triggers for man-db (2.12.0-4build2) ...
Processing triggers for ca-certificates-java (20240118) ...
done.
Setting up openjdk-8-jdk-headless:amd64 (8u462-ga~us1-0ubuntu2~24.04.2) ...
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode
Setting up openjdk-8-jre:amd64 (8u462-ga~us1-0ubuntu2~24.04.2) ...
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode
Setting up openjdk-8-jdk:amd64 (8u462-ga~us1-0ubuntu2~24.04.2) ...
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode
Processing triggers for libc-bin (2.39-0ubuntu8.6) ...
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ export HADOOP_HOME=/usr/local/hadoop
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ echo $HADOOP_HOME
/usr/local/hadoop
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano ~/.bashrc
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ java --version
openjdk 21.0.8 2025-07-15
OpenJDK Runtime Environment (build 21.0.8+9-Ubuntu-0ubuntu124.04.1)
OpenJDK 64-Bit Server VM (build 21.0.8+9-Ubuntu-0ubuntu124.04.1, mixed mode, sharing)
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ echo $JAVA_HOME
/usr/lib/jvm/java-21-openjdk-amd64
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ source ~/.bashrc
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-dfs.sh
Starting namenodes on [DESKTOP-I4C3SAV]
DESKTOP-I4C3SAV: ERROR: Cannot set priority of namenode process 4858
Starting datanodes
localhost: ERROR: Cannot set priority of datanode process 4970
2025-10-10 12:53:48,662 ERROR conf.Configuration: error parsing conf hdfs-site.xml
com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1413)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1385)
        at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1727)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:348)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:581)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:182)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:164)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
        at org.apache.hadoop.hdfs.tools.GetConf.main(GetConf.java:361)
Exception in thread "main" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1413)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1385)
        at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1727)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:348)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:581)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:182)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:164)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
        at org.apache.hadoop.hdfs.tools.GetConf.main(GetConf.java:361)
Caused by: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        ... 13 more
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-yarn.sh
Starting resourcemanagers on []
Starting nodemanagers
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
5609 Jps
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo update-alternatives --config java
There are 3 choices for the alternative java (providing /usr/bin/java).

  Selection    Path                                            Priority   Status
------------------------------------------------------------
* 0            /usr/lib/jvm/java-21-openjdk-amd64/bin/java      2111      auto mode
  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode
  2            /usr/lib/jvm/java-21-openjdk-amd64/bin/java      2111      manual mode
  3            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode

Press <enter> to keep the current choice[*], or type selection number: 3
update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ echo $JAVA_HOME
/usr/lib/jvm/java-8-openjdk-amd64
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ source ~/.bashrc
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-dfs.sh
Starting namenodes on [DESKTOP-I4C3SAV]
DESKTOP-I4C3SAV: ERROR: Cannot set priority of namenode process 5784
Starting datanodes
localhost: ERROR: Cannot set priority of datanode process 5897
2025-10-10 12:55:00,570 ERROR conf.Configuration: error parsing conf hdfs-site.xml
com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1413)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1385)
        at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1727)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:348)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:581)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:182)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:164)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
        at org.apache.hadoop.hdfs.tools.GetConf.main(GetConf.java:361)
Exception in thread "main" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1413)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1385)
        at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1727)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:348)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:581)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:182)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:164)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
        at org.apache.hadoop.hdfs.tools.GetConf.main(GetConf.java:361)
Caused by: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        ... 13 more
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano ~/.bashrc
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ source ~/.bashrc
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-dfs.sh
Starting namenodes on [DESKTOP-I4C3SAV]
DESKTOP-I4C3SAV: ERROR: Cannot set priority of namenode process 6211
Starting datanodes
localhost: ERROR: Cannot set priority of datanode process 6325
2025-10-10 12:56:12,272 ERROR conf.Configuration: error parsing conf hdfs-site.xml
com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1413)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1385)
        at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1727)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:348)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:581)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:182)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:164)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
        at org.apache.hadoop.hdfs.tools.GetConf.main(GetConf.java:361)
Exception in thread "main" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1413)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1385)
        at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1727)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:348)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:581)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:182)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:164)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
        at org.apache.hadoop.hdfs.tools.GetConf.main(GetConf.java:361)
Caused by: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        ... 13 more
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hadoop namenode -format
WARNING: Use of this script to execute namenode is deprecated.
WARNING: Attempting to execute replacement "hdfs namenode" instead.

2025-10-10 12:56:34,815 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = DESKTOP-I4C3SAV/127.0.1.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.3.6
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z
STARTUP_MSG:   java = 21.0.8
************************************************************/
2025-10-10 12:56:34,823 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2025-10-10 12:56:34,897 ERROR conf.Configuration: error parsing conf hdfs-site.xml
com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
        at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
        at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:775)
        at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:759)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1846)
2025-10-10 12:56:34,899 ERROR namenode.NameNode: Failed to start namenode.
java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
        at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
        at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:775)
        at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:759)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1846)
Caused by: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        ... 12 more
2025-10-10 12:56:34,901 INFO util.ExitUtil: Exiting with status 1: java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
2025-10-10 12:56:34,920 ERROR conf.Configuration: error parsing conf hdfs-site.xml
com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
        at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
Exception in thread "Thread-1" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1864)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1841)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
        at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)
Caused by: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        ... 10 more
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-dfs.sh
Starting namenodes on [DESKTOP-I4C3SAV]
DESKTOP-I4C3SAV: ERROR: Cannot set priority of namenode process 6691
Starting datanodes
localhost: ERROR: Cannot set priority of datanode process 6806
2025-10-10 12:56:46,021 ERROR conf.Configuration: error parsing conf hdfs-site.xml
com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1413)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1385)
        at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1727)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:348)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:581)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:182)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:164)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
        at org.apache.hadoop.hdfs.tools.GetConf.main(GetConf.java:361)
Exception in thread "main" java.lang.RuntimeException: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1413)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1385)
        at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1727)
        at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:348)
        at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:581)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:182)
        at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:164)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)
        at org.apache.hadoop.hdfs.tools.GetConf.main(GetConf.java:361)
Caused by: com.ctc.wstx.exc.WstxParsingException: Invalid XML pseudo-attribute 'version' value '0.0'; expected "1.0" or "1.1"
 at [row,col,system-id]: [1,19,"file:/usr/local/hadoop/etc/hadoop/hdfs-site.xml"]
        at com.ctc.wstx.io.InputBootstrapper.reportPseudoAttrProblem(InputBootstrapper.java:528)
        at com.ctc.wstx.io.InputBootstrapper.readXmlVersion(InputBootstrapper.java:345)
        at com.ctc.wstx.io.InputBootstrapper.readXmlDecl(InputBootstrapper.java:263)
        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:171)
        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)
        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3034)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:3018)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3114)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3072)
        ... 13 more
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ mkdir -p /usr/local/hadoop/data/namenode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ mkdir -p /usr/local/hadoop/data/datanode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo chown -R hduser:hadoop /usr/local/hadoop/data
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hadoop namenode -format
WARNING: Use of this script to execute namenode is deprecated.
WARNING: Attempting to execute replacement "hdfs namenode" instead.

2025-10-10 12:58:24,863 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = DESKTOP-I4C3SAV/127.0.1.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.3.6
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z
STARTUP_MSG:   java = 21.0.8
************************************************************/
2025-10-10 12:58:24,870 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2025-10-10 12:58:24,956 INFO namenode.NameNode: createNameNode [-format]
2025-10-10 12:58:25,110 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-10-10 12:58:25,373 INFO namenode.NameNode: Formatting using clusterid: CID-fdf3754f-0c48-4f4d-82d5-deddae89c2ca
2025-10-10 12:58:25,406 INFO namenode.FSEditLog: Edit logging is async:true
2025-10-10 12:58:25,428 INFO namenode.FSNamesystem: KeyProvider: null
2025-10-10 12:58:25,429 INFO namenode.FSNamesystem: fsLock is fair: true
2025-10-10 12:58:25,429 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2025-10-10 12:58:25,444 INFO namenode.FSNamesystem: fsOwner                = hduser (auth:SIMPLE)
2025-10-10 12:58:25,444 INFO namenode.FSNamesystem: supergroup             = supergroup
2025-10-10 12:58:25,444 INFO namenode.FSNamesystem: isPermissionEnabled    = true
2025-10-10 12:58:25,444 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true
2025-10-10 12:58:25,445 INFO namenode.FSNamesystem: HA Enabled: false
2025-10-10 12:58:25,478 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2025-10-10 12:58:25,563 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000
2025-10-10 12:58:25,563 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2025-10-10 12:58:25,565 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2025-10-10 12:58:25,565 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Oct 10 12:58:25
2025-10-10 12:58:25,567 INFO util.GSet: Computing capacity for map BlocksMap
2025-10-10 12:58:25,568 INFO util.GSet: VM type       = 64-bit
2025-10-10 12:58:25,568 INFO util.GSet: 2.0% max memory 1.9 GB = 38.9 MB
2025-10-10 12:58:25,569 INFO util.GSet: capacity      = 2^22 = 4194304 entries
2025-10-10 12:58:25,585 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
2025-10-10 12:58:25,585 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
2025-10-10 12:58:25,588 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999
2025-10-10 12:58:25,588 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2025-10-10 12:58:25,589 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2025-10-10 12:58:25,590 INFO blockmanagement.BlockManager: defaultReplication         = 1
2025-10-10 12:58:25,590 INFO blockmanagement.BlockManager: maxReplication             = 512
2025-10-10 12:58:25,590 INFO blockmanagement.BlockManager: minReplication             = 1
2025-10-10 12:58:25,590 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
2025-10-10 12:58:25,590 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2025-10-10 12:58:25,591 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
2025-10-10 12:58:25,591 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2025-10-10 12:58:25,616 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
2025-10-10 12:58:25,616 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
2025-10-10 12:58:25,616 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
2025-10-10 12:58:25,616 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
2025-10-10 12:58:25,626 INFO util.GSet: Computing capacity for map INodeMap
2025-10-10 12:58:25,626 INFO util.GSet: VM type       = 64-bit
2025-10-10 12:58:25,626 INFO util.GSet: 1.0% max memory 1.9 GB = 19.5 MB
2025-10-10 12:58:25,626 INFO util.GSet: capacity      = 2^21 = 2097152 entries
2025-10-10 12:58:25,631 INFO namenode.FSDirectory: ACLs enabled? true
2025-10-10 12:58:25,631 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
2025-10-10 12:58:25,631 INFO namenode.FSDirectory: XAttrs enabled? true
2025-10-10 12:58:25,631 INFO namenode.NameNode: Caching file names occurring more than 10 times
2025-10-10 12:58:25,635 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2025-10-10 12:58:25,637 INFO snapshot.SnapshotManager: SkipList is disabled
2025-10-10 12:58:25,641 INFO util.GSet: Computing capacity for map cachedBlocks
2025-10-10 12:58:25,641 INFO util.GSet: VM type       = 64-bit
2025-10-10 12:58:25,641 INFO util.GSet: 0.25% max memory 1.9 GB = 4.9 MB
2025-10-10 12:58:25,641 INFO util.GSet: capacity      = 2^19 = 524288 entries
2025-10-10 12:58:25,652 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2025-10-10 12:58:25,652 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2025-10-10 12:58:25,652 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2025-10-10 12:58:25,656 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2025-10-10 12:58:25,656 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2025-10-10 12:58:25,657 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2025-10-10 12:58:25,658 INFO util.GSet: VM type       = 64-bit
2025-10-10 12:58:25,658 INFO util.GSet: 0.029999999329447746% max memory 1.9 GB = 597.8 KB
2025-10-10 12:58:25,658 INFO util.GSet: capacity      = 2^16 = 65536 entries
2025-10-10 12:58:25,678 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1164605212-127.0.1.1-1760101105673
2025-10-10 12:58:25,708 INFO common.Storage: Storage directory /usr/local/hadoop_store/hdfs/namenode has been successfully formatted.
2025-10-10 12:58:25,731 INFO namenode.FSImageFormatProtobuf: Saving image file /usr/local/hadoop_store/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
2025-10-10 12:58:25,805 INFO namenode.FSImageFormatProtobuf: Image file /usr/local/hadoop_store/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .
2025-10-10 12:58:25,815 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2025-10-10 12:58:25,834 INFO namenode.FSNamesystem: Stopping services started for active state
2025-10-10 12:58:25,834 INFO namenode.FSNamesystem: Stopping services started for standby state
2025-10-10 12:58:25,838 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
2025-10-10 12:58:25,838 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at DESKTOP-I4C3SAV/127.0.1.1
************************************************************/
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-dfs.sh
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes [DESKTOP-I4C3SAV]
2025-10-10 12:58:39,628 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-yarn.sh
Starting resourcemanager
Starting nodemanagers
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
8102 Jps
7575 SecondaryNameNode
7371 DataNode
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-yarn.sh
Starting resourcemanager
Starting nodemanagers
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
8512 Jps
7575 SecondaryNameNode
7371 DataNode
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ echo $JAVA_HOME
/usr/lib/jvm/java-8-openjdk-amd64
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano /usr/local/hadoop/etc/hadoop/yarn-site.xml
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ stop-yarn.sh
Stopping nodemanagers
Stopping resourcemanager
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-yarn.sh
Starting resourcemanager
Starting nodemanagers
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
9173 Jps
7575 SecondaryNameNode
7371 DataNode
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano /usr/local/hadoop/etc/hadoop/yarn-site.xml
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf /tmp/hadoop-*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf /usr/local/hadoop/logs/*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo chown -R hduser:hadoop /usr/local/hadoop
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ stop-yarn.sh
Stopping nodemanagers
Stopping resourcemanager
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-yarn.sh
Starting resourcemanager
Starting nodemanagers
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
7575 SecondaryNameNode
9799 Jps
9611 NodeManager
7371 DataNode
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ ps -ef | grep ResourceManager
hduser      9828    2323  0 13:03 pts/5    00:00:00 grep --color=auto ResourceManager
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs dfs -mkdir -p /user/hduser/input
2025-10-10 13:03:26,568 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ echo "Hello Hadoop" > ~/test.txt
hdfs dfs -put ~/test.txt /user/hduser/input/
2025-10-10 13:03:41,236 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /user/hduser/input /user/hduser/output
JAR does not exist or is not a normal file: /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs dfs -cat /user/hduser/output/part-r-00000
2025-10-10 13:04:19,561 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
cat: `/user/hduser/output/part-r-00000': No such file or directory
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ cat $HADOOP_HOME/logs/yarn-hduser-*-resourcemanager-*.log
cat: '/usr/local/hadoop/logs/yarn-hduser-*-resourcemanager-*.log': No such file or directory
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ ls $HADOOP_HOME/share/hadoop/mapreduce/
hadoop-mapreduce-client-app-3.3.6.jar     hadoop-mapreduce-client-hs-plugins-3.3.6.jar       hadoop-mapreduce-client-shuffle-3.3.6.jar   lib-examples
hadoop-mapreduce-client-common-3.3.6.jar  hadoop-mapreduce-client-jobclient-3.3.6-tests.jar  hadoop-mapreduce-client-uploader-3.3.6.jar  sources
hadoop-mapreduce-client-core-3.3.6.jar    hadoop-mapreduce-client-jobclient-3.3.6.jar        hadoop-mapreduce-examples-3.3.6.jar
hadoop-mapreduce-client-hs-3.3.6.jar      hadoop-mapreduce-client-nativetask-3.3.6.jar       jdiff
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /user/hduser/inp
ut /user/hduser/output
2025-10-10 13:05:38,125 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-10-10 13:05:38,552 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2025-10-10 13:05:38,630 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2025-10-10 13:05:38,630 INFO impl.MetricsSystemImpl: JobTracker metrics system started
2025-10-10 13:05:38,846 INFO input.FileInputFormat: Total input files to process : 1
2025-10-10 13:05:38,877 INFO mapreduce.JobSubmitter: number of splits:1
2025-10-10 13:05:38,967 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1562098617_0001
2025-10-10 13:05:38,967 INFO mapreduce.JobSubmitter: Executing with tokens: []
2025-10-10 13:05:39,098 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
2025-10-10 13:05:39,099 INFO mapreduce.Job: Running job: job_local1562098617_0001
2025-10-10 13:05:39,100 INFO mapred.LocalJobRunner: OutputCommitter set in config null
2025-10-10 13:05:39,107 INFO output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory
2025-10-10 13:05:39,107 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2025-10-10 13:05:39,107 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-10-10 13:05:39,108 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2025-10-10 13:05:39,142 INFO mapred.LocalJobRunner: Waiting for map tasks
2025-10-10 13:05:39,142 INFO mapred.LocalJobRunner: Starting task: attempt_local1562098617_0001_m_000000_0
2025-10-10 13:05:39,160 INFO output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory
2025-10-10 13:05:39,160 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2025-10-10 13:05:39,160 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-10-10 13:05:39,174 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2025-10-10 13:05:39,176 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/hduser/input/test.txt:0+13
2025-10-10 13:05:39,204 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2025-10-10 13:05:39,204 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2025-10-10 13:05:39,204 INFO mapred.MapTask: soft limit at 83886080
2025-10-10 13:05:39,204 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2025-10-10 13:05:39,204 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2025-10-10 13:05:39,208 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2025-10-10 13:05:39,302 INFO mapred.LocalJobRunner:
2025-10-10 13:05:39,304 INFO mapred.MapTask: Starting flush of map output
2025-10-10 13:05:39,304 INFO mapred.MapTask: Spilling map output
2025-10-10 13:05:39,304 INFO mapred.MapTask: bufstart = 0; bufend = 21; bufvoid = 104857600
2025-10-10 13:05:39,305 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
2025-10-10 13:05:39,325 INFO mapred.MapTask: Finished spill 0
2025-10-10 13:05:39,337 INFO mapred.Task: Task:attempt_local1562098617_0001_m_000000_0 is done. And is in the process of committing
2025-10-10 13:05:39,340 INFO mapred.LocalJobRunner: map
2025-10-10 13:05:39,340 INFO mapred.Task: Task 'attempt_local1562098617_0001_m_000000_0' done.
2025-10-10 13:05:39,348 INFO mapred.Task: Final Counters for attempt_local1562098617_0001_m_000000_0: Counters: 24
        File System Counters
                FILE: Number of bytes read=281534
                FILE: Number of bytes written=924742
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=13
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=5
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=1
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=21
                Map output materialized bytes=31
                Input split bytes=114
                Combine input records=2
                Combine output records=2
                Spilled Records=2
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=0
                Total committed heap usage (bytes)=276824064
        File Input Format Counters
                Bytes Read=13
2025-10-10 13:05:39,348 INFO mapred.LocalJobRunner: Finishing task: attempt_local1562098617_0001_m_000000_0
2025-10-10 13:05:39,348 INFO mapred.LocalJobRunner: map task executor complete.
2025-10-10 13:05:39,352 INFO mapred.LocalJobRunner: Waiting for reduce tasks
2025-10-10 13:05:39,352 INFO mapred.LocalJobRunner: Starting task: attempt_local1562098617_0001_r_000000_0
2025-10-10 13:05:39,358 INFO output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory
2025-10-10 13:05:39,359 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2025-10-10 13:05:39,359 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-10-10 13:05:39,360 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2025-10-10 13:05:39,365 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@16a4fbec
2025-10-10 13:05:39,366 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2025-10-10 13:05:39,385 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=1269825536, maxSingleShuffleLimit=317456384, mergeThreshold=838084864, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2025-10-10 13:05:39,387 INFO reduce.EventFetcher: attempt_local1562098617_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2025-10-10 13:05:39,409 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1562098617_0001_m_000000_0 decomp: 27 len: 31 to MEMORY
2025-10-10 13:05:39,411 INFO reduce.InMemoryMapOutput: Read 27 bytes from map-output for attempt_local1562098617_0001_m_000000_0
2025-10-10 13:05:39,412 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 27, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->27
2025-10-10 13:05:39,414 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
2025-10-10 13:05:39,415 INFO mapred.LocalJobRunner: 1 / 1 copied.
2025-10-10 13:05:39,415 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2025-10-10 13:05:39,422 INFO mapred.Merger: Merging 1 sorted segments
2025-10-10 13:05:39,423 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18 bytes
2025-10-10 13:05:39,426 INFO reduce.MergeManagerImpl: Merged 1 segments, 27 bytes to disk to satisfy reduce memory limit
2025-10-10 13:05:39,427 INFO reduce.MergeManagerImpl: Merging 1 files, 31 bytes from disk
2025-10-10 13:05:39,427 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
2025-10-10 13:05:39,427 INFO mapred.Merger: Merging 1 sorted segments
2025-10-10 13:05:39,428 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18 bytes
2025-10-10 13:05:39,428 INFO mapred.LocalJobRunner: 1 / 1 copied.
2025-10-10 13:05:39,452 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2025-10-10 13:05:39,503 INFO mapred.Task: Task:attempt_local1562098617_0001_r_000000_0 is done. And is in the process of committing
2025-10-10 13:05:39,506 INFO mapred.LocalJobRunner: 1 / 1 copied.
2025-10-10 13:05:39,506 INFO mapred.Task: Task attempt_local1562098617_0001_r_000000_0 is allowed to commit now
2025-10-10 13:05:39,522 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1562098617_0001_r_000000_0' to hdfs://localhost:54310/user/hduser/output
2025-10-10 13:05:39,523 INFO mapred.LocalJobRunner: reduce > reduce
2025-10-10 13:05:39,524 INFO mapred.Task: Task 'attempt_local1562098617_0001_r_000000_0' done.
2025-10-10 13:05:39,524 INFO mapred.Task: Final Counters for attempt_local1562098617_0001_r_000000_0: Counters: 30
        File System Counters
                FILE: Number of bytes read=281628
                FILE: Number of bytes written=924773
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=13
                HDFS: Number of bytes written=17
                HDFS: Number of read operations=10
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=3
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Combine input records=0
                Combine output records=0
                Reduce input groups=2
                Reduce shuffle bytes=31
                Reduce input records=2
                Reduce output records=2
                Spilled Records=2
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=6
                Total committed heap usage (bytes)=314048512
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Output Format Counters
                Bytes Written=17
2025-10-10 13:05:39,524 INFO mapred.LocalJobRunner: Finishing task: attempt_local1562098617_0001_r_000000_0
2025-10-10 13:05:39,524 INFO mapred.LocalJobRunner: reduce task executor complete.
2025-10-10 13:05:40,105 INFO mapreduce.Job: Job job_local1562098617_0001 running in uber mode : false
2025-10-10 13:05:40,106 INFO mapreduce.Job:  map 100% reduce 100%
2025-10-10 13:05:40,107 INFO mapreduce.Job: Job job_local1562098617_0001 completed successfully
2025-10-10 13:05:40,112 INFO mapreduce.Job: Counters: 36
        File System Counters
                FILE: Number of bytes read=563162
                FILE: Number of bytes written=1849515
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=26
                HDFS: Number of bytes written=17
                HDFS: Number of read operations=15
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=4
                HDFS: Number of bytes read erasure-coded=0
        Map-Reduce Framework
                Map input records=1
                Map output records=2
                Map output bytes=21
                Map output materialized bytes=31
                Input split bytes=114
                Combine input records=2
                Combine output records=2
                Reduce input groups=2
                Reduce shuffle bytes=31
                Reduce input records=2
                Reduce output records=2
                Spilled Records=4
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=6
                Total committed heap usage (bytes)=590872576
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=13
        File Output Format Counters
                Bytes Written=17
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ ls $HADOOP_HOME/share/hadoop/mapreduce/
hadoop-mapreduce-client-app-3.3.6.jar     hadoop-mapreduce-client-hs-plugins-3.3.6.jar       hadoop-mapreduce-client-shuffle-3.3.6.jar   lib-examples
hadoop-mapreduce-client-common-3.3.6.jar  hadoop-mapreduce-client-jobclient-3.3.6-tests.jar  hadoop-mapreduce-client-uploader-3.3.6.jar  sources
hadoop-mapreduce-client-core-3.3.6.jar    hadoop-mapreduce-client-jobclient-3.3.6.jar        hadoop-mapreduce-examples-3.3.6.jar
hadoop-mapreduce-client-hs-3.3.6.jar      hadoop-mapreduce-client-nativetask-3.3.6.jar       jdiff
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs dfs -cat /user/hduser/output/part-r-00000
2025-10-10 13:06:13,582 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Hadoop  1
Hello   1
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
7575 SecondaryNameNode
9611 NodeManager
7371 DataNode
10237 Jps
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf /tmp/hadoop-*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf $HADOOP_HOME/logs/*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ stop-yarn.sh
Stopping nodemanagers
Stopping resourcemanager
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-yarn.sh
Starting resourcemanager
Starting nodemanagers
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
10835 Jps
7575 SecondaryNameNode
9611 NodeManager
7371 DataNode
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano yarn-site.xml
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano /usr/local/hadoop/etc/hadoop/yarn-site.xml
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ stop-yarn.sh
Stopping nodemanagers
Stopping resourcemanager
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf $HADOOP_HOME/logs/*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-yarn.sh
Starting resourcemanager
Starting nodemanagers
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
11458 Jps
7575 SecondaryNameNode
9611 NodeManager
7371 DataNode
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ netstat -tulnp | grep 8088
Command 'netstat' not found, but can be installed with:
sudo apt install net-tools
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo apt install net-tools
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages were automatically installed and are no longer required:
  hwloc-nox libmpich12 libslurm40t64
Use 'sudo apt autoremove' to remove them.
The following NEW packages will be installed:
  net-tools
0 upgraded, 1 newly installed, 0 to remove and 125 not upgraded.
Need to get 204 kB of archives.
After this operation, 811 kB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 net-tools amd64 2.10-0.1ubuntu4.4 [204 kB]
Fetched 204 kB in 1s (150 kB/s)
Selecting previously unselected package net-tools.
(Reading database ... 68335 files and directories currently installed.)
Preparing to unpack .../net-tools_2.10-0.1ubuntu4.4_amd64.deb ...
Unpacking net-tools (2.10-0.1ubuntu4.4) ...
Setting up net-tools (2.10-0.1ubuntu4.4) ...
Processing triggers for man-db (2.12.0-4build2) ...
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ netstat -tulnp | grep 8088
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 127.0.0.1:8088          0.0.0.0:*               LISTEN      -
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo kill -9 0
Killed
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ netstat -tulnp | grep 8088
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 127.0.0.1:8088          0.0.0.0:*               LISTEN      -
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo kill -9 0
Killed
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ netstat -tulnp | grep 8088
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 127.0.0.1:8088          0.0.0.0:*               LISTEN      -
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo lsof -i :8088
COMMAND PID     USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
influxd 217 influxdb    3u  IPv4   7372      0t0  TCP localhost:omniorb (LISTEN)
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo kill -9 217
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ netstat -tulnp | grep 8088
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 127.0.0.1:8088          0.0.0.0:*               LISTEN      -
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo lsof -i :8088
COMMAND   PID     USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
influxd 11647 influxdb    3u  IPv4 151389      0t0  TCP localhost:omniorb (LISTEN)
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo kill -9 11647
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo systemctl stop influxdb
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo service influxdb stop
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo lsof -i :8088
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ stop-yarn.sh
Stopping nodemanagers
Stopping resourcemanager
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf $HADOOP_HOME/logs/*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-yarn.sh
Starting resourcemanager
Starting nodemanagers
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
12499 Jps
7575 SecondaryNameNode
12010 ResourceManager
9611 NodeManager
7371 DataNode
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ cd ..
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop$ cd ..
hduser@DESKTOP-I4C3SAV:/usr/local$ cd ..
hduser@DESKTOP-I4C3SAV:/usr$ cd ..
hduser@DESKTOP-I4C3SAV:/$ cd ~
hduser@DESKTOP-I4C3SAV:~$ netstat -plten | grep java
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 0.0.0.0:9868            0.0.0.0:*               LISTEN      1001       77927      7575/java
tcp        0      0 0.0.0.0:9870            0.0.0.0:*               LISTEN      1001       53614      7199/java
tcp        0      0 0.0.0.0:9864            0.0.0.0:*               LISTEN      1001       72655      7371/java
tcp        0      0 0.0.0.0:9866            0.0.0.0:*               LISTEN      1001       75858      7371/java
tcp        0      0 0.0.0.0:9867            0.0.0.0:*               LISTEN      1001       75863      7371/java
tcp        0      0 0.0.0.0:13562           0.0.0.0:*               LISTEN      1001       111547     9611/java
tcp        0      0 0.0.0.0:33729           0.0.0.0:*               LISTEN      1001       111533     9611/java
tcp        0      0 0.0.0.0:8040            0.0.0.0:*               LISTEN      1001       111543     9611/java
tcp        0      0 0.0.0.0:8042            0.0.0.0:*               LISTEN      1001       108294     9611/java
tcp        0      0 127.0.0.1:33805         0.0.0.0:*               LISTEN      1001       75861      7371/java
tcp        0      0 127.0.0.1:54310         0.0.0.0:*               LISTEN      1001       75391      7199/java
tcp6       0      0 :::8032                 :::*                    LISTEN      1001       145015     12010/java
tcp6       0      0 :::8033                 :::*                    LISTEN      1001       155987     12010/java
tcp6       0      0 :::8030                 :::*                    LISTEN      1001       145010     12010/java
tcp6       0      0 :::8031                 :::*                    LISTEN      1001       145005     12010/java
tcp6       0      0 :::8088                 :::*                    LISTEN      1001       146634     12010/java
hduser@DESKTOP-I4C3SAV:~$ ^C
hduser@DESKTOP-I4C3SAV:~$ pwd/usr/local/hadoop/sbin
bash: pwd/usr/local/hadoop/sbin: No such file or directory
hduser@DESKTOP-I4C3SAV:~$ pwd /usr/local/hadoop/sbin
/home/hduser
hduser@DESKTOP-I4C3SAV:~$ pwd
/home/hduser
hduser@DESKTOP-I4C3SAV:~$ cd /usr/local/hadoop/sbin
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ ^C
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ pwd
/usr/local/hadoop/sbin
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ ls
FederationStateStore   httpfs.sh                start-all.cmd      start-dfs.sh         stop-all.cmd      stop-dfs.sh         workers.sh
distribute-exclude.sh  kms.sh                   start-all.sh       start-secure-dns.sh  stop-all.sh       stop-secure-dns.sh  yarn-daemon.sh
hadoop-daemon.sh       mr-jobhistory-daemon.sh  start-balancer.sh  start-yarn.cmd       stop-balancer.sh  stop-yarn.cmd       yarn-daemons.sh
hadoop-daemons.sh      refresh-namenodes.sh     start-dfs.cmd      start-yarn.sh        stop-dfs.cmd      stop-yarn.sh
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ pwd
/usr/local/hadoop/sbin
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ ls
FederationStateStore   httpfs.sh                start-all.cmd      start-dfs.sh         stop-all.cmd      stop-dfs.sh         workers.sh
distribute-exclude.sh  kms.sh                   start-all.sh       start-secure-dns.sh  stop-all.sh       stop-secure-dns.sh  yarn-daemon.sh
hadoop-daemon.sh       mr-jobhistory-daemon.sh  start-balancer.sh  start-yarn.cmd       stop-balancer.sh  stop-yarn.cmd       yarn-daemons.sh
hadoop-daemons.sh      refresh-namenodes.sh     start-dfs.cmd      start-yarn.sh        stop-dfs.cmd      stop-yarn.sh
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ stop-all.sh
WARNING: Stopping all Apache Hadoop daemons as hduser in 10 seconds.
WARNING: Use CTRL-C to abort.
Stopping namenodes on [localhost]
Stopping datanodes
Stopping secondary namenodes [DESKTOP-I4C3SAV]
2025-10-10 13:16:02,126 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Stopping nodemanagers
Stopping resourcemanager
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ start-all.sh
WARNING: Attempting to start all Apache Hadoop daemons as hduser in 10 seconds.
WARNING: This is not a recommended production deployment configuration.
WARNING: Use CTRL-C to abort.
Starting namenodes on [localhost]
localhost: ERROR: Cannot set priority of namenode process 13330
Starting datanodes
Starting secondary namenodes [DESKTOP-I4C3SAV]
DESKTOP-I4C3SAV: ERROR: Cannot set priority of secondarynamenode process 13599
2025-10-10 13:16:27,548 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting resourcemanager
Starting nodemanagers
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
7575 SecondaryNameNode
13802 ResourceManager
9611 NodeManager
7371 DataNode
14302 Jps
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
14324 Jps
7575 SecondaryNameNode
13802 ResourceManager
9611 NodeManager
7371 DataNode
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo systemctl stop influxdb
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo service influxdb stop
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo lsof -i :8088
COMMAND   PID   USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
java    13802 hduser  385u  IPv6 166839      0t0  TCP *:omniorb (LISTEN)
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
7575 SecondaryNameNode
13802 ResourceManager
9611 NodeManager
7371 DataNode
14380 Jps
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$JAVA_HOME/bin:$PATH
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf /tmp/hadoop-*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf $HADOOP_HOME/logs/*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/stop-dfs.sh
Stopping namenodes on [localhost]
Stopping datanodes
Stopping secondary namenodes [DESKTOP-I4C3SAV]
2025-10-10 13:20:22,504 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/start-dfs.sh
Starting namenodes on [localhost]
localhost: ERROR: Cannot set priority of namenode process 14955
Starting datanodes
localhost: ERROR: Cannot set priority of datanode process 15068
Starting secondary namenodes [DESKTOP-I4C3SAV]
2025-10-10 13:20:39,030 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
15347 Jps
7575 SecondaryNameNode
13802 ResourceManager
9611 NodeManager
7371 DataNode
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ netstat -tulnp | grep 50070
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/stop-dfs.sh
Stopping namenodes on [localhost]
Stopping datanodes
Stopping secondary namenodes [DESKTOP-I4C3SAV]
2025-10-10 13:23:46,088 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/start-dfs.sh
Starting namenodes on [localhost]
Starting datanodes
localhost: ERROR: Cannot set priority of datanode process 16083
Starting secondary namenodes [DESKTOP-I4C3SAV]
DESKTOP-I4C3SAV: ERROR: Cannot set priority of secondarynamenode process 16240
2025-10-10 13:23:58,567 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ netstat -tulnp | grep 50070
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ echo $JAVA_HOME
/usr/lib/jvm/java-8-openjdk-amd64
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf /tmp/hadoop-*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf $HADOOP_HOME/logs/*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf /usr/local/hadoop_store/hdfs/namenode/*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf /usr/local/hadoop_store/hdfs/datanode/*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs namenode -format
2025-10-10 13:25:36,942 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = DESKTOP-I4C3SAV/127.0.1.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.3.6
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z
STARTUP_MSG:   java = 1.8.0_462
************************************************************/
2025-10-10 13:25:36,950 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2025-10-10 13:25:37,011 INFO namenode.NameNode: createNameNode [-format]
2025-10-10 13:25:37,123 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-10-10 13:25:37,270 INFO namenode.NameNode: Formatting using clusterid: CID-c7d6e2d3-6b07-43c0-ad6e-9a00cc88f032
2025-10-10 13:25:37,299 INFO namenode.FSEditLog: Edit logging is async:true
2025-10-10 13:25:37,328 INFO namenode.FSNamesystem: KeyProvider: null
2025-10-10 13:25:37,330 INFO namenode.FSNamesystem: fsLock is fair: true
2025-10-10 13:25:37,331 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2025-10-10 13:25:37,337 INFO namenode.FSNamesystem: fsOwner                = hduser (auth:SIMPLE)
2025-10-10 13:25:37,337 INFO namenode.FSNamesystem: supergroup             = supergroup
2025-10-10 13:25:37,337 INFO namenode.FSNamesystem: isPermissionEnabled    = true
2025-10-10 13:25:37,337 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true
2025-10-10 13:25:37,337 INFO namenode.FSNamesystem: HA Enabled: false
2025-10-10 13:25:37,379 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2025-10-10 13:25:37,480 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000
2025-10-10 13:25:37,480 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2025-10-10 13:25:37,484 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2025-10-10 13:25:37,484 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Oct 10 13:25:37
2025-10-10 13:25:37,486 INFO util.GSet: Computing capacity for map BlocksMap
2025-10-10 13:25:37,486 INFO util.GSet: VM type       = 64-bit
2025-10-10 13:25:37,487 INFO util.GSet: 2.0% max memory 1.7 GB = 34.6 MB
2025-10-10 13:25:37,487 INFO util.GSet: capacity      = 2^22 = 4194304 entries
2025-10-10 13:25:37,495 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
2025-10-10 13:25:37,495 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
2025-10-10 13:25:37,503 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999
2025-10-10 13:25:37,504 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2025-10-10 13:25:37,504 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2025-10-10 13:25:37,504 INFO blockmanagement.BlockManager: defaultReplication         = 1
2025-10-10 13:25:37,504 INFO blockmanagement.BlockManager: maxReplication             = 512
2025-10-10 13:25:37,504 INFO blockmanagement.BlockManager: minReplication             = 1
2025-10-10 13:25:37,504 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
2025-10-10 13:25:37,504 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2025-10-10 13:25:37,504 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
2025-10-10 13:25:37,504 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2025-10-10 13:25:37,532 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
2025-10-10 13:25:37,532 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
2025-10-10 13:25:37,532 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
2025-10-10 13:25:37,532 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
2025-10-10 13:25:37,543 INFO util.GSet: Computing capacity for map INodeMap
2025-10-10 13:25:37,543 INFO util.GSet: VM type       = 64-bit
2025-10-10 13:25:37,544 INFO util.GSet: 1.0% max memory 1.7 GB = 17.3 MB
2025-10-10 13:25:37,544 INFO util.GSet: capacity      = 2^21 = 2097152 entries
2025-10-10 13:25:37,585 INFO namenode.FSDirectory: ACLs enabled? true
2025-10-10 13:25:37,586 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
2025-10-10 13:25:37,586 INFO namenode.FSDirectory: XAttrs enabled? true
2025-10-10 13:25:37,586 INFO namenode.NameNode: Caching file names occurring more than 10 times
2025-10-10 13:25:37,592 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2025-10-10 13:25:37,593 INFO snapshot.SnapshotManager: SkipList is disabled
2025-10-10 13:25:37,597 INFO util.GSet: Computing capacity for map cachedBlocks
2025-10-10 13:25:37,597 INFO util.GSet: VM type       = 64-bit
2025-10-10 13:25:37,598 INFO util.GSet: 0.25% max memory 1.7 GB = 4.3 MB
2025-10-10 13:25:37,598 INFO util.GSet: capacity      = 2^19 = 524288 entries
2025-10-10 13:25:37,608 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2025-10-10 13:25:37,608 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2025-10-10 13:25:37,608 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2025-10-10 13:25:37,611 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2025-10-10 13:25:37,611 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2025-10-10 13:25:37,612 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2025-10-10 13:25:37,612 INFO util.GSet: VM type       = 64-bit
2025-10-10 13:25:37,612 INFO util.GSet: 0.029999999329447746% max memory 1.7 GB = 531.5 KB
2025-10-10 13:25:37,612 INFO util.GSet: capacity      = 2^16 = 65536 entries
2025-10-10 13:25:37,636 INFO namenode.FSImage: Allocated new BlockPoolId: BP-830105410-127.0.1.1-1760102737630
2025-10-10 13:25:37,657 INFO common.Storage: Storage directory /usr/local/hadoop_store/hdfs/namenode has been successfully formatted.
2025-10-10 13:25:37,765 INFO namenode.FSImageFormatProtobuf: Saving image file /usr/local/hadoop_store/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
2025-10-10 13:25:37,837 INFO namenode.FSImageFormatProtobuf: Image file /usr/local/hadoop_store/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .
2025-10-10 13:25:37,852 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2025-10-10 13:25:37,873 INFO namenode.FSNamesystem: Stopping services started for active state
2025-10-10 13:25:37,873 INFO namenode.FSNamesystem: Stopping services started for standby state
2025-10-10 13:25:37,878 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
2025-10-10 13:25:37,878 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at DESKTOP-I4C3SAV/127.0.1.1
************************************************************/
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/start-dfs.sh
Starting namenodes on [localhost]
Starting datanodes
localhost: ERROR: Cannot set priority of datanode process 16697
Starting secondary namenodes [DESKTOP-I4C3SAV]
DESKTOP-I4C3SAV: ERROR: Cannot set priority of secondarynamenode process 16853
2025-10-10 13:25:52,713 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
7575 SecondaryNameNode
13802 ResourceManager
9611 NodeManager
7371 DataNode
16974 Jps
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ netstat -tulnp | grep 50070
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/stop-dfs.sh
Stopping namenodes on [localhost]
Stopping datanodes
Stopping secondary namenodes [DESKTOP-I4C3SAV]
2025-10-10 13:27:24,443 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/start-dfs.sh
Starting namenodes on [localhost]
Starting datanodes
localhost: ERROR: Cannot set priority of datanode process 17694
Starting secondary namenodes [DESKTOP-I4C3SAV]
DESKTOP-I4C3SAV: ERROR: Cannot set priority of secondarynamenode process 17851
2025-10-10 13:27:35,294 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/stop-dfs.sh
Stopping namenodes on [localhost]
Stopping datanodes
Stopping secondary namenodes [DESKTOP-I4C3SAV]
2025-10-10 13:29:05,483 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/stop-yarn.sh
Stopping nodemanagers
Stopping resourcemanager
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
7575 SecondaryNameNode
18601 Jps
13802 ResourceManager
9611 NodeManager
7371 DataNode
7199 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo kill -9 7575 13802 9611 7371 7199
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
18630 Jps
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo lsof -i :50070
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo lsof -i :8088
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf /tmp/hadoop-*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf $HADOOP_HOME/logs/*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf /usr/local/hadoop_store/hdfs/namenode/*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ rm -rf /usr/local/hadoop_store/hdfs/datanode/*
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs namenode -format
2025-10-10 13:30:20,005 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = DESKTOP-I4C3SAV/127.0.1.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.3.6
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/usr/local/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z
STARTUP_MSG:   java = 1.8.0_462
************************************************************/
2025-10-10 13:30:20,015 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2025-10-10 13:30:20,076 INFO namenode.NameNode: createNameNode [-format]
2025-10-10 13:30:20,195 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-10-10 13:30:20,338 INFO namenode.NameNode: Formatting using clusterid: CID-06f84741-572d-42f1-b0a7-8f471515beab
2025-10-10 13:30:20,371 INFO namenode.FSEditLog: Edit logging is async:true
2025-10-10 13:30:20,401 INFO namenode.FSNamesystem: KeyProvider: null
2025-10-10 13:30:20,402 INFO namenode.FSNamesystem: fsLock is fair: true
2025-10-10 13:30:20,403 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2025-10-10 13:30:20,408 INFO namenode.FSNamesystem: fsOwner                = hduser (auth:SIMPLE)
2025-10-10 13:30:20,408 INFO namenode.FSNamesystem: supergroup             = supergroup
2025-10-10 13:30:20,408 INFO namenode.FSNamesystem: isPermissionEnabled    = true
2025-10-10 13:30:20,408 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true
2025-10-10 13:30:20,408 INFO namenode.FSNamesystem: HA Enabled: false
2025-10-10 13:30:20,452 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2025-10-10 13:30:20,555 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000
2025-10-10 13:30:20,555 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2025-10-10 13:30:20,559 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2025-10-10 13:30:20,559 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Oct 10 13:30:20
2025-10-10 13:30:20,562 INFO util.GSet: Computing capacity for map BlocksMap
2025-10-10 13:30:20,562 INFO util.GSet: VM type       = 64-bit
2025-10-10 13:30:20,563 INFO util.GSet: 2.0% max memory 1.7 GB = 34.6 MB
2025-10-10 13:30:20,563 INFO util.GSet: capacity      = 2^22 = 4194304 entries
2025-10-10 13:30:20,572 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
2025-10-10 13:30:20,572 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
2025-10-10 13:30:20,577 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999
2025-10-10 13:30:20,577 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2025-10-10 13:30:20,577 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2025-10-10 13:30:20,578 INFO blockmanagement.BlockManager: defaultReplication         = 1
2025-10-10 13:30:20,578 INFO blockmanagement.BlockManager: maxReplication             = 512
2025-10-10 13:30:20,578 INFO blockmanagement.BlockManager: minReplication             = 1
2025-10-10 13:30:20,578 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
2025-10-10 13:30:20,578 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2025-10-10 13:30:20,578 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
2025-10-10 13:30:20,578 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2025-10-10 13:30:20,604 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
2025-10-10 13:30:20,604 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
2025-10-10 13:30:20,604 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
2025-10-10 13:30:20,604 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
2025-10-10 13:30:20,616 INFO util.GSet: Computing capacity for map INodeMap
2025-10-10 13:30:20,616 INFO util.GSet: VM type       = 64-bit
2025-10-10 13:30:20,616 INFO util.GSet: 1.0% max memory 1.7 GB = 17.3 MB
2025-10-10 13:30:20,617 INFO util.GSet: capacity      = 2^21 = 2097152 entries
2025-10-10 13:30:20,667 INFO namenode.FSDirectory: ACLs enabled? true
2025-10-10 13:30:20,667 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
2025-10-10 13:30:20,667 INFO namenode.FSDirectory: XAttrs enabled? true
2025-10-10 13:30:20,668 INFO namenode.NameNode: Caching file names occurring more than 10 times
2025-10-10 13:30:20,673 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2025-10-10 13:30:20,675 INFO snapshot.SnapshotManager: SkipList is disabled
2025-10-10 13:30:20,678 INFO util.GSet: Computing capacity for map cachedBlocks
2025-10-10 13:30:20,678 INFO util.GSet: VM type       = 64-bit
2025-10-10 13:30:20,679 INFO util.GSet: 0.25% max memory 1.7 GB = 4.3 MB
2025-10-10 13:30:20,679 INFO util.GSet: capacity      = 2^19 = 524288 entries
2025-10-10 13:30:20,684 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2025-10-10 13:30:20,684 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2025-10-10 13:30:20,684 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2025-10-10 13:30:20,689 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2025-10-10 13:30:20,689 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2025-10-10 13:30:20,690 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2025-10-10 13:30:20,690 INFO util.GSet: VM type       = 64-bit
2025-10-10 13:30:20,690 INFO util.GSet: 0.029999999329447746% max memory 1.7 GB = 531.5 KB
2025-10-10 13:30:20,690 INFO util.GSet: capacity      = 2^16 = 65536 entries
2025-10-10 13:30:20,713 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1866462120-127.0.1.1-1760103020707
2025-10-10 13:30:20,736 INFO common.Storage: Storage directory /usr/local/hadoop_store/hdfs/namenode has been successfully formatted.
2025-10-10 13:30:20,812 INFO namenode.FSImageFormatProtobuf: Saving image file /usr/local/hadoop_store/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
2025-10-10 13:30:20,876 INFO namenode.FSImageFormatProtobuf: Image file /usr/local/hadoop_store/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .
2025-10-10 13:30:20,891 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2025-10-10 13:30:20,908 INFO namenode.FSNamesystem: Stopping services started for active state
2025-10-10 13:30:20,909 INFO namenode.FSNamesystem: Stopping services started for standby state
2025-10-10 13:30:20,912 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
2025-10-10 13:30:20,913 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at DESKTOP-I4C3SAV/127.0.1.1
************************************************************/
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/start-dfs.sh
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes [DESKTOP-I4C3SAV]
2025-10-10 13:30:35,840 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
19014 DataNode
19210 SecondaryNameNode
18859 NameNode
19340 Jps
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/start-yarn.sh
Starting resourcemanager
Starting nodemanagers
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
19444 ResourceManager
19572 NodeManager
19014 DataNode
19959 Jps
19210 SecondaryNameNode
18859 NameNode
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ pip install pyspark
error: externally-managed-environment

× This environment is externally managed
╰─> To install Python packages system-wide, try apt install
    python3-xyz, where xyz is the package you are trying to
    install.

    If you wish to install a non-Debian-packaged Python package,
    create a virtual environment using python3 -m venv path/to/venv.
    Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make
    sure you have python3-full installed.

    If you wish to install a non-Debian packaged Python application,
    it may be easiest to use pipx install xyz, which will manage a
    virtual environment for you. Make sure you have pipx installed.

    See /usr/share/doc/python3.12/README.venv for more information.

note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.
hint: See PEP 668 for the detailed specification.
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ python3 -m venv ~/pyspark-env
The virtual environment was not created successfully because ensurepip is not
available.  On Debian/Ubuntu systems, you need to install the python3-venv
package using the following command.

    apt install python3.12-venv

You may need to use sudo with that command.  After installing the python3-venv
package, recreate your virtual environment.

Failing command: /home/hduser/pyspark-env/bin/python3

hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$  apt install python3.12-venv
E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)
E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo  apt install python3.12-venv
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages were automatically installed and are no longer required:
  hwloc-nox libmpich12 libslurm40t64
Use 'sudo apt autoremove' to remove them.
The following additional packages will be installed:
  python3-pip-whl python3-setuptools-whl
The following NEW packages will be installed:
  python3-pip-whl python3-setuptools-whl python3.12-venv
0 upgraded, 3 newly installed, 0 to remove and 125 not upgraded.
Need to get 2429 kB of archives.
After this operation, 2777 kB of additional disk space will be used.
Do you want to continue? [Y/n] Y
Get:1 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3-pip-whl all 24.0+dfsg-1ubuntu1.3 [1707 kB]
Get:2 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3-setuptools-whl all 68.1.2-2ubuntu1.2 [716 kB]
Get:3 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3.12-venv amd64 3.12.3-1ubuntu0.8 [5678 B]
Fetched 2429 kB in 3s (754 kB/s)
Selecting previously unselected package python3-pip-whl.
(Reading database ... 68383 files and directories currently installed.)
Preparing to unpack .../python3-pip-whl_24.0+dfsg-1ubuntu1.3_all.deb ...
Unpacking python3-pip-whl (24.0+dfsg-1ubuntu1.3) ...
Selecting previously unselected package python3-setuptools-whl.
Preparing to unpack .../python3-setuptools-whl_68.1.2-2ubuntu1.2_all.deb ...
Unpacking python3-setuptools-whl (68.1.2-2ubuntu1.2) ...
Selecting previously unselected package python3.12-venv.
Preparing to unpack .../python3.12-venv_3.12.3-1ubuntu0.8_amd64.deb ...
Unpacking python3.12-venv (3.12.3-1ubuntu0.8) ...
Setting up python3-setuptools-whl (68.1.2-2ubuntu1.2) ...
Setting up python3-pip-whl (24.0+dfsg-1ubuntu1.3) ...
Setting up python3.12-venv (3.12.3-1ubuntu0.8) ...
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ python3 -m venv ~/pyspark-env
hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ source ~/pyspark-env/bin/activate
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ pip install pyspark
Collecting pyspark
  Downloading pyspark-4.0.1.tar.gz (434.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 434.2/434.2 MB 1.3 MB/s eta 0:00:00
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting py4j==0.10.9.9 (from pyspark)
  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)
Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203.0/203.0 kB 1.2 MB/s eta 0:00:00
Building wheels for collected packages: pyspark
  Building wheel for pyspark (pyproject.toml) ... done
  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434813860 sha256=3476cf19f68f2fe0af42b1fa3c6fc9ba5bef5c7efeae0374db43017980a1291e
  Stored in directory: /home/hduser/.cache/pip/wheels/31/9f/68/f89fb34ccd886909be7d0e390eaaf97f21efdf540c0ee8dbcd
Successfully built pyspark
Installing collected packages: py4j, pyspark
Successfully installed py4j-0.10.9.9 pyspark-4.0.1
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs dfs -mkdir -p /user/hduser/input
2025-10-10 13:42:04,260 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
19444 ResourceManager
19572 NodeManager
19014 DataNode
19210 SecondaryNameNode
18859 NameNode
20301 Jps
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs dfs -ls /user/hduser
2025-10-10 13:45:10,666 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
drwxr-xr-x   - hduser supergroup          0 2025-10-10 13:42 /user/hduser/input
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs dfs -mkdir -p /user/hduser/input
2025-10-10 13:45:22,688 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs dfs -chmod 755 /user/hduser/input
2025-10-10 13:45:27,893 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs dfs -put /mnt/c/Users/Welcome/Downloads/lending-club/accepted_2007_to_2018Q4.csv /user/hduser/input/
2025-10-10 13:47:27,182 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs dfs -ls /user/hduser/input
2025-10-10 13:50:48,484 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
-rw-r--r--   1 hduser supergroup 1675133810 2025-10-10 13:50 /user/hduser/input/accepted_2007_to_2018Q4.csv
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ pyspark
Error: A JNI error has occurred, please check your installation and try again
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/apache/spark/launcher/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 52.0
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:756)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:473)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:621)
/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ pyspark
Error: A JNI error has occurred, please check your installation and try again
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/apache/spark/launcher/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 52.0
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:756)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:473)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:621)
/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ java -version
openjdk version "1.8.0_462"
OpenJDK Runtime Environment (build 1.8.0_462-8u462-ga~us1-0ubuntu2~24.04.2-b08)
OpenJDK 64-Bit Server VM (build 25.462-b08, mixed mode)
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo apt install openjdk-17-jdk -y
[sudo] password for hduser:
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages were automatically installed and are no longer required:
  hwloc-nox libmpich12 libslurm40t64
Use 'sudo apt autoremove' to remove them.
The following additional packages will be installed:
  openjdk-17-jdk-headless openjdk-17-jre openjdk-17-jre-headless
Suggested packages:
  openjdk-17-demo openjdk-17-source visualvm libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei | fonts-wqy-zenhei fonts-indic
The following NEW packages will be installed:
  openjdk-17-jdk openjdk-17-jdk-headless openjdk-17-jre openjdk-17-jre-headless
0 upgraded, 4 newly installed, 0 to remove and 125 not upgraded.
Need to get 121 MB of archives.
After this operation, 274 MB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 openjdk-17-jre-headless amd64 17.0.16+8~us1-0ubuntu1~24.04.1 [48.0 MB]
Get:2 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 openjdk-17-jre amd64 17.0.16+8~us1-0ubuntu1~24.04.1 [227 kB]
Get:3 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 openjdk-17-jdk-headless amd64 17.0.16+8~us1-0ubuntu1~24.04.1 [71.4 MB]
Get:4 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 openjdk-17-jdk amd64 17.0.16+8~us1-0ubuntu1~24.04.1 [1521 kB]
Fetched 121 MB in 2min 17s (883 kB/s)
Selecting previously unselected package openjdk-17-jre-headless:amd64.
(Reading database ... 68399 files and directories currently installed.)
Preparing to unpack .../openjdk-17-jre-headless_17.0.16+8~us1-0ubuntu1~24.04.1_amd64.deb ...
Unpacking openjdk-17-jre-headless:amd64 (17.0.16+8~us1-0ubuntu1~24.04.1) ...
Selecting previously unselected package openjdk-17-jre:amd64.
Preparing to unpack .../openjdk-17-jre_17.0.16+8~us1-0ubuntu1~24.04.1_amd64.deb ...
Unpacking openjdk-17-jre:amd64 (17.0.16+8~us1-0ubuntu1~24.04.1) ...
Selecting previously unselected package openjdk-17-jdk-headless:amd64.
Preparing to unpack .../openjdk-17-jdk-headless_17.0.16+8~us1-0ubuntu1~24.04.1_amd64.deb ...
Unpacking openjdk-17-jdk-headless:amd64 (17.0.16+8~us1-0ubuntu1~24.04.1) ...
Selecting previously unselected package openjdk-17-jdk:amd64.
Preparing to unpack .../openjdk-17-jdk_17.0.16+8~us1-0ubuntu1~24.04.1_amd64.deb ...
Unpacking openjdk-17-jdk:amd64 (17.0.16+8~us1-0ubuntu1~24.04.1) ...
Setting up openjdk-17-jre-headless:amd64 (17.0.16+8~us1-0ubuntu1~24.04.1) ...
Processing triggers for hicolor-icon-theme (0.17-2) ...
Processing triggers for ca-certificates-java (20240118) ...
done.
Setting up openjdk-17-jdk-headless:amd64 (17.0.16+8~us1-0ubuntu1~24.04.1) ...
Setting up openjdk-17-jre:amd64 (17.0.16+8~us1-0ubuntu1~24.04.1) ...
Setting up openjdk-17-jdk:amd64 (17.0.16+8~us1-0ubuntu1~24.04.1) ...
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ ls /usr/lib/jvm/
default-java               java-1.17.0-openjdk-amd64  java-1.8.0-openjdk-amd64  java-17-openjdk-amd64  java-8-openjdk-amd64  openjdk-17
java-1.11.0-openjdk-amd64  java-1.21.0-openjdk-amd64  java-11-openjdk-amd64     java-21-openjdk-amd64  openjdk-11            openjdk-21
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano start-hadoop.sh
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ chmod +x start-hadoop.sh
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano start-pyspark.sh
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ chmod +x start-pyspark.sh
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ ./start-hadoop.sh
Starting namenodes on [localhost]
localhost: namenode is running as process 18859.  Stop it first and ensure /tmp/hadoop-hduser-namenode.pid file is empty before retry.
Starting datanodes
localhost: datanode is running as process 19014.  Stop it first and ensure /tmp/hadoop-hduser-datanode.pid file is empty before retry.
Starting secondary namenodes [DESKTOP-I4C3SAV]
DESKTOP-I4C3SAV: secondarynamenode is running as process 19210.  Stop it first and ensure /tmp/hadoop-hduser-secondarynamenode.pid file is empty before retry.
2025-10-10 14:00:22,667 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting resourcemanager
resourcemanager is running as process 19444.  Stop it first and ensure /tmp/hadoop-hduser-resourcemanager.pid file is empty before retry.
Starting nodemanagers
localhost: nodemanager is running as process 19572.  Stop it first and ensure /tmp/hadoop-hduser-nodemanager.pid file is empty before retry.
Hadoop started with Java 8
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ ./start-pyspark.sh
/home/hduser/pyspark-env/bin/pyspark: line 24: /usr/local/spark/bin/load-spark-env.sh: No such file or directory
/home/hduser/pyspark-env/bin/pyspark: line 102: /usr/local/spark/bin/spark-submit: No such file or directory
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ find / -type d -name "spark*" 2>/dev/null
^C
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ find / -type d -name "spark*" 2>/dev/null
^C
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ find /usr/local -type d -name "spark*" 2>/dev/null
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ find ~ -type d -name "spark*" 2>/dev/null
/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/pandas/spark
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano ~/start-pyspark.sh
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano start-pyspark.sh
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ nano start-pyspark.sh
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ ./start-pyspark.sh
/home/hduser/pyspark-env/bin/pyspark: line 24: /home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/pandas/spark/bin/load-spark-env.sh: No such file or directory
/home/hduser/pyspark-env/bin/pyspark: line 102: /home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/pandas/spark/bin/spark-submit: No such file or directory
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ source ~/pyspark-env/bin/activate
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from pyspark.sql import SparkSession
>>>
>>> spark = SparkSession.builder \
...     .appName("LendingClub") \
...     .master("local[*]") \
...     .getOrCreate()
Error: A JNI error has occurred, please check your installation and try again
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/apache/spark/launcher/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 52.0
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:756)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:473)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:621)
/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript
Traceback (most recent call last):
  File "<stdin>", line 4, in <module>
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/sql/session.py", line 556, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/core/context.py", line 523, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/core/context.py", line 205, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/core/context.py", line 444, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/java_gateway.py", line 111, in launch_gateway
    raise PySparkRuntimeError(
pyspark.errors.exceptions.base.PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.
>>>
>>> df = spark.read.option("header", "true") \
...                .option("inferSchema", "true") \
...                .csv("hdfs://localhost:9000/user/hduser/input/accepted_2007_to_2018Q4.csv")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'spark' is not defined
>>>
>>> df.show(5)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'df' is not defined
>>> export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
  File "<stdin>", line 1
    export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
           ^^^^^^^^^
SyntaxError: invalid syntax
>>> exit
Use exit() or Ctrl-D (i.e. EOF) to exit
>>> exit()
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ export PATH=$JAVA_HOME/bin:$PATH
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from pyspark.sql import SparkSession
er \
    .appName("LendingClub") \
    .master("local[*]") \
    .getOrCreate()
>>>
>>> spark = SparkSession.builder \
...     .appName("LendingClub") \
...     .master("local[*]") \
...     .getOrCreate()
WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/10/10 14:33:16 WARN Utils: Your hostname, DESKTOP-I4C3SAV, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/10/10 14:33:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/10/10 14:33:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
>>> df = spark.read.option("header", "true") \
...                .option("inferSchema", "true") \
...                .csv("hdfs://localhost:9000/user/hduser/input/accepted_2007_to_2018Q4.csv")
es and data types
25/10/10 14:33:55 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: hdfs://localhost:9000/user/hduser/input/accepted_2007_to_2018Q4.csv.
java.net.ConnectException: Call From DESKTOP-I4C3SAV/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:961)
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:876)
        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
        at org.apache.hadoop.ipc.Client.call(Client.java:1529)
        at org.apache.hadoop.ipc.Client.call(Client.java:1426)
        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)
        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)
        at jdk.proxy2/jdk.proxy2.$Proxy46.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.lambda$getFileInfo$41(ClientNamenodeProtocolTranslatorPB.java:820)
        at org.apache.hadoop.ipc.internal.ShadedProtobufHelper.ipc(ShadedProtobufHelper.java:160)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:820)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:437)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:170)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:162)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:100)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:366)
        at jdk.proxy2/jdk.proxy2.$Proxy47.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1770)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1828)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1825)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1840)
        at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
        at scala.Option.getOrElse(Option.scala:201)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
        at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
        at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
        at scala.collection.immutable.List.foldLeft(List.scala:79)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
        at scala.collection.immutable.List.foreach(List.scala:334)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
        at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
        at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
        at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
        at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
        at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
        at scala.util.Try$.apply(Try.scala:217)
        at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
        at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
        at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
        at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
        at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
        at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
        at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
        at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
        at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
        at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
        at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)
        at org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)
        at org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
        at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.ConnectException: Connection refused
        at java.base/sun.nio.ch.Net.pollConnect(Native Method)
        at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
        at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:614)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:789)
        at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:364)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
        at org.apache.hadoop.ipc.Client.call(Client.java:1473)
        ... 99 more
Traceback (most recent call last):
  File "<stdin>", line 3, in <module>
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 838, in csv
    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o31.csv.
: java.net.ConnectException: Call From DESKTOP-I4C3SAV/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:961)
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:876)
        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
        at org.apache.hadoop.ipc.Client.call(Client.java:1529)
        at org.apache.hadoop.ipc.Client.call(Client.java:1426)
        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)
        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)
        at jdk.proxy2/jdk.proxy2.$Proxy46.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.lambda$getFileInfo$41(ClientNamenodeProtocolTranslatorPB.java:820)
        at org.apache.hadoop.ipc.internal.ShadedProtobufHelper.ipc(ShadedProtobufHelper.java:160)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:820)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:437)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:170)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:162)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:100)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:366)
        at jdk.proxy2/jdk.proxy2.$Proxy47.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1770)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1828)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1825)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1840)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)
        at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)
        at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)
        at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
        at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)
        at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
        at scala.Option.getOrElse(Option.scala:201)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
        at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
        at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
        at scala.collection.immutable.List.foldLeft(List.scala:79)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
        at scala.collection.immutable.List.foreach(List.scala:334)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
        at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
        at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
        at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
        at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
        at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
        at scala.util.Try$.apply(Try.scala:217)
        at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
        at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
        at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
        at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
        at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
        at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
        at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
        at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
        at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
        at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)
        at org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)
        at org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
        at java.base/java.lang.Thread.run(Thread.java:840)
        Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
                at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
                at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
                at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
                at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
                at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
                at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:961)
                at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:876)
                at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
                at org.apache.hadoop.ipc.Client.call(Client.java:1529)
                at org.apache.hadoop.ipc.Client.call(Client.java:1426)
                at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)
                at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)
                at jdk.proxy2/jdk.proxy2.$Proxy46.getFileInfo(Unknown Source)
                at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.lambda$getFileInfo$41(ClientNamenodeProtocolTranslatorPB.java:820)
                at org.apache.hadoop.ipc.internal.ShadedProtobufHelper.ipc(ShadedProtobufHelper.java:160)
                at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:820)
                at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
                at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.base/java.lang.reflect.Method.invoke(Method.java:569)
                at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:437)
                at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:170)
                at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:162)
                at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:100)
                at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:366)
                at jdk.proxy2/jdk.proxy2.$Proxy47.getFileInfo(Unknown Source)
                at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1770)
                at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1828)
                at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1825)
                at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
                at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1840)
                at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)
                at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)
                at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)
                at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
                at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)
                at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
                at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
                at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
                at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
                at scala.Option.getOrElse(Option.scala:201)
                at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
                at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
                at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
                at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
                at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
                at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
                at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
                at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
                at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
                at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
                at scala.collection.immutable.List.foldLeft(List.scala:79)
                at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
                at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
                at scala.collection.immutable.List.foreach(List.scala:334)
                at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
                at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
                at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
                at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
                at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
                at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
                at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
                at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
                at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
                at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
                at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
                at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
                at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
                at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
                at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
                at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
                at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
                at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
                at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
                at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
                at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
                at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
                at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
                at scala.util.Try$.apply(Try.scala:217)
                at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
                at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
                at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
                ... 23 more
Caused by: java.net.ConnectException: Connection refused
        at java.base/sun.nio.ch.Net.pollConnect(Native Method)
        at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
        at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:614)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:789)
        at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:364)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
        at org.apache.hadoop.ipc.Client.call(Client.java:1473)
        at org.apache.hadoop.ipc.Client.call(Client.java:1426)
        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)
        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)
        at jdk.proxy2/jdk.proxy2.$Proxy46.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.lambda$getFileInfo$41(ClientNamenodeProtocolTranslatorPB.java:820)
        at org.apache.hadoop.ipc.internal.ShadedProtobufHelper.ipc(ShadedProtobufHelper.java:160)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:820)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:437)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:170)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:162)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:100)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:366)
        at jdk.proxy2/jdk.proxy2.$Proxy47.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1770)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1828)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1825)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1840)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)
        at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)
        at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)
        at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:416)
        at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
        at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
        at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
        at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
        at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
        at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
        at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
        at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)

>>>
>>> df.show(5)       # Shows first 5 rows
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'df' is not defined
>>> df.printSchema() # Shows column names and data types
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'df' is not defined
>>> jps
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'jps' is not defined
>>> :q
  File "<stdin>", line 1
    :q
    ^
SyntaxError: invalid syntax
>>> exit
Use exit() or Ctrl-D (i.e. EOF) to exit
>>>
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
19444 ResourceManager
19572 NodeManager
19014 DataNode
21928 Jps
19210 SecondaryNameNode
18859 NameNode
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs dfs -ls /
2025-10-10 14:35:53,003 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
drwxr-xr-x   - hduser supergroup          0 2025-10-10 13:42 /user
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from pyspark.sql import SparkSession

df = spark.read.option("header", "true") \
               .option("inferSchema", "true") \
               .csv("hdfs://localhost:9000/user/hduser/input/accepted_2007_to_2018Q4.csv")

df.show(5)
>>>
>>> spark = SparkSession.builder \
...     .appName("LendingClub") \
...     .master("local[*]") \
...     .getOrCreate()
WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/10/10 14:36:16 WARN Utils: Your hostname, DESKTOP-I4C3SAV, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/10/10 14:36:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/10/10 14:36:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
>>>
>>> df = spark.read.option("header", "true") \
...                .option("inferSchema", "true") \
...                .csv("hdfs://localhost:9000/user/hduser/input/accepted_2007_to_2018Q4.csv")
25/10/10 14:36:21 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: hdfs://localhost:9000/user/hduser/input/accepted_2007_to_2018Q4.csv.
java.net.ConnectException: Call From DESKTOP-I4C3SAV/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:961)
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:876)
        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
        at org.apache.hadoop.ipc.Client.call(Client.java:1529)
        at org.apache.hadoop.ipc.Client.call(Client.java:1426)
        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)
        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)
        at jdk.proxy2/jdk.proxy2.$Proxy46.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.lambda$getFileInfo$41(ClientNamenodeProtocolTranslatorPB.java:820)
        at org.apache.hadoop.ipc.internal.ShadedProtobufHelper.ipc(ShadedProtobufHelper.java:160)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:820)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:437)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:170)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:162)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:100)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:366)
        at jdk.proxy2/jdk.proxy2.$Proxy47.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1770)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1828)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1825)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1840)
        at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
        at scala.Option.getOrElse(Option.scala:201)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
        at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
        at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
        at scala.collection.immutable.List.foldLeft(List.scala:79)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
        at scala.collection.immutable.List.foreach(List.scala:334)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
        at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
        at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
        at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
        at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
        at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
        at scala.util.Try$.apply(Try.scala:217)
        at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
        at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
        at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
        at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
        at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
        at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
        at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
        at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
        at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
        at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
        at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)
        at org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)
        at org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
        at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.ConnectException: Connection refused
        at java.base/sun.nio.ch.Net.pollConnect(Native Method)
        at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
        at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:614)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:789)
        at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:364)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
        at org.apache.hadoop.ipc.Client.call(Client.java:1473)
        ... 99 more
Traceback (most recent call last):
  File "<stdin>", line 3, in <module>
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 838, in csv
    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/hduser/pyspark-env/lib/python3.12/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o31.csv.
: java.net.ConnectException: Call From DESKTOP-I4C3SAV/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:961)
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:876)
        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
        at org.apache.hadoop.ipc.Client.call(Client.java:1529)
        at org.apache.hadoop.ipc.Client.call(Client.java:1426)
        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)
        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)
        at jdk.proxy2/jdk.proxy2.$Proxy46.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.lambda$getFileInfo$41(ClientNamenodeProtocolTranslatorPB.java:820)
        at org.apache.hadoop.ipc.internal.ShadedProtobufHelper.ipc(ShadedProtobufHelper.java:160)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:820)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:437)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:170)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:162)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:100)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:366)
        at jdk.proxy2/jdk.proxy2.$Proxy47.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1770)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1828)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1825)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1840)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)
        at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)
        at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)
        at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
        at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)
        at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
        at scala.Option.getOrElse(Option.scala:201)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
        at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
        at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
        at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
        at scala.collection.immutable.List.foldLeft(List.scala:79)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
        at scala.collection.immutable.List.foreach(List.scala:334)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
        at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
        at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
        at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
        at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
        at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
        at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
        at scala.util.Try$.apply(Try.scala:217)
        at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
        at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
        at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
        at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
        at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
        at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
        at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
        at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
        at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
        at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)
        at org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)
        at org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
        at java.base/java.lang.Thread.run(Thread.java:840)
        Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
                at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
                at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
                at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
                at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
                at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
                at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:961)
                at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:876)
                at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)
                at org.apache.hadoop.ipc.Client.call(Client.java:1529)
                at org.apache.hadoop.ipc.Client.call(Client.java:1426)
                at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)
                at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)
                at jdk.proxy2/jdk.proxy2.$Proxy46.getFileInfo(Unknown Source)
                at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.lambda$getFileInfo$41(ClientNamenodeProtocolTranslatorPB.java:820)
                at org.apache.hadoop.ipc.internal.ShadedProtobufHelper.ipc(ShadedProtobufHelper.java:160)
                at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:820)
                at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
                at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.base/java.lang.reflect.Method.invoke(Method.java:569)
                at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:437)
                at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:170)
                at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:162)
                at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:100)
                at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:366)
                at jdk.proxy2/jdk.proxy2.$Proxy47.getFileInfo(Unknown Source)
                at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1770)
                at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1828)
                at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1825)
                at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
                at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1840)
                at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)
                at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)
                at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)
                at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
                at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:814)
                at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
                at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
                at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
                at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
                at scala.Option.getOrElse(Option.scala:201)
                at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
                at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
                at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
                at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
                at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
                at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
                at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
                at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
                at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
                at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
                at scala.collection.immutable.List.foldLeft(List.scala:79)
                at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
                at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
                at scala.collection.immutable.List.foreach(List.scala:334)
                at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
                at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
                at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
                at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
                at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
                at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
                at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
                at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
                at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
                at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
                at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
                at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
                at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
                at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
                at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
                at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
                at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
                at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
                at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
                at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
                at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
                at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
                at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
                at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
                at scala.util.Try$.apply(Try.scala:217)
                at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
                at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
                at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
                ... 23 more
Caused by: java.net.ConnectException: Connection refused
        at java.base/sun.nio.ch.Net.pollConnect(Native Method)
        at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
        at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:614)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:789)
        at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:364)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)
        at org.apache.hadoop.ipc.Client.call(Client.java:1473)
        at org.apache.hadoop.ipc.Client.call(Client.java:1426)
        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)
        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)
        at jdk.proxy2/jdk.proxy2.$Proxy46.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.lambda$getFileInfo$41(ClientNamenodeProtocolTranslatorPB.java:820)
        at org.apache.hadoop.ipc.internal.ShadedProtobufHelper.ipc(ShadedProtobufHelper.java:160)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:820)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:437)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:170)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:162)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:100)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:366)
        at jdk.proxy2/jdk.proxy2.$Proxy47.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1770)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1828)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1825)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1840)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)
        at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:809)
        at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:807)
        at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:416)
        at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
        at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
        at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
        at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
        at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
        at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
        at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
        at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)

>>>
>>> df.show(5)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'df' is not defined
>>>
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/etc/hadoop/core-site.xml
bash: /usr/local/hadoop/etc/hadoop/core-site.xml: Permission denied
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo $HADOOP_HOME/etc/hadoop/core-site.xml
[sudo] password for hduser:
sudo: /usr/local/hadoop/etc/hadoop/core-site.xml: command not found
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ su $HADOOP_HOME/etc/hadoop/core-site.xml
su: user /usr/local/hadoop/etc/hadoop/core-site.xml does not exist or the user entry does not contain all the required fields
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/stop-all.sh
WARNING: Stopping all Apache Hadoop daemons as hduser in 10 seconds.
WARNING: Use CTRL-C to abort.
Stopping namenodes on [localhost]
Stopping datanodes
Stopping secondary namenodes [DESKTOP-I4C3SAV]
2025-10-10 14:42:09,527 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Stopping nodemanagers
Stopping resourcemanager
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ $HADOOP_HOME/sbin/start-all.sh
WARNING: Attempting to start all Apache Hadoop daemons as hduser in 10 seconds.
WARNING: This is not a recommended production deployment configuration.
WARNING: Use CTRL-C to abort.
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes [DESKTOP-I4C3SAV]
2025-10-10 14:42:34,996 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting resourcemanager
Starting nodemanagers
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jps
23584 ResourceManager
23713 NodeManager
24099 Jps
23142 DataNode
23019 NameNode
23372 SecondaryNameNode
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ hdfs dfs -ls /
2025-10-10 14:42:47,656 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
drwxr-xr-x   - hduser supergroup          0 2025-10-10 13:42 /user
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ pyspark
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/10/10 14:43:26 WARN Utils: Your hostname, DESKTOP-I4C3SAV, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/10/10 14:43:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/10/10 14:43:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 4.0.1
      /_/

Using Python version 3.12.3 (main, Aug 14 2025 17:47:21)
Spark context Web UI available at http://10.255.255.254:4040
Spark context available as 'sc' (master = local[*], app id = local-1760107408053).
SparkSession available as 'spark'.
>>> df = spark.read.option("header", "true").option("inferSchema", "true") \
...     .csv("hdfs://localhost:9000/user/hduser/input/accepted_2007_to_2018Q4.csv")
w(5)
>>>
>>> df.show(5)
25/10/10 14:43:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
+--------+---------+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+--------+-----------+----------+--------------------+----+------------------+------------------+--------+----------+-----+-----------+----------------+--------------+---------------+--------------+----------------------+----------------------+--------+-------+---------+----------+---------+-------------------+---------+-------------+------------------+---------------+---------------+-------------+------------------+----------+-----------------------+------------+---------------+------------+------------------+--------------------+-------------------+--------------------------+---------------------------+-----------+----------------+----------------+---------+-------------------------+--------------+------------+-----------+-----------+-----------+-----------+-----------+------------------+------------+-------+-----------+-----------+----------+--------+----------------+------+-----------+------------+--------------------+-----------+--------------+-------+------------------------+-----------+------------------+--------------------+---------------------+--------------+--------+--------------------+------------------------+---------------------+------------------------------+---------------------+--------------+---------------+-----------+---------+---------+-------------+-------------+-------------------+--------+----------------+------------+------------------+------------------+--------------+----------------+--------------------+---------+---------------+-----------------+--------------+--------------------------+---------------+----------------------+-----------------------+------------------------+----------------------+----------------+----------------+------------------+-------------------+---------------------+--------------------------------+----------------------------------+-----------------------------------+-------------+-------------+---------------+---------------+-------------+---------------+-------------------+-----------------+-----------------------+---------------+------------+--------------------+------------------------------------------+------------------------------+----------------------------+-------------------+--------------------+-------------------------+-----------------+---------------+-----------------+---------------------+---------------+
|      id|member_id|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|           emp_title|emp_length|home_ownership|annual_inc|verification_status| issue_d|loan_status|pymnt_plan|                 url|desc|           purpose|             title|zip_code|addr_state|  dti|delinq_2yrs|earliest_cr_line|fico_range_low|fico_range_high|inq_last_6mths|mths_since_last_delinq|mths_since_last_record|open_acc|pub_rec|revol_bal|revol_util|total_acc|initial_list_status|out_prncp|out_prncp_inv|       total_pymnt|total_pymnt_inv|total_rec_prncp|total_rec_int|total_rec_late_fee|recoveries|collection_recovery_fee|last_pymnt_d|last_pymnt_amnt|next_pymnt_d|last_credit_pull_d|last_fico_range_high|last_fico_range_low|collections_12_mths_ex_med|mths_since_last_major_derog|policy_code|application_type|annual_inc_joint|dti_joint|verification_status_joint|acc_now_delinq|tot_coll_amt|tot_cur_bal|open_acc_6m|open_act_il|open_il_12m|open_il_24m|mths_since_rcnt_il|total_bal_il|il_util|open_rv_12m|open_rv_24m|max_bal_bc|all_util|total_rev_hi_lim|inq_fi|total_cu_tl|inq_last_12m|acc_open_past_24mths|avg_cur_bal|bc_open_to_buy|bc_util|chargeoff_within_12_mths|delinq_amnt|mo_sin_old_il_acct|mo_sin_old_rev_tl_op|mo_sin_rcnt_rev_tl_op|mo_sin_rcnt_tl|mort_acc|mths_since_recent_bc|mths_since_recent_bc_dlq|mths_since_recent_inq|mths_since_recent_revol_delinq|num_accts_ever_120_pd|num_actv_bc_tl|num_actv_rev_tl|num_bc_sats|num_bc_tl|num_il_tl|num_op_rev_tl|num_rev_accts|num_rev_tl_bal_gt_0|num_sats|num_tl_120dpd_2m|num_tl_30dpd|num_tl_90g_dpd_24m|num_tl_op_past_12m|pct_tl_nvr_dlq|percent_bc_gt_75|pub_rec_bankruptcies|tax_liens|tot_hi_cred_lim|total_bal_ex_mort|total_bc_limit|total_il_high_credit_limit|revol_bal_joint|sec_app_fico_range_low|sec_app_fico_range_high|sec_app_earliest_cr_line|sec_app_inq_last_6mths|sec_app_mort_acc|sec_app_open_acc|sec_app_revol_util|sec_app_open_act_il|sec_app_num_rev_accts|sec_app_chargeoff_within_12_mths|sec_app_collections_12_mths_ex_med|sec_app_mths_since_last_major_derog|hardship_flag|hardship_type|hardship_reason|hardship_status|deferral_term|hardship_amount|hardship_start_date|hardship_end_date|payment_plan_start_date|hardship_length|hardship_dpd|hardship_loan_status|orig_projected_additional_accrued_interest|hardship_payoff_balance_amount|hardship_last_payment_amount|disbursement_method|debt_settlement_flag|debt_settlement_flag_date|settlement_status|settlement_date|settlement_amount|settlement_percentage|settlement_term|
+--------+---------+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+--------+-----------+----------+--------------------+----+------------------+------------------+--------+----------+-----+-----------+----------------+--------------+---------------+--------------+----------------------+----------------------+--------+-------+---------+----------+---------+-------------------+---------+-------------+------------------+---------------+---------------+-------------+------------------+----------+-----------------------+------------+---------------+------------+------------------+--------------------+-------------------+--------------------------+---------------------------+-----------+----------------+----------------+---------+-------------------------+--------------+------------+-----------+-----------+-----------+-----------+-----------+------------------+------------+-------+-----------+-----------+----------+--------+----------------+------+-----------+------------+--------------------+-----------+--------------+-------+------------------------+-----------+------------------+--------------------+---------------------+--------------+--------+--------------------+------------------------+---------------------+------------------------------+---------------------+--------------+---------------+-----------+---------+---------+-------------+-------------+-------------------+--------+----------------+------------+------------------+------------------+--------------+----------------+--------------------+---------+---------------+-----------------+--------------+--------------------------+---------------+----------------------+-----------------------+------------------------+----------------------+----------------+----------------+------------------+-------------------+---------------------+--------------------------------+----------------------------------+-----------------------------------+-------------+-------------+---------------+---------------+-------------+---------------+-------------------+-----------------+-----------------------+---------------+------------+--------------------+------------------------------------------+------------------------------+----------------------------+-------------------+--------------------+-------------------------+-----------------+---------------+-----------------+---------------------+---------------+
|68407277|     NULL|   3600.0|     3600.0|         3600.0| 36 months|   13.99|     123.03|    C|       C4|             leadman| 10+ years|      MORTGAGE|   55000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|NULL|debt_consolidation|Debt consolidation|   190xx|        PA| 5.91|        0.0|        Aug-2003|         675.0|          679.0|           1.0|                  30.0|                  NULL|     7.0|    0.0|   2765.0|      29.7|     13.0|                  w|      0.0|          0.0| 4421.723916800001|        4421.72|         3600.0|       821.72|               0.0|       0.0|                    0.0|    Jan-2019|         122.67|        NULL|          Mar-2019|               564.0|              560.0|                       0.0|                       30.0|        1.0|      Individual|            NULL|     NULL|                     NULL|           0.0|       722.0|   144904.0|        2.0|        2.0|        0.0|        1.0|              21.0|      4981.0|   36.0|        3.0|        3.0|     722.0|    34.0|          9300.0|   3.0|        1.0|         4.0|                 4.0|    20701.0|        1506.0|   37.2|                     0.0|        0.0|             148.0|               128.0|                  3.0|           3.0|     1.0|                 4.0|                    69.0|                  4.0|                          69.0|                  2.0|           2.0|            4.0|        2.0|      5.0|      3.0|          4.0|          9.0|                4.0|     7.0|             0.0|         0.0|               0.0|               3.0|          76.9|             0.0|                 0.0|      0.0|       178050.0|           7746.0|        2400.0|                   13734.0|           NULL|                  NULL|                   NULL|                    NULL|                  NULL|            NULL|            NULL|              NULL|               NULL|                 NULL|                            NULL|                              NULL|                               NULL|            N|         NULL|           NULL|           NULL|         NULL|           NULL|               NULL|             NULL|                   NULL|           NULL|        NULL|                NULL|                                      NULL|                          NULL|                        NULL|               Cash|                   N|                     NULL|             NULL|           NULL|             NULL|                 NULL|           NULL|
|68355089|     NULL|  24700.0|    24700.0|        24700.0| 36 months|   11.99|     820.28|    C|       C1|            Engineer| 10+ years|      MORTGAGE|   65000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|NULL|    small_business|          Business|   577xx|        SD|16.06|        1.0|        Dec-1999|         715.0|          719.0|           4.0|                   6.0|                  NULL|    22.0|    0.0|  21470.0|      19.2|     38.0|                  w|      0.0|          0.0|          25679.66|       25679.66|        24700.0|       979.66|               0.0|       0.0|                    0.0|    Jun-2016|         926.35|        NULL|          Mar-2019|               699.0|              695.0|                       0.0|                       NULL|        1.0|      Individual|            NULL|     NULL|                     NULL|           0.0|         0.0|   204396.0|        1.0|        1.0|        0.0|        1.0|              19.0|     18005.0|   73.0|        2.0|        3.0|    6472.0|    29.0|        111800.0|   0.0|        0.0|         6.0|                 4.0|     9733.0|       57830.0|   27.1|                     0.0|        0.0|             113.0|               192.0|                  2.0|           2.0|     4.0|                 2.0|                    NULL|                  0.0|                           6.0|                  0.0|           5.0|            5.0|       13.0|     17.0|      6.0|         20.0|         27.0|                5.0|    22.0|             0.0|         0.0|               0.0|               2.0|          97.4|             7.7|                 0.0|      0.0|       314017.0|          39475.0|       79300.0|                   24667.0|           NULL|                  NULL|                   NULL|                    NULL|                  NULL|            NULL|            NULL|              NULL|               NULL|                 NULL|                            NULL|                              NULL|                               NULL|            N|         NULL|           NULL|           NULL|         NULL|           NULL|               NULL|             NULL|                   NULL|           NULL|        NULL|                NULL|                                      NULL|                          NULL|                        NULL|               Cash|                   N|                     NULL|             NULL|           NULL|             NULL|                 NULL|           NULL|
|68341763|     NULL|  20000.0|    20000.0|        20000.0| 60 months|   10.78|     432.66|    B|       B4|        truck driver| 10+ years|      MORTGAGE|   63000.0|       Not Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|NULL|  home_improvement|              NULL|   605xx|        IL|10.78|        0.0|        Aug-2000|         695.0|          699.0|           0.0|                  NULL|                  NULL|     6.0|    0.0|   7869.0|      56.2|     18.0|                  w|      0.0|          0.0|22705.924293878397|       22705.92|        20000.0|      2705.92|               0.0|       0.0|                    0.0|    Jun-2017|        15813.3|        NULL|          Mar-2019|               704.0|              700.0|                       0.0|                       NULL|        1.0|       Joint App|         71000.0|    13.85|             Not Verified|           0.0|         0.0|   189699.0|        0.0|        1.0|        0.0|        4.0|              19.0|     10827.0|   73.0|        0.0|        2.0|    2081.0|    65.0|         14000.0|   2.0|        5.0|         1.0|                 6.0|    31617.0|        2737.0|   55.9|                     0.0|        0.0|             125.0|               184.0|                 14.0|          14.0|     5.0|               101.0|                    NULL|                 10.0|                          NULL|                  0.0|           2.0|            3.0|        2.0|      4.0|      6.0|          4.0|          7.0|                3.0|     6.0|             0.0|         0.0|               0.0|               0.0|         100.0|            50.0|                 0.0|      0.0|       218418.0|          18696.0|        6200.0|                   14877.0|           NULL|                  NULL|                   NULL|                    NULL|                  NULL|            NULL|            NULL|              NULL|               NULL|                 NULL|                            NULL|                              NULL|                               NULL|            N|         NULL|           NULL|           NULL|         NULL|           NULL|               NULL|             NULL|                   NULL|           NULL|        NULL|                NULL|                                      NULL|                          NULL|                        NULL|               Cash|                   N|                     NULL|             NULL|           NULL|             NULL|                 NULL|           NULL|
|66310712|     NULL|  35000.0|    35000.0|        35000.0| 60 months|   14.85|      829.9|    C|       C5|Information Syste...| 10+ years|      MORTGAGE|  110000.0|    Source Verified|Dec-2015|    Current|         n|https://lendingcl...|NULL|debt_consolidation|Debt consolidation|   076xx|        NJ|17.06|        0.0|        Sep-2008|         785.0|          789.0|           0.0|                  NULL|                  NULL|    13.0|    0.0|   7802.0|      11.6|     17.0|                  w| 15897.65|     15897.65|          31464.01|       31464.01|       19102.35|     12361.66|               0.0|       0.0|                    0.0|    Feb-2019|          829.9|    Apr-2019|          Mar-2019|               679.0|              675.0|                       0.0|                       NULL|        1.0|      Individual|            NULL|     NULL|                     NULL|           0.0|         0.0|   301500.0|        1.0|        1.0|        0.0|        1.0|              23.0|     12609.0|   70.0|        1.0|        1.0|    6987.0|    45.0|         67300.0|   0.0|        1.0|         0.0|                 2.0|    23192.0|       54962.0|   12.1|                     0.0|        0.0|              36.0|                87.0|                  2.0|           2.0|     1.0|                 2.0|                    NULL|                 NULL|                          NULL|                  0.0|           4.0|            5.0|        8.0|     10.0|      2.0|         10.0|         13.0|                5.0|    13.0|             0.0|         0.0|               0.0|               1.0|         100.0|             0.0|                 0.0|      0.0|       381215.0|          52226.0|       62500.0|                   18000.0|           NULL|                  NULL|                   NULL|                    NULL|                  NULL|            NULL|            NULL|              NULL|               NULL|                 NULL|                            NULL|                              NULL|                               NULL|            N|         NULL|           NULL|           NULL|         NULL|           NULL|               NULL|             NULL|                   NULL|           NULL|        NULL|                NULL|                                      NULL|                          NULL|                        NULL|               Cash|                   N|                     NULL|             NULL|           NULL|             NULL|                 NULL|           NULL|
|68476807|     NULL|  10400.0|    10400.0|        10400.0| 60 months|   22.45|     289.91|    F|       F1| Contract Specialist|   3 years|      MORTGAGE|  104433.0|    Source Verified|Dec-2015| Fully Paid|         n|https://lendingcl...|NULL|    major_purchase|    Major purchase|   174xx|        PA|25.37|        1.0|        Jun-1998|         695.0|          699.0|           3.0|                  12.0|                  NULL|    12.0|    0.0|  21929.0|      64.5|     35.0|                  w|      0.0|          0.0|           11740.5|        11740.5|        10400.0|       1340.5|               0.0|       0.0|                    0.0|    Jul-2016|       10128.96|        NULL|          Mar-2018|               704.0|              700.0|                       0.0|                       NULL|        1.0|      Individual|            NULL|     NULL|                     NULL|           0.0|         0.0|   331730.0|        1.0|        3.0|        0.0|        3.0|              14.0|     73839.0|   84.0|        4.0|        7.0|    9702.0|    78.0|         34000.0|   2.0|        1.0|         3.0|                10.0|    27644.0|        4567.0|   77.5|                     0.0|        0.0|             128.0|               210.0|                  4.0|           4.0|     6.0|                 4.0|                    12.0|                  1.0|                          12.0|                  0.0|           4.0|            6.0|        5.0|      9.0|     10.0|          7.0|         19.0|                6.0|    12.0|             0.0|         0.0|               0.0|               4.0|          96.6|            60.0|                 0.0|      0.0|       439570.0|          95768.0|       20300.0|                   88097.0|           NULL|                  NULL|                   NULL|                    NULL|                  NULL|            NULL|            NULL|              NULL|               NULL|                 NULL|                            NULL|                              NULL|                               NULL|            N|         NULL|           NULL|           NULL|         NULL|           NULL|               NULL|             NULL|                   NULL|           NULL|        NULL|                NULL|                                      NULL|                          NULL|                        NULL|               Cash|                   N|                     NULL|             NULL|           NULL|             NULL|                 NULL|           NULL|
+--------+---------+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+--------+-----------+----------+--------------------+----+------------------+------------------+--------+----------+-----+-----------+----------------+--------------+---------------+--------------+----------------------+----------------------+--------+-------+---------+----------+---------+-------------------+---------+-------------+------------------+---------------+---------------+-------------+------------------+----------+-----------------------+------------+---------------+------------+------------------+--------------------+-------------------+--------------------------+---------------------------+-----------+----------------+----------------+---------+-------------------------+--------------+------------+-----------+-----------+-----------+-----------+-----------+------------------+------------+-------+-----------+-----------+----------+--------+----------------+------+-----------+------------+--------------------+-----------+--------------+-------+------------------------+-----------+------------------+--------------------+---------------------+--------------+--------+--------------------+------------------------+---------------------+------------------------------+---------------------+--------------+---------------+-----------+---------+---------+-------------+-------------+-------------------+--------+----------------+------------+------------------+------------------+--------------+----------------+--------------------+---------+---------------+-----------------+--------------+--------------------------+---------------+----------------------+-----------------------+------------------------+----------------------+----------------+----------------+------------------+-------------------+---------------------+--------------------------------+----------------------------------+-----------------------------------+-------------+-------------+---------------+---------------+-------------+---------------+-------------------+-----------------+-----------------------+---------------+------------+--------------------+------------------------------------------+------------------------------+----------------------------+-------------------+--------------------+-------------------------+-----------------+---------------+-----------------+---------------------+---------------+
only showing top 5 rows
>>> df.printSchema()
root
 |-- id: string (nullable = true)
 |-- member_id: string (nullable = true)
 |-- loan_amnt: double (nullable = true)
 |-- funded_amnt: double (nullable = true)
 |-- funded_amnt_inv: double (nullable = true)
 |-- term: string (nullable = true)
 |-- int_rate: double (nullable = true)
 |-- installment: double (nullable = true)
 |-- grade: string (nullable = true)
 |-- sub_grade: string (nullable = true)
 |-- emp_title: string (nullable = true)
 |-- emp_length: string (nullable = true)
 |-- home_ownership: string (nullable = true)
 |-- annual_inc: string (nullable = true)
 |-- verification_status: string (nullable = true)
 |-- issue_d: string (nullable = true)
 |-- loan_status: string (nullable = true)
 |-- pymnt_plan: string (nullable = true)
 |-- url: string (nullable = true)
 |-- desc: string (nullable = true)
 |-- purpose: string (nullable = true)
 |-- title: string (nullable = true)
 |-- zip_code: string (nullable = true)
 |-- addr_state: string (nullable = true)
 |-- dti: string (nullable = true)
 |-- delinq_2yrs: string (nullable = true)
 |-- earliest_cr_line: string (nullable = true)
 |-- fico_range_low: string (nullable = true)
 |-- fico_range_high: string (nullable = true)
 |-- inq_last_6mths: string (nullable = true)
 |-- mths_since_last_delinq: string (nullable = true)
 |-- mths_since_last_record: string (nullable = true)
 |-- open_acc: string (nullable = true)
 |-- pub_rec: string (nullable = true)
 |-- revol_bal: string (nullable = true)
 |-- revol_util: string (nullable = true)
 |-- total_acc: string (nullable = true)
 |-- initial_list_status: string (nullable = true)
 |-- out_prncp: string (nullable = true)
 |-- out_prncp_inv: string (nullable = true)
 |-- total_pymnt: string (nullable = true)
 |-- total_pymnt_inv: string (nullable = true)
 |-- total_rec_prncp: string (nullable = true)
 |-- total_rec_int: string (nullable = true)
 |-- total_rec_late_fee: string (nullable = true)
 |-- recoveries: string (nullable = true)
 |-- collection_recovery_fee: string (nullable = true)
 |-- last_pymnt_d: string (nullable = true)
 |-- last_pymnt_amnt: string (nullable = true)
 |-- next_pymnt_d: string (nullable = true)
 |-- last_credit_pull_d: string (nullable = true)
 |-- last_fico_range_high: string (nullable = true)
 |-- last_fico_range_low: string (nullable = true)
 |-- collections_12_mths_ex_med: string (nullable = true)
 |-- mths_since_last_major_derog: string (nullable = true)
 |-- policy_code: string (nullable = true)
 |-- application_type: string (nullable = true)
 |-- annual_inc_joint: string (nullable = true)
 |-- dti_joint: string (nullable = true)
 |-- verification_status_joint: string (nullable = true)
 |-- acc_now_delinq: string (nullable = true)
 |-- tot_coll_amt: string (nullable = true)
 |-- tot_cur_bal: string (nullable = true)
 |-- open_acc_6m: string (nullable = true)
 |-- open_act_il: string (nullable = true)
 |-- open_il_12m: string (nullable = true)
 |-- open_il_24m: string (nullable = true)
 |-- mths_since_rcnt_il: string (nullable = true)
 |-- total_bal_il: string (nullable = true)
 |-- il_util: string (nullable = true)
 |-- open_rv_12m: string (nullable = true)
 |-- open_rv_24m: string (nullable = true)
 |-- max_bal_bc: string (nullable = true)
 |-- all_util: string (nullable = true)
 |-- total_rev_hi_lim: string (nullable = true)
 |-- inq_fi: string (nullable = true)
 |-- total_cu_tl: string (nullable = true)
 |-- inq_last_12m: double (nullable = true)
 |-- acc_open_past_24mths: double (nullable = true)
 |-- avg_cur_bal: string (nullable = true)
 |-- bc_open_to_buy: double (nullable = true)
 |-- bc_util: string (nullable = true)
 |-- chargeoff_within_12_mths: double (nullable = true)
 |-- delinq_amnt: double (nullable = true)
 |-- mo_sin_old_il_acct: double (nullable = true)
 |-- mo_sin_old_rev_tl_op: double (nullable = true)
 |-- mo_sin_rcnt_rev_tl_op: double (nullable = true)
 |-- mo_sin_rcnt_tl: double (nullable = true)
 |-- mort_acc: double (nullable = true)
 |-- mths_since_recent_bc: double (nullable = true)
 |-- mths_since_recent_bc_dlq: double (nullable = true)
 |-- mths_since_recent_inq: double (nullable = true)
 |-- mths_since_recent_revol_delinq: double (nullable = true)
 |-- num_accts_ever_120_pd: double (nullable = true)
 |-- num_actv_bc_tl: double (nullable = true)
 |-- num_actv_rev_tl: double (nullable = true)
 |-- num_bc_sats: double (nullable = true)
 |-- num_bc_tl: double (nullable = true)
 |-- num_il_tl: double (nullable = true)
 |-- num_op_rev_tl: double (nullable = true)
 |-- num_rev_accts: double (nullable = true)
 |-- num_rev_tl_bal_gt_0: double (nullable = true)
 |-- num_sats: double (nullable = true)
 |-- num_tl_120dpd_2m: double (nullable = true)
 |-- num_tl_30dpd: double (nullable = true)
 |-- num_tl_90g_dpd_24m: double (nullable = true)
 |-- num_tl_op_past_12m: double (nullable = true)
 |-- pct_tl_nvr_dlq: double (nullable = true)
 |-- percent_bc_gt_75: double (nullable = true)
 |-- pub_rec_bankruptcies: double (nullable = true)
 |-- tax_liens: double (nullable = true)
 |-- tot_hi_cred_lim: double (nullable = true)
 |-- total_bal_ex_mort: double (nullable = true)
 |-- total_bc_limit: double (nullable = true)
 |-- total_il_high_credit_limit: double (nullable = true)
 |-- revol_bal_joint: double (nullable = true)
 |-- sec_app_fico_range_low: double (nullable = true)
 |-- sec_app_fico_range_high: double (nullable = true)
 |-- sec_app_earliest_cr_line: string (nullable = true)
 |-- sec_app_inq_last_6mths: double (nullable = true)
 |-- sec_app_mort_acc: double (nullable = true)
 |-- sec_app_open_acc: double (nullable = true)
 |-- sec_app_revol_util: double (nullable = true)
 |-- sec_app_open_act_il: double (nullable = true)
 |-- sec_app_num_rev_accts: double (nullable = true)
 |-- sec_app_chargeoff_within_12_mths: double (nullable = true)
 |-- sec_app_collections_12_mths_ex_med: double (nullable = true)
 |-- sec_app_mths_since_last_major_derog: double (nullable = true)
 |-- hardship_flag: string (nullable = true)
 |-- hardship_type: string (nullable = true)
 |-- hardship_reason: string (nullable = true)
 |-- hardship_status: string (nullable = true)
 |-- deferral_term: string (nullable = true)
 |-- hardship_amount: string (nullable = true)
 |-- hardship_start_date: string (nullable = true)
 |-- hardship_end_date: string (nullable = true)
 |-- payment_plan_start_date: string (nullable = true)
 |-- hardship_length: string (nullable = true)
 |-- hardship_dpd: string (nullable = true)
 |-- hardship_loan_status: string (nullable = true)
 |-- orig_projected_additional_accrued_interest: string (nullable = true)
 |-- hardship_payoff_balance_amount: string (nullable = true)
 |-- hardship_last_payment_amount: string (nullable = true)
 |-- disbursement_method: string (nullable = true)
 |-- debt_settlement_flag: string (nullable = true)
 |-- debt_settlement_flag_date: string (nullable = true)
 |-- settlement_status: string (nullable = true)
 |-- settlement_date: string (nullable = true)
 |-- settlement_amount: string (nullable = true)
 |-- settlement_percentage: string (nullable = true)
 |-- settlement_term: string (nullable = true)

>>> df.select("loan_amnt", "int_rate", "annual_inc").show(10)
+---------+--------+----------+
|loan_amnt|int_rate|annual_inc|
+---------+--------+----------+
|   3600.0|   13.99|   55000.0|
|  24700.0|   11.99|   65000.0|
|  20000.0|   10.78|   63000.0|
|  35000.0|   14.85|  110000.0|
|  10400.0|   22.45|  104433.0|
|  11950.0|   13.44|   34000.0|
|  20000.0|    9.17|  180000.0|
|  20000.0|    8.49|   85000.0|
|  10000.0|    6.49|   85000.0|
|   8000.0|   11.48|   42000.0|
+---------+--------+----------+
only showing top 10 rows
>>> df.describe("loan_amnt", "int_rate").show()
+-------+------------------+------------------+
|summary|         loan_amnt|          int_rate|
+-------+------------------+------------------+
|  count|           2260668|           2260668|
|   mean|15046.931227849467|13.092829115115324|
| stddev| 9190.245488232755| 4.832138364571108|
|    min|             500.0|              5.31|
|    max|           40000.0|             30.99|
+-------+------------------+------------------+

>>>
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jupyter notebook
Command 'jupyter' not found, but can be installed with:
sudo apt install jupyter-core
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ sudo apt install jupyter-core
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages were automatically installed and are no longer required:
  hwloc-nox libmpich12 libslurm40t64
Use 'sudo apt autoremove' to remove them.
The following additional packages will be installed:
  python3-jupyter-core python3-platformdirs python3-traitlets
The following NEW packages will be installed:
  jupyter-core python3-jupyter-core python3-platformdirs python3-traitlets
0 upgraded, 4 newly installed, 0 to remove and 125 not upgraded.
Need to get 131 kB of archives.
After this operation, 716 kB of additional disk space will be used.
Do you want to continue? [Y/n] Y
Get:1 http://archive.ubuntu.com/ubuntu noble/main amd64 python3-platformdirs all 4.2.0-1 [16.1 kB]
Get:2 http://archive.ubuntu.com/ubuntu noble/universe amd64 python3-traitlets all 5.5.0-2 [85.0 kB]
Get:3 http://archive.ubuntu.com/ubuntu noble/universe amd64 python3-jupyter-core all 5.3.2-1ubuntu1 [25.5 kB]
Get:4 http://archive.ubuntu.com/ubuntu noble/universe amd64 jupyter-core all 5.3.2-1ubuntu1 [4044 B]
Fetched 131 kB in 1s (129 kB/s)
Selecting previously unselected package python3-platformdirs.
(Reading database ... 68869 files and directories currently installed.)
Preparing to unpack .../python3-platformdirs_4.2.0-1_all.deb ...
Unpacking python3-platformdirs (4.2.0-1) ...
Selecting previously unselected package python3-traitlets.
Preparing to unpack .../python3-traitlets_5.5.0-2_all.deb ...
Unpacking python3-traitlets (5.5.0-2) ...
Selecting previously unselected package python3-jupyter-core.
Preparing to unpack .../python3-jupyter-core_5.3.2-1ubuntu1_all.deb ...
Unpacking python3-jupyter-core (5.3.2-1ubuntu1) ...
Selecting previously unselected package jupyter-core.
Preparing to unpack .../jupyter-core_5.3.2-1ubuntu1_all.deb ...
Unpacking jupyter-core (5.3.2-1ubuntu1) ...
Setting up python3-platformdirs (4.2.0-1) ...
Setting up python3-traitlets (5.5.0-2) ...
Setting up python3-jupyter-core (5.3.2-1ubuntu1) ...
Setting up jupyter-core (5.3.2-1ubuntu1) ...
Processing triggers for man-db (2.12.0-4build2) ...
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jupyter notebook
usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir] [--paths] [--json] [--debug] [subcommand]

Jupyter: Interactive Computing

positional arguments:
  subcommand     the subcommand to launch

options:
  -h, --help     show this help message and exit
  --version      show the versions of core jupyter packages and exit
  --config-dir   show Jupyter config dir
  --data-dir     show Jupyter data dir
  --runtime-dir  show Jupyter runtime dir
  --paths        show all Jupyter paths. Add --json for machine-readable format.
  --json         output paths as machine-readable json
  --debug        output debug information about paths

Available subcommands: migrate troubleshoot

Jupyter command `jupyter-notebook` not found.
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ pip install jupyter notebook
Collecting jupyter
  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)
Collecting notebook
  Downloading notebook-7.4.7-py3-none-any.whl.metadata (10 kB)
Collecting jupyter-console (from jupyter)
  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)
Collecting nbconvert (from jupyter)
  Downloading nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)
Collecting ipykernel (from jupyter)
  Downloading ipykernel-6.30.1-py3-none-any.whl.metadata (6.2 kB)
Collecting ipywidgets (from jupyter)
  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)
Collecting jupyterlab (from jupyter)
  Downloading jupyterlab-4.4.9-py3-none-any.whl.metadata (16 kB)
Collecting jupyter-server<3,>=2.4.0 (from notebook)
  Downloading jupyter_server-2.17.0-py3-none-any.whl.metadata (8.5 kB)
Collecting jupyterlab-server<3,>=2.27.1 (from notebook)
  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)
Collecting notebook-shim<0.3,>=0.2 (from notebook)
  Downloading notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)
Collecting tornado>=6.2.0 (from notebook)
  Downloading tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)
Collecting anyio>=3.1.0 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)
Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)
Collecting jinja2>=3.0.3 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting jupyter-client>=7.4.4 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)
Collecting jupyter-core!=5.0.*,>=4.12 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading jupyter_core-5.8.1-py3-none-any.whl.metadata (1.6 kB)
Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)
Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)
Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)
Collecting packaging>=22.0 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading prometheus_client-0.23.1-py3-none-any.whl.metadata (1.9 kB)
Collecting pyzmq>=24 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading pyzmq-27.1.0-cp312-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (6.0 kB)
Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)
Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)
Collecting traitlets>=5.6.0 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)
Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->notebook)
  Downloading websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)
Collecting async-lru>=1.0.0 (from jupyterlab->jupyter)
  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)
Collecting httpx<1,>=0.25.0 (from jupyterlab->jupyter)
  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)
Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter)
  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)
Collecting setuptools>=41.1.0 (from jupyterlab->jupyter)
  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)
Collecting comm>=0.1.1 (from ipykernel->jupyter)
  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)
Collecting debugpy>=1.6.5 (from ipykernel->jupyter)
  Downloading debugpy-1.8.17-cp312-cp312-manylinux_2_34_x86_64.whl.metadata (1.4 kB)
Collecting ipython>=7.23.1 (from ipykernel->jupyter)
  Downloading ipython-9.6.0-py3-none-any.whl.metadata (4.4 kB)
Collecting matplotlib-inline>=0.1 (from ipykernel->jupyter)
  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)
Collecting nest-asyncio>=1.4 (from ipykernel->jupyter)
  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)
Collecting psutil>=5.7 (from ipykernel->jupyter)
  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)
Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->notebook)
  Downloading babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)
Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->notebook)
  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)
Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.27.1->notebook)
  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)
Collecting requests>=2.31 (from jupyterlab-server<3,>=2.27.1->notebook)
  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)
Collecting beautifulsoup4 (from nbconvert->jupyter)
  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)
Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert->jupyter)
  Downloading bleach-6.2.0-py3-none-any.whl.metadata (30 kB)
Collecting defusedxml (from nbconvert->jupyter)
  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)
Collecting jupyterlab-pygments (from nbconvert->jupyter)
  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)
Collecting markupsafe>=2.0 (from nbconvert->jupyter)
  Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)
Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter)
  Downloading mistune-3.1.4-py3-none-any.whl.metadata (1.8 kB)
Collecting nbclient>=0.5.0 (from nbconvert->jupyter)
  Downloading nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)
Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter)
  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)
Collecting pygments>=2.4.1 (from nbconvert->jupyter)
  Downloading pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)
Collecting widgetsnbextension~=4.0.14 (from ipywidgets->jupyter)
  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)
Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets->jupyter)
  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)
Collecting prompt-toolkit>=3.0.30 (from jupyter-console->jupyter)
  Downloading prompt_toolkit-3.0.52-py3-none-any.whl.metadata (6.4 kB)
Collecting idna>=2.8 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting sniffio>=1.1 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)
Collecting typing_extensions>=4.5 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook)
  Downloading argon2_cffi_bindings-25.1.0-cp39-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (7.4 kB)
Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter)
  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)
Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert->jupyter)
  Downloading tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)
Collecting certifi (from httpx<1,>=0.25.0->jupyterlab->jupyter)
  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)
Collecting httpcore==1.* (from httpx<1,>=0.25.0->jupyterlab->jupyter)
  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)
Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter)
  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)
Collecting decorator (from ipython>=7.23.1->ipykernel->jupyter)
  Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)
Collecting ipython-pygments-lexers (from ipython>=7.23.1->ipykernel->jupyter)
  Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)
Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter)
  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)
Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel->jupyter)
  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)
Collecting stack_data (from ipython>=7.23.1->ipykernel->jupyter)
  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)
Collecting attrs>=22.2.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook)
  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)
Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook)
  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)
Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook)
  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)
Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook)
  Downloading rpds_py-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
Collecting python-dateutil>=2.8.2 (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook)
  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook)
  Downloading platformdirs-4.5.0-py3-none-any.whl.metadata (12 kB)
Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading python_json_logger-4.0.0-py3-none-any.whl.metadata (4.0 kB)
Collecting pyyaml>=5.3 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)
Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)
Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)
Collecting wcwidth (from prompt-toolkit>=3.0.30->jupyter-console->jupyter)
  Downloading wcwidth-0.2.14-py2.py3-none-any.whl.metadata (15 kB)
Collecting charset_normalizer<4,>=2 (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook)
  Downloading charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)
Collecting urllib3<3,>=1.21.1 (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook)
  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)
Collecting ptyprocess (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->notebook)
  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter)
  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)
Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter)
  Downloading parso-0.8.5-py2.py3-none-any.whl.metadata (8.3 kB)
Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)
Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)
Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)
Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)
Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)
Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)
Collecting six>=1.5 (from python-dateutil>=2.8.2->jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook)
  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook)
  Downloading cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)
Collecting executing>=1.2.0 (from stack_data->ipython>=7.23.1->ipykernel->jupyter)
  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)
Collecting asttokens>=2.1.0 (from stack_data->ipython>=7.23.1->ipykernel->jupyter)
  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)
Collecting pure-eval (from stack_data->ipython>=7.23.1->ipykernel->jupyter)
  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)
Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook)
  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)
Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading lark-1.3.0-py3-none-any.whl.metadata (1.8 kB)
Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)
Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook)
  Downloading types_python_dateutil-2.9.0.20251008-py3-none-any.whl.metadata (1.8 kB)
Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)
Downloading notebook-7.4.7-py3-none-any.whl (14.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.3/14.3 MB 2.8 MB/s eta 0:00:00
Downloading jupyter_server-2.17.0-py3-none-any.whl (388 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.2/388.2 kB 1.4 MB/s eta 0:00:00
Downloading jupyterlab-4.4.9-py3-none-any.whl (12.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 1.8 MB/s eta 0:00:00
Downloading ipykernel-6.30.1-py3-none-any.whl (117 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.5/117.5 kB 1.1 MB/s eta 0:00:00
Downloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.7/59.7 kB 1.4 MB/s eta 0:00:00
Downloading nbconvert-7.16.6-py3-none-any.whl (258 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 258.5/258.5 kB 1.4 MB/s eta 0:00:00
Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)
Downloading tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 443.9/443.9 kB 1.4 MB/s eta 0:00:00
Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.8/139.8 kB 1.2 MB/s eta 0:00:00
Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)
Downloading anyio-4.11.0-py3-none-any.whl (109 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109.1/109.1 kB 1.3 MB/s eta 0:00:00
Downloading argon2_cffi-25.1.0-py3-none-any.whl (14 kB)
Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)
Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/10.2 MB 1.3 MB/s eta 0:00:00
Downloading bleach-6.2.0-py3-none-any.whl (163 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.4/163.4 kB 755.8 kB/s eta 0:00:00
Downloading comm-0.2.3-py3-none-any.whl (7.3 kB)
Downloading debugpy-1.8.17-cp312-cp312-manylinux_2_34_x86_64.whl (4.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 1.3 MB/s eta 0:00:00
Downloading httpx-0.28.1-py3-none-any.whl (73 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.5/73.5 kB 855.5 kB/s eta 0:00:00
Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.8/78.8 kB 607.7 kB/s eta 0:00:00
Downloading ipython-9.6.0-py3-none-any.whl (616 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 616.2/616.2 kB 501.2 kB/s eta 0:00:00
Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.9/134.9 kB 725.2 kB/s eta 0:00:00
Downloading json5-0.12.1-py3-none-any.whl (36 kB)
Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.0/90.0 kB 625.9 kB/s eta 0:00:00
Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.1/106.1 kB 717.3 kB/s eta 0:00:00
Downloading jupyter_core-5.8.1-py3-none-any.whl (28 kB)
Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)
Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.7/76.7 kB 1.3 MB/s eta 0:00:00
Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)
Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 216.6/216.6 kB 1.6 MB/s eta 0:00:00
Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)
Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)
Downloading mistune-3.1.4-py3-none-any.whl (53 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.5/53.5 kB 1.0 MB/s eta 0:00:00
Downloading nbclient-0.10.2-py3-none-any.whl (25 kB)
Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 1.3 MB/s eta 0:00:00
Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)
Downloading packaging-25.0-py3-none-any.whl (66 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.5/66.5 kB 1.3 MB/s eta 0:00:00
Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)
Downloading prometheus_client-0.23.1-py3-none-any.whl (61 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.1/61.1 kB 1.3 MB/s eta 0:00:00
Downloading prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 391.4/391.4 kB 1.7 MB/s eta 0:00:00
Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 291.2/291.2 kB 1.0 MB/s eta 0:00:00
Downloading pygments-2.19.2-py3-none-any.whl (1.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 1.6 MB/s eta 0:00:00
Downloading pyzmq-27.1.0-cp312-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (840 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 841.0/841.0 kB 1.3 MB/s eta 0:00:00
Downloading requests-2.32.5-py3-none-any.whl (64 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.7/64.7 kB 1.3 MB/s eta 0:00:00
Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)
Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)
Downloading terminado-0.18.1-py3-none-any.whl (14 kB)
Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.4/85.4 kB 1.6 MB/s eta 0:00:00
Downloading websocket_client-1.9.0-py3-none-any.whl (82 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.6/82.6 kB 1.3 MB/s eta 0:00:00
Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 1.8 MB/s eta 0:00:00
Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.4/106.4 kB 2.0 MB/s eta 0:00:00
Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)
Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)
Downloading attrs-25.4.0-py3-none-any.whl (67 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.6/67.6 kB 1.4 MB/s eta 0:00:00
Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.3/163.3 kB 2.1 MB/s eta 0:00:00
Downloading charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.8/151.8 kB 2.1 MB/s eta 0:00:00
Downloading fastjsonschema-2.21.2-py3-none-any.whl (24 kB)
Downloading idna-3.10-py3-none-any.whl (70 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.4/70.4 kB 1.7 MB/s eta 0:00:00
Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 1.8 MB/s eta 0:00:00
Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)
Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.8/63.8 kB 1.2 MB/s eta 0:00:00
Downloading platformdirs-4.5.0-py3-none-any.whl (18 kB)
Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)
Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 1.4 MB/s eta 0:00:00
Downloading python_json_logger-4.0.0-py3-none-any.whl (15 kB)
Downloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 807.9/807.9 kB 1.8 MB/s eta 0:00:00
Downloading referencing-0.36.2-py3-none-any.whl (26 kB)
Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)
Downloading rpds_py-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (386 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 386.9/386.9 kB 1.9 MB/s eta 0:00:00
Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)
Downloading soupsieve-2.8-py3-none-any.whl (36 kB)
Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)
Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.6/44.6 kB 1.2 MB/s eta 0:00:00
Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.8/129.8 kB 926.2 kB/s eta 0:00:00
Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)
Downloading argon2_cffi_bindings-25.1.0-cp39-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (87 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.1/87.1 kB 1.3 MB/s eta 0:00:00
Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)
Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)
Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)
Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)
Downloading wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)
Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)
Downloading cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (219 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 219.6/219.6 kB 1.0 MB/s eta 0:00:00
Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)
Downloading h11-0.16.0-py3-none-any.whl (37 kB)
Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)
Downloading parso-0.8.5-py2.py3-none-any.whl (106 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.7/106.7 kB 590.8 kB/s eta 0:00:00
Downloading rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)
Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)
Downloading webcolors-24.11.1-py3-none-any.whl (14 kB)
Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)
Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)
Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)
Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)
Downloading arrow-1.3.0-py3-none-any.whl (66 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.4/66.4 kB 640.9 kB/s eta 0:00:00
Downloading lark-1.3.0-py3-none-any.whl (113 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113.0/113.0 kB 722.5 kB/s eta 0:00:00
Downloading pycparser-2.23-py3-none-any.whl (118 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.1/118.1 kB 703.3 kB/s eta 0:00:00
Downloading types_python_dateutil-2.9.0.20251008-py3-none-any.whl (17 kB)
Installing collected packages: webencodings, pure-eval, ptyprocess, fastjsonschema, widgetsnbextension, websocket-client, webcolors, wcwidth, urllib3, uri-template, typing_extensions, types-python-dateutil, traitlets, tornado, tinycss2, soupsieve, sniffio, six, setuptools, send2trash, rpds-py, rfc3986-validator, pyzmq, pyyaml, python-json-logger, pygments, pycparser, psutil, prometheus-client, platformdirs, pexpect, parso, pandocfilters, packaging, nest-asyncio, mistune, markupsafe, lark, jupyterlab_widgets, jupyterlab-pygments, jsonpointer, json5, idna, h11, fqdn, executing, defusedxml, decorator, debugpy, comm, charset_normalizer, certifi, bleach, babel, attrs, async-lru, asttokens, terminado, stack_data, rfc3987-syntax, rfc3339-validator, requests, referencing, python-dateutil, prompt-toolkit, matplotlib-inline, jupyter-core, jinja2, jedi, ipython-pygments-lexers, httpcore, cffi, beautifulsoup4, anyio, jupyter-server-terminals, jupyter-client, jsonschema-specifications, ipython, httpx, arrow, argon2-cffi-bindings, jsonschema, isoduration, ipywidgets, ipykernel, argon2-cffi, nbformat, jupyter-console, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter
Successfully installed anyio-4.11.0 argon2-cffi-25.1.0 argon2-cffi-bindings-25.1.0 arrow-1.3.0 asttokens-3.0.0 async-lru-2.0.5 attrs-25.4.0 babel-2.17.0 beautifulsoup4-4.14.2 bleach-6.2.0 certifi-2025.10.5 cffi-2.0.0 charset_normalizer-3.4.3 comm-0.2.3 debugpy-1.8.17 decorator-5.2.1 defusedxml-0.7.1 executing-2.2.1 fastjsonschema-2.21.2 fqdn-1.5.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 ipykernel-6.30.1 ipython-9.6.0 ipython-pygments-lexers-1.1.1 ipywidgets-8.1.7 isoduration-20.11.0 jedi-0.19.2 jinja2-3.1.6 json5-0.12.1 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 jupyter-1.1.1 jupyter-client-8.6.3 jupyter-console-6.6.3 jupyter-core-5.8.1 jupyter-events-0.12.0 jupyter-lsp-2.3.0 jupyter-server-2.17.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.9 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 jupyterlab_widgets-3.0.15 lark-1.3.0 markupsafe-3.0.3 matplotlib-inline-0.1.7 mistune-3.1.4 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 nest-asyncio-1.6.0 notebook-7.4.7 notebook-shim-0.2.4 packaging-25.0 pandocfilters-1.5.1 parso-0.8.5 pexpect-4.9.0 platformdirs-4.5.0 prometheus-client-0.23.1 prompt-toolkit-3.0.52 psutil-7.1.0 ptyprocess-0.7.0 pure-eval-0.2.3 pycparser-2.23 pygments-2.19.2 python-dateutil-2.9.0.post0 python-json-logger-4.0.0 pyyaml-6.0.3 pyzmq-27.1.0 referencing-0.36.2 requests-2.32.5 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 rpds-py-0.27.1 send2trash-1.8.3 setuptools-80.9.0 six-1.17.0 sniffio-1.3.1 soupsieve-2.8 stack_data-0.6.3 terminado-0.18.1 tinycss2-1.4.0 tornado-6.5.2 traitlets-5.14.3 types-python-dateutil-2.9.0.20251008 typing_extensions-4.15.0 uri-template-1.3.0 urllib3-2.5.0 wcwidth-0.2.14 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.9.0 widgetsnbextension-4.0.14
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jupyter notebook
[I 2025-10-10 14:53:12.788 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2025-10-10 14:53:12.791 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2025-10-10 14:53:12.794 ServerApp] jupyterlab | extension was successfully linked.
[I 2025-10-10 14:53:12.796 ServerApp] notebook | extension was successfully linked.
[I 2025-10-10 14:53:12.798 ServerApp] Writing Jupyter server cookie secret to /home/hduser/.local/share/jupyter/runtime/jupyter_cookie_secret
[I 2025-10-10 14:53:13.004 ServerApp] notebook_shim | extension was successfully linked.
[I 2025-10-10 14:53:13.015 ServerApp] notebook_shim | extension was successfully loaded.
[I 2025-10-10 14:53:13.017 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2025-10-10 14:53:13.017 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2025-10-10 14:53:13.019 LabApp] JupyterLab extension loaded from /home/hduser/pyspark-env/lib/python3.12/site-packages/jupyterlab
[I 2025-10-10 14:53:13.019 LabApp] JupyterLab application directory is /home/hduser/pyspark-env/share/jupyter/lab
[I 2025-10-10 14:53:13.019 LabApp] Extension Manager is 'pypi'.
[I 2025-10-10 14:53:13.050 ServerApp] jupyterlab | extension was successfully loaded.
[I 2025-10-10 14:53:13.052 ServerApp] notebook | extension was successfully loaded.
[I 2025-10-10 14:53:13.053 ServerApp] Serving notebooks from local directory: /usr/local/hadoop/sbin
[I 2025-10-10 14:53:13.053 ServerApp] Jupyter Server 2.17.0 is running at:
[I 2025-10-10 14:53:13.053 ServerApp] http://localhost:8888/tree?token=7f944227db442f09e09a341449a7a52957f310996e6b3b3b
[I 2025-10-10 14:53:13.053 ServerApp]     http://127.0.0.1:8888/tree?token=7f944227db442f09e09a341449a7a52957f310996e6b3b3b
[I 2025-10-10 14:53:13.053 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 2025-10-10 14:53:13.057 ServerApp] No web browser found: Error('could not locate runnable browser').
[C 2025-10-10 14:53:13.058 ServerApp]

    To access the server, open this file in a browser:
        file:///home/hduser/.local/share/jupyter/runtime/jpserver-24766-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/tree?token=7f944227db442f09e09a341449a7a52957f310996e6b3b3b
        http://127.0.0.1:8888/tree?token=7f944227db442f09e09a341449a7a52957f310996e6b3b3b
[I 2025-10-10 14:53:13.067 ServerApp] Skipped non-installed server(s): basedpyright, bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyrefly, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[I 2025-10-10 14:53:55.007 ServerApp] 302 GET / (@127.0.0.1) 0.55ms
[W 2025-10-10 14:53:55.020 ServerApp] Clearing invalid/expired login cookie username-localhost-8888
[I 2025-10-10 14:53:55.020 JupyterNotebookApp] 302 GET /tree? (@127.0.0.1) 1.39ms
[W 2025-10-10 14:54:22.021 ServerApp] 401 POST /login?next=%2Ftree%3F (@127.0.0.1) 43.27ms referer=http://localhost:8888/login?next=%2Ftree%3F
[W 2025-10-10 14:54:27.956 ServerApp] 401 POST /login?next=%2Ftree%3F (@127.0.0.1) 44.63ms referer=http://localhost:8888/login?next=%2Ftree%3F
[W 2025-10-10 14:54:30.036 ServerApp] 401 POST /login?next=%2Ftree%3F (@127.0.0.1) 44.25ms referer=http://localhost:8888/login?next=%2Ftree%3F
[W 2025-10-10 14:56:18.655 ServerApp] 401 POST /login?next=%2Ftree%3F (@127.0.0.1) 43.93ms referer=http://localhost:8888/login?next=%2Ftree%3F
[W 2025-10-10 14:56:26.236 ServerApp] 401 POST /login?next=%2Ftree%3F (@127.0.0.1) 41.84ms referer=http://localhost:8888/login?next=%2Ftree%3F
[W 2025-10-10 14:56:41.868 ServerApp] 401 POST /login?next=%2Ftree%3F (@127.0.0.1) 45.00ms referer=http://localhost:8888/login?next=%2Ftree%3F
^C[I 2025-10-10 15:02:04.016 ServerApp] interrupted
[I 2025-10-10 15:02:04.018 ServerApp] Serving notebooks from local directory: /usr/local/hadoop/sbin
    0 active kernels
    Jupyter Server 2.17.0 is running at:
    http://localhost:8888/tree?token=7f944227db442f09e09a341449a7a52957f310996e6b3b3b
        http://127.0.0.1:8888/tree?token=7f944227db442f09e09a341449a7a52957f310996e6b3b3b
Shut down this Jupyter server (y/[n])? y
[C 2025-10-10 15:02:05.842 ServerApp] Shutdown confirmed
[I 2025-10-10 15:02:05.843 ServerApp] Shutting down 5 extensions
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ pip install notebook
Requirement already satisfied: notebook in /home/hduser/pyspark-env/lib/python3.12/site-packages (7.4.7)
Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from notebook) (2.17.0)
Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from notebook) (2.27.3)
Requirement already satisfied: jupyterlab<4.5,>=4.4.9 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from notebook) (4.4.9)
Requirement already satisfied: notebook-shim<0.3,>=0.2 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from notebook) (0.2.4)
Requirement already satisfied: tornado>=6.2.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from notebook) (6.5.2)
Requirement already satisfied: anyio>=3.1.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (4.11.0)
Requirement already satisfied: argon2-cffi>=21.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (25.1.0)
Requirement already satisfied: jinja2>=3.0.3 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (3.1.6)
Requirement already satisfied: jupyter-client>=7.4.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (8.6.3)
Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.8.1)
Requirement already satisfied: jupyter-events>=0.11.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.12.0)
Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.5.3)
Requirement already satisfied: nbconvert>=6.4.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (7.16.6)
Requirement already satisfied: nbformat>=5.3.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.10.4)
Requirement already satisfied: packaging>=22.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (25.0)
Requirement already satisfied: prometheus-client>=0.9 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.23.1)
Requirement already satisfied: pyzmq>=24 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (27.1.0)
Requirement already satisfied: send2trash>=1.8.2 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.3)
Requirement already satisfied: terminado>=0.8.3 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.18.1)
Requirement already satisfied: traitlets>=5.6.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.14.3)
Requirement already satisfied: websocket-client>=1.7 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.9.0)
Requirement already satisfied: async-lru>=1.0.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab<4.5,>=4.4.9->notebook) (2.0.5)
Requirement already satisfied: httpx<1,>=0.25.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab<4.5,>=4.4.9->notebook) (0.28.1)
Requirement already satisfied: ipykernel!=6.30.0,>=6.5.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab<4.5,>=4.4.9->notebook) (6.30.1)
Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab<4.5,>=4.4.9->notebook) (2.3.0)
Requirement already satisfied: setuptools>=41.1.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab<4.5,>=4.4.9->notebook) (80.9.0)
Requirement already satisfied: babel>=2.10 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.17.0)
Requirement already satisfied: json5>=0.9.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (0.12.1)
Requirement already satisfied: jsonschema>=4.18.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (4.25.1)
Requirement already satisfied: requests>=2.31 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.32.5)
Requirement already satisfied: idna>=2.8 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (3.10)
Requirement already satisfied: sniffio>=1.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (1.3.1)
Requirement already satisfied: typing_extensions>=4.5 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (4.15.0)
Requirement already satisfied: argon2-cffi-bindings in /home/hduser/pyspark-env/lib/python3.12/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (25.1.0)
Requirement already satisfied: certifi in /home/hduser/pyspark-env/lib/python3.12/site-packages (from httpx<1,>=0.25.0->jupyterlab<4.5,>=4.4.9->notebook) (2025.10.5)
Requirement already satisfied: httpcore==1.* in /home/hduser/pyspark-env/lib/python3.12/site-packages (from httpx<1,>=0.25.0->jupyterlab<4.5,>=4.4.9->notebook) (1.0.9)
Requirement already satisfied: h11>=0.16 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab<4.5,>=4.4.9->notebook) (0.16.0)
Requirement already satisfied: comm>=0.1.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (0.2.3)
Requirement already satisfied: debugpy>=1.6.5 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (1.8.17)
Requirement already satisfied: ipython>=7.23.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (9.6.0)
Requirement already satisfied: matplotlib-inline>=0.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (0.1.7)
Requirement already satisfied: nest-asyncio>=1.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (1.6.0)
Requirement already satisfied: psutil>=5.7 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (7.1.0)
Requirement already satisfied: MarkupSafe>=2.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook) (3.0.3)
Requirement already satisfied: attrs>=22.2.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (25.4.0)
Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (2025.9.1)
Requirement already satisfied: referencing>=0.28.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.36.2)
Requirement already satisfied: rpds-py>=0.7.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.27.1)
Requirement already satisfied: python-dateutil>=2.8.2 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook) (2.9.0.post0)
Requirement already satisfied: platformdirs>=2.5 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook) (4.5.0)
Requirement already satisfied: python-json-logger>=2.0.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (4.0.0)
Requirement already satisfied: pyyaml>=5.3 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (6.0.3)
Requirement already satisfied: rfc3339-validator in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (0.1.4)
Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (0.1.1)
Requirement already satisfied: beautifulsoup4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (4.14.2)
Requirement already satisfied: bleach!=5.0.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (6.2.0)
Requirement already satisfied: defusedxml in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.7.1)
Requirement already satisfied: jupyterlab-pygments in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.3.0)
Requirement already satisfied: mistune<4,>=2.0.3 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (3.1.4)
Requirement already satisfied: nbclient>=0.5.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.10.2)
Requirement already satisfied: pandocfilters>=1.4.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.5.1)
Requirement already satisfied: pygments>=2.4.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (2.19.2)
Requirement already satisfied: fastjsonschema>=2.15 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook) (2.21.2)
Requirement already satisfied: charset_normalizer<4,>=2 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (3.4.3)
Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (2.5.0)
Requirement already satisfied: ptyprocess in /home/hduser/pyspark-env/lib/python3.12/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->notebook) (0.7.0)
Requirement already satisfied: webencodings in /home/hduser/pyspark-env/lib/python3.12/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.5.1)
Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.4.0)
Requirement already satisfied: decorator in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (5.2.1)
Requirement already satisfied: ipython-pygments-lexers in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (1.1.1)
Requirement already satisfied: jedi>=0.16 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (0.19.2)
Requirement already satisfied: pexpect>4.3 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (4.9.0)
Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (3.0.52)
Requirement already satisfied: stack_data in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (0.6.3)
Requirement already satisfied: fqdn in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.5.1)
Requirement already satisfied: isoduration in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (20.11.0)
Requirement already satisfied: jsonpointer>1.13 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (3.0.0)
Requirement already satisfied: rfc3987-syntax>=1.1.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.1.0)
Requirement already satisfied: uri-template in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)
Requirement already satisfied: webcolors>=24.6.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (24.11.1)
Requirement already satisfied: six>=1.5 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook) (1.17.0)
Requirement already satisfied: cffi>=1.0.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (2.0.0)
Requirement already satisfied: soupsieve>1.2 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (2.8)
Requirement already satisfied: pycparser in /home/hduser/pyspark-env/lib/python3.12/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (2.23)
Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (0.8.5)
Requirement already satisfied: wcwidth in /home/hduser/pyspark-env/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (0.2.14)
Requirement already satisfied: lark>=1.2.2 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)
Requirement already satisfied: arrow>=0.15.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)
Requirement already satisfied: executing>=1.2.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (2.2.1)
Requirement already satisfied: asttokens>=2.1.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (3.0.0)
Requirement already satisfied: pure-eval in /home/hduser/pyspark-env/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab<4.5,>=4.4.9->notebook) (0.2.3)
Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (2.9.0.20251008)
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ pip install jupyterlab
Requirement already satisfied: jupyterlab in /home/hduser/pyspark-env/lib/python3.12/site-packages (4.4.9)
Requirement already satisfied: async-lru>=1.0.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (2.0.5)
Requirement already satisfied: httpx<1,>=0.25.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (0.28.1)
Requirement already satisfied: ipykernel!=6.30.0,>=6.5.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (6.30.1)
Requirement already satisfied: jinja2>=3.0.3 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (3.1.6)
Requirement already satisfied: jupyter-core in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (5.8.1)
Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (2.3.0)
Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (2.17.0)
Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (2.27.3)
Requirement already satisfied: notebook-shim>=0.2 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (0.2.4)
Requirement already satisfied: packaging in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (25.0)
Requirement already satisfied: setuptools>=41.1.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (80.9.0)
Requirement already satisfied: tornado>=6.2.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (6.5.2)
Requirement already satisfied: traitlets in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab) (5.14.3)
Requirement already satisfied: anyio in /home/hduser/pyspark-env/lib/python3.12/site-packages (from httpx<1,>=0.25.0->jupyterlab) (4.11.0)
Requirement already satisfied: certifi in /home/hduser/pyspark-env/lib/python3.12/site-packages (from httpx<1,>=0.25.0->jupyterlab) (2025.10.5)
Requirement already satisfied: httpcore==1.* in /home/hduser/pyspark-env/lib/python3.12/site-packages (from httpx<1,>=0.25.0->jupyterlab) (1.0.9)
Requirement already satisfied: idna in /home/hduser/pyspark-env/lib/python3.12/site-packages (from httpx<1,>=0.25.0->jupyterlab) (3.10)
Requirement already satisfied: h11>=0.16 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab) (0.16.0)
Requirement already satisfied: comm>=0.1.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (0.2.3)
Requirement already satisfied: debugpy>=1.6.5 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (1.8.17)
Requirement already satisfied: ipython>=7.23.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (9.6.0)
Requirement already satisfied: jupyter-client>=8.0.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (8.6.3)
Requirement already satisfied: matplotlib-inline>=0.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (0.1.7)
Requirement already satisfied: nest-asyncio>=1.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (1.6.0)
Requirement already satisfied: psutil>=5.7 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (7.1.0)
Requirement already satisfied: pyzmq>=25 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (27.1.0)
Requirement already satisfied: MarkupSafe>=2.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jinja2>=3.0.3->jupyterlab) (3.0.3)
Requirement already satisfied: platformdirs>=2.5 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-core->jupyterlab) (4.5.0)
Requirement already satisfied: argon2-cffi>=21.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (25.1.0)
Requirement already satisfied: jupyter-events>=0.11.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.12.0)
Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.5.3)
Requirement already satisfied: nbconvert>=6.4.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.16.6)
Requirement already satisfied: nbformat>=5.3.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (5.10.4)
Requirement already satisfied: prometheus-client>=0.9 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.23.1)
Requirement already satisfied: send2trash>=1.8.2 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.8.3)
Requirement already satisfied: terminado>=0.8.3 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.18.1)
Requirement already satisfied: websocket-client>=1.7 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.9.0)
Requirement already satisfied: babel>=2.10 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (2.17.0)
Requirement already satisfied: json5>=0.9.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (0.12.1)
Requirement already satisfied: jsonschema>=4.18.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (4.25.1)
Requirement already satisfied: requests>=2.31 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (2.32.5)
Requirement already satisfied: sniffio>=1.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from anyio->httpx<1,>=0.25.0->jupyterlab) (1.3.1)
Requirement already satisfied: typing_extensions>=4.5 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from anyio->httpx<1,>=0.25.0->jupyterlab) (4.15.0)
Requirement already satisfied: argon2-cffi-bindings in /home/hduser/pyspark-env/lib/python3.12/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (25.1.0)
Requirement already satisfied: decorator in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (5.2.1)
Requirement already satisfied: ipython-pygments-lexers in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (1.1.1)
Requirement already satisfied: jedi>=0.16 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (0.19.2)
Requirement already satisfied: pexpect>4.3 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (4.9.0)
Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (3.0.52)
Requirement already satisfied: pygments>=2.4.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (2.19.2)
Requirement already satisfied: stack_data in /home/hduser/pyspark-env/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (0.6.3)
Requirement already satisfied: attrs>=22.2.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (25.4.0)
Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (2025.9.1)
Requirement already satisfied: referencing>=0.28.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (0.36.2)
Requirement already satisfied: rpds-py>=0.7.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (0.27.1)
Requirement already satisfied: python-dateutil>=2.8.2 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-client>=8.0.0->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (2.9.0.post0)
Requirement already satisfied: python-json-logger>=2.0.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (4.0.0)
Requirement already satisfied: pyyaml>=5.3 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (6.0.3)
Requirement already satisfied: rfc3339-validator in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.4)
Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.1)
Requirement already satisfied: beautifulsoup4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (4.14.2)
Requirement already satisfied: bleach!=5.0.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (6.2.0)
Requirement already satisfied: defusedxml in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.1)
Requirement already satisfied: jupyterlab-pygments in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.3.0)
Requirement already satisfied: mistune<4,>=2.0.3 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (3.1.4)
Requirement already satisfied: nbclient>=0.5.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.10.2)
Requirement already satisfied: pandocfilters>=1.4.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.1)
Requirement already satisfied: fastjsonschema>=2.15 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.21.2)
Requirement already satisfied: charset_normalizer<4,>=2 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab) (3.4.3)
Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab) (2.5.0)
Requirement already satisfied: ptyprocess in /home/hduser/pyspark-env/lib/python3.12/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.0)
Requirement already satisfied: webencodings in /home/hduser/pyspark-env/lib/python3.12/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.5.1)
Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.4.0)
Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (0.8.5)
Requirement already satisfied: fqdn in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.1)
Requirement already satisfied: isoduration in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (20.11.0)
Requirement already satisfied: jsonpointer>1.13 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (3.0.0)
Requirement already satisfied: rfc3987-syntax>=1.1.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.1.0)
Requirement already satisfied: uri-template in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.0)
Requirement already satisfied: webcolors>=24.6.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (24.11.1)
Requirement already satisfied: wcwidth in /home/hduser/pyspark-env/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (0.2.14)
Requirement already satisfied: six>=1.5 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (1.17.0)
Requirement already satisfied: cffi>=1.0.1 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (2.0.0)
Requirement already satisfied: soupsieve>1.2 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.8)
Requirement already satisfied: executing>=1.2.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (2.2.1)
Requirement already satisfied: asttokens>=2.1.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (3.0.0)
Requirement already satisfied: pure-eval in /home/hduser/pyspark-env/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (0.2.3)
Requirement already satisfied: pycparser in /home/hduser/pyspark-env/lib/python3.12/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (2.23)
Requirement already satisfied: lark>=1.2.2 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.0)
Requirement already satisfied: arrow>=0.15.0 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.0)
Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/hduser/pyspark-env/lib/python3.12/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.9.0.20251008)
(pyspark-env) hduser@DESKTOP-I4C3SAV:/usr/local/hadoop/sbin$ jupyter notebook
[I 2025-10-10 15:02:26.925 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2025-10-10 15:02:26.928 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2025-10-10 15:02:26.932 ServerApp] jupyterlab | extension was successfully linked.
[I 2025-10-10 15:02:26.934 ServerApp] notebook | extension was successfully linked.
[I 2025-10-10 15:02:27.125 ServerApp] notebook_shim | extension was successfully linked.
[I 2025-10-10 15:02:27.134 ServerApp] notebook_shim | extension was successfully loaded.
[I 2025-10-10 15:02:27.136 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2025-10-10 15:02:27.137 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2025-10-10 15:02:27.138 LabApp] JupyterLab extension loaded from /home/hduser/pyspark-env/lib/python3.12/site-packages/jupyterlab
[I 2025-10-10 15:02:27.138 LabApp] JupyterLab application directory is /home/hduser/pyspark-env/share/jupyter/lab
[I 2025-10-10 15:02:27.139 LabApp] Extension Manager is 'pypi'.
[I 2025-10-10 15:02:27.172 ServerApp] jupyterlab | extension was successfully loaded.
[I 2025-10-10 15:02:27.174 ServerApp] notebook | extension was successfully loaded.
[I 2025-10-10 15:02:27.175 ServerApp] Serving notebooks from local directory: /usr/local/hadoop/sbin
[I 2025-10-10 15:02:27.175 ServerApp] Jupyter Server 2.17.0 is running at:
[I 2025-10-10 15:02:27.175 ServerApp] http://localhost:8888/tree?token=23e1ee1153f48a1071bd1e231583f4c4e335815407c6cf5f
[I 2025-10-10 15:02:27.175 ServerApp]     http://127.0.0.1:8888/tree?token=23e1ee1153f48a1071bd1e231583f4c4e335815407c6cf5f
[I 2025-10-10 15:02:27.175 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 2025-10-10 15:02:27.178 ServerApp] No web browser found: Error('could not locate runnable browser').
[C 2025-10-10 15:02:27.178 ServerApp]

    To access the server, open this file in a browser:
        file:///home/hduser/.local/share/jupyter/runtime/jpserver-24890-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/tree?token=23e1ee1153f48a1071bd1e231583f4c4e335815407c6cf5f
        http://127.0.0.1:8888/tree?token=23e1ee1153f48a1071bd1e231583f4c4e335815407c6cf5f
[I 2025-10-10 15:02:27.187 ServerApp] Skipped non-installed server(s): basedpyright, bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyrefly, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[I 2025-10-10 15:02:54.771 ServerApp] Uploading file to /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[I 2025-10-10 15:02:56.365 ServerApp] Writing notebook-signing key to /home/hduser/.local/share/jupyter/notebook_secret
[W 2025-10-10 15:02:56.382 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[W 2025-10-10 15:02:57.216 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 15:02:57.693 ServerApp] Kernel started: 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b
[I 2025-10-10 15:02:58.204 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[W 2025-10-10 15:02:58.206 ServerApp] The websocket_ping_timeout (90000) cannot be longer than the websocket_ping_interval (30000).
    Setting websocket_ping_timeout=30000
[I 2025-10-10 15:02:58.219 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[I 2025-10-10 15:02:58.242 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[I 2025-10-10 15:03:04.836 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[I 2025-10-10 15:04:57.784 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 15:04:57.786 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/10/10 15:06:36 WARN Utils: Your hostname, DESKTOP-I4C3SAV, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/10/10 15:06:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/10/10 15:06:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[I 2025-10-10 15:06:57.245 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 15:06:57.245 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
25/10/10 15:07:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
25/10/10 15:08:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[I 2025-10-10 15:08:57.383 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 15:08:57.384 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
25/10/10 15:09:06 ERROR Executor: Exception in task 9.0 in stage 6.0 (TID 38)13]
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 36 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 15:09:06 ERROR Executor: Exception in task 1.0 in stage 6.0 (TID 30)
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 36 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 15:09:06 ERROR Executor: Exception in task 3.0 in stage 6.0 (TID 32)
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 36 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 15:09:06 ERROR Executor: Exception in task 4.0 in stage 6.0 (TID 33)
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 60 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 15:09:06 ERROR Executor: Exception in task 8.0 in stage 6.0 (TID 37)
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 60 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 15:09:06 ERROR Executor: Exception in task 2.0 in stage 6.0 (TID 31)
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 60 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 15:09:06 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 29)
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 36 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 15:09:06 ERROR Executor: Exception in task 11.0 in stage 6.0 (TID 40)
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 36 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 15:09:06 ERROR Executor: Exception in task 10.0 in stage 6.0 (TID 39)
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 36 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 15:09:06 ERROR Executor: Exception in task 6.0 in stage 6.0 (TID 35)
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 36 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 15:09:06 ERROR Executor: Exception in task 7.0 in stage 6.0 (TID 36)
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 36 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 15:09:06 ERROR Executor: Exception in task 5.0 in stage 6.0 (TID 34)
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 36 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 15:09:06 WARN TaskSetManager: Lost task 10.0 in stage 6.0 (TID 39) (10.255.255.254 executor driver): org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 36 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)

25/10/10 15:09:06 ERROR TaskSetManager: Task 10 in stage 6.0 failed 1 times; aborting job
25/10/10 15:09:06 WARN TaskSetManager: Lost task 8.0 in stage 6.0 (TID 37) (10.255.255.254 executor driver): org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value ' 60 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.Or_2$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply_0_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:211)
        at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:209)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:192)
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)

25/10/10 15:09:06 WARN TaskSetManager: Lost task 12.0 in stage 6.0 (TID 41) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: [CAST_INVALID_INPUT] The value ' 36 months' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"isnan" was called from
java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
)
[I 2025-10-10 15:10:59.419 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 15:10:59.420 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 15:12:58.970 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 15:12:58.971 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 15:14:01.759 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 15:14:01.760 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 15:15:37.454 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 15:15:37.455 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 15:16:06.343 ServerApp] Starting buffering for 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b:f595aaf5-2eee-45e9-964d-14e3c87063f2
[I 2025-10-10 15:16:07.308 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[I 2025-10-10 15:16:07.308 ServerApp] Restoring connection for 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b:f595aaf5-2eee-45e9-964d-14e3c87063f2
[I 2025-10-10 16:06:16.345 ServerApp] Starting buffering for 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b:f595aaf5-2eee-45e9-964d-14e3c87063f2
[I 2025-10-10 16:06:24.694 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[I 2025-10-10 16:06:24.694 ServerApp] Restoring connection for 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b:f595aaf5-2eee-45e9-964d-14e3c87063f2
[I 2025-10-10 16:08:40.882 ServerApp] Starting buffering for 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b:f595aaf5-2eee-45e9-964d-14e3c87063f2
[I 2025-10-10 16:08:41.742 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[I 2025-10-10 16:08:41.742 ServerApp] Restoring connection for 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b:f595aaf5-2eee-45e9-964d-14e3c87063f2
[I 2025-10-10 16:14:02.445 ServerApp] Starting buffering for 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b:f595aaf5-2eee-45e9-964d-14e3c87063f2
[I 2025-10-10 16:14:02.993 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[I 2025-10-10 16:14:02.993 ServerApp] Restoring connection for 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b:f595aaf5-2eee-45e9-964d-14e3c87063f2
[I 2025-10-10 16:45:20.799 ServerApp] Starting buffering for 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b:f595aaf5-2eee-45e9-964d-14e3c87063f2
[I 2025-10-10 17:02:35.525 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[I 2025-10-10 17:02:35.525 ServerApp] Restoring connection for 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b:f595aaf5-2eee-45e9-964d-14e3c87063f2
[I 2025-10-10 17:04:31.091 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[I 2025-10-10 17:13:26.588 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:13:26.596 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:15:27.114 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:15:27.115 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:17:28.095 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:17:28.096 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:19:28.486 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:19:28.487 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:21:29.831 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:21:29.832 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:27:31.163 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:27:31.164 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:29:31.670 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:29:31.672 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:31:32.232 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:31:32.233 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:33:33.062 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:33:33.063 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[W 2025-10-10 17:33:45.268 LabApp] Could not determine jupyterlab build status without nodejs
[I 2025-10-10 17:33:45.613 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[I 2025-10-10 17:37:35.517 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:37:35.518 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:39:36.536 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:39:36.537 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:43:37.608 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:43:37.609 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:45:38.754 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:45:38.754 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
25/10/10 17:46:03 ERROR Executor: Exception in task 0.0 in stage 86.0 (TID 467)
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '0.0' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"__eq__" was called from
line 25 in cell [57]

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)
        at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toLongExact(UTF8StringUtils.scala:31)
        at org.apache.spark.sql.catalyst.util.UTF8StringUtils.toLongExact(UTF8StringUtils.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
25/10/10 17:46:03 WARN TaskSetManager: Lost task 0.0 in stage 86.0 (TID 467) (10.255.255.254 executor driver): org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '0.0' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== DataFrame ==
"__eq__" was called from
line 25 in cell [57]

        at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)
        at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)
        at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toLongExact(UTF8StringUtils.scala:31)
        at org.apache.spark.sql.catalyst.util.UTF8StringUtils.toLongExact(UTF8StringUtils.scala)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
        at org.apache.spark.scheduler.Task.run(Task.scala:147)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)

25/10/10 17:46:03 ERROR TaskSetManager: Task 0 in stage 86.0 failed 1 times; aborting job
[I 2025-10-10 17:47:38.799 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:47:38.800 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:51:40.502 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:51:40.503 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:53:39.735 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:53:39.736 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:55:40.046 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:55:40.047 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
[I 2025-10-10 17:57:40.731 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[W 2025-10-10 17:57:40.733 ServerApp] Notebook Bigdata-proj-LoanOutcomeAnalysis.ipynb is not trusted
25/10/10 17:58:35 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
[I 2025-10-10 17:59:41.376 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[I 2025-10-10 18:01:40.925 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[I 2025-10-10 18:02:32.010 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[I 2025-10-10 18:03:40.867 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[I 2025-10-10 18:07:41.519 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[I 2025-10-10 18:09:41.891 ServerApp] Saving file at /Bigdata-proj-LoanOutcomeAnalysis.ipynb
[I 2025-10-10 18:18:42.964 ServerApp] Starting buffering for 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b:f595aaf5-2eee-45e9-964d-14e3c87063f2
[I 2025-10-10 18:18:43.648 ServerApp] Connecting to kernel 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b.
[I 2025-10-10 18:18:43.649 ServerApp] Restoring connection for 4279f8d5-b56b-47eb-b5b1-6ed2f9fe506b:f595aaf5-2eee-45e9-964d-14e3c87063f2